{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.9809910774445147,
  "eval_steps": 500,
  "global_step": 146000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0020417747105784347,
      "grad_norm": 11.748659133911133,
      "learning_rate": 1.9986660405224225e-05,
      "loss": 4.0795,
      "step": 100
    },
    {
      "epoch": 0.0040835494211568695,
      "grad_norm": 14.720346450805664,
      "learning_rate": 1.9973048573820367e-05,
      "loss": 3.7887,
      "step": 200
    },
    {
      "epoch": 0.006125324131735305,
      "grad_norm": 13.206952095031738,
      "learning_rate": 1.995943674241651e-05,
      "loss": 3.6993,
      "step": 300
    },
    {
      "epoch": 0.008167098842313739,
      "grad_norm": 14.391979217529297,
      "learning_rate": 1.9945824911012653e-05,
      "loss": 3.555,
      "step": 400
    },
    {
      "epoch": 0.010208873552892173,
      "grad_norm": 13.157404899597168,
      "learning_rate": 1.99322130796088e-05,
      "loss": 3.5551,
      "step": 500
    },
    {
      "epoch": 0.01225064826347061,
      "grad_norm": 12.925856590270996,
      "learning_rate": 1.9918601248204942e-05,
      "loss": 3.4968,
      "step": 600
    },
    {
      "epoch": 0.014292422974049044,
      "grad_norm": 11.39137077331543,
      "learning_rate": 1.9904989416801085e-05,
      "loss": 3.4697,
      "step": 700
    },
    {
      "epoch": 0.016334197684627478,
      "grad_norm": 10.776779174804688,
      "learning_rate": 1.989137758539723e-05,
      "loss": 3.5308,
      "step": 800
    },
    {
      "epoch": 0.018375972395205914,
      "grad_norm": 12.52234935760498,
      "learning_rate": 1.9877765753993373e-05,
      "loss": 3.4974,
      "step": 900
    },
    {
      "epoch": 0.020417747105784347,
      "grad_norm": 12.089134216308594,
      "learning_rate": 1.9864153922589516e-05,
      "loss": 3.4999,
      "step": 1000
    },
    {
      "epoch": 0.022459521816362783,
      "grad_norm": 10.861238479614258,
      "learning_rate": 1.985054209118566e-05,
      "loss": 3.4408,
      "step": 1100
    },
    {
      "epoch": 0.02450129652694122,
      "grad_norm": 12.200742721557617,
      "learning_rate": 1.9836930259781805e-05,
      "loss": 3.4285,
      "step": 1200
    },
    {
      "epoch": 0.02654307123751965,
      "grad_norm": 11.333937644958496,
      "learning_rate": 1.9823318428377948e-05,
      "loss": 3.4089,
      "step": 1300
    },
    {
      "epoch": 0.028584845948098087,
      "grad_norm": 15.016937255859375,
      "learning_rate": 1.980970659697409e-05,
      "loss": 3.4878,
      "step": 1400
    },
    {
      "epoch": 0.030626620658676523,
      "grad_norm": 15.355469703674316,
      "learning_rate": 1.9796094765570237e-05,
      "loss": 3.4295,
      "step": 1500
    },
    {
      "epoch": 0.032668395369254956,
      "grad_norm": 13.239477157592773,
      "learning_rate": 1.978248293416638e-05,
      "loss": 3.4856,
      "step": 1600
    },
    {
      "epoch": 0.03471017007983339,
      "grad_norm": 11.78258228302002,
      "learning_rate": 1.9768871102762522e-05,
      "loss": 3.4054,
      "step": 1700
    },
    {
      "epoch": 0.03675194479041183,
      "grad_norm": 10.723127365112305,
      "learning_rate": 1.9755259271358668e-05,
      "loss": 3.3996,
      "step": 1800
    },
    {
      "epoch": 0.03879371950099026,
      "grad_norm": 12.71819019317627,
      "learning_rate": 1.974164743995481e-05,
      "loss": 3.4016,
      "step": 1900
    },
    {
      "epoch": 0.04083549421156869,
      "grad_norm": 11.879324913024902,
      "learning_rate": 1.9728035608550954e-05,
      "loss": 3.3488,
      "step": 2000
    },
    {
      "epoch": 0.04287726892214713,
      "grad_norm": 13.576035499572754,
      "learning_rate": 1.9714423777147096e-05,
      "loss": 3.381,
      "step": 2100
    },
    {
      "epoch": 0.044919043632725565,
      "grad_norm": 10.627731323242188,
      "learning_rate": 1.9700811945743243e-05,
      "loss": 3.3522,
      "step": 2200
    },
    {
      "epoch": 0.046960818343304,
      "grad_norm": 12.120246887207031,
      "learning_rate": 1.9687200114339385e-05,
      "loss": 3.3913,
      "step": 2300
    },
    {
      "epoch": 0.04900259305388244,
      "grad_norm": 11.259878158569336,
      "learning_rate": 1.9673588282935528e-05,
      "loss": 3.307,
      "step": 2400
    },
    {
      "epoch": 0.05104436776446087,
      "grad_norm": 13.106424331665039,
      "learning_rate": 1.9659976451531674e-05,
      "loss": 3.3496,
      "step": 2500
    },
    {
      "epoch": 0.0530861424750393,
      "grad_norm": 12.161965370178223,
      "learning_rate": 1.9646364620127817e-05,
      "loss": 3.356,
      "step": 2600
    },
    {
      "epoch": 0.05512791718561774,
      "grad_norm": 11.973413467407227,
      "learning_rate": 1.963275278872396e-05,
      "loss": 3.3472,
      "step": 2700
    },
    {
      "epoch": 0.057169691896196174,
      "grad_norm": 12.420760154724121,
      "learning_rate": 1.9619140957320106e-05,
      "loss": 3.3728,
      "step": 2800
    },
    {
      "epoch": 0.05921146660677461,
      "grad_norm": 11.700454711914062,
      "learning_rate": 1.960552912591625e-05,
      "loss": 3.3802,
      "step": 2900
    },
    {
      "epoch": 0.06125324131735305,
      "grad_norm": 12.17754077911377,
      "learning_rate": 1.959191729451239e-05,
      "loss": 3.2787,
      "step": 3000
    },
    {
      "epoch": 0.06329501602793147,
      "grad_norm": 11.458930969238281,
      "learning_rate": 1.9578305463108534e-05,
      "loss": 3.3054,
      "step": 3100
    },
    {
      "epoch": 0.06533679073850991,
      "grad_norm": 13.307306289672852,
      "learning_rate": 1.956469363170468e-05,
      "loss": 3.2802,
      "step": 3200
    },
    {
      "epoch": 0.06737856544908835,
      "grad_norm": 12.39387321472168,
      "learning_rate": 1.9551081800300823e-05,
      "loss": 3.3154,
      "step": 3300
    },
    {
      "epoch": 0.06942034015966678,
      "grad_norm": 11.059921264648438,
      "learning_rate": 1.9537469968896966e-05,
      "loss": 3.3098,
      "step": 3400
    },
    {
      "epoch": 0.07146211487024522,
      "grad_norm": 11.09432315826416,
      "learning_rate": 1.9523858137493112e-05,
      "loss": 3.3002,
      "step": 3500
    },
    {
      "epoch": 0.07350388958082366,
      "grad_norm": 13.593613624572754,
      "learning_rate": 1.9510246306089254e-05,
      "loss": 3.3263,
      "step": 3600
    },
    {
      "epoch": 0.07554566429140208,
      "grad_norm": 12.131340026855469,
      "learning_rate": 1.9496634474685397e-05,
      "loss": 3.2499,
      "step": 3700
    },
    {
      "epoch": 0.07758743900198052,
      "grad_norm": 10.772418975830078,
      "learning_rate": 1.9483022643281543e-05,
      "loss": 3.3036,
      "step": 3800
    },
    {
      "epoch": 0.07962921371255896,
      "grad_norm": 12.189437866210938,
      "learning_rate": 1.9469410811877686e-05,
      "loss": 3.3228,
      "step": 3900
    },
    {
      "epoch": 0.08167098842313739,
      "grad_norm": 11.730237007141113,
      "learning_rate": 1.945579898047383e-05,
      "loss": 3.3401,
      "step": 4000
    },
    {
      "epoch": 0.08371276313371583,
      "grad_norm": 11.052814483642578,
      "learning_rate": 1.944218714906997e-05,
      "loss": 3.2459,
      "step": 4100
    },
    {
      "epoch": 0.08575453784429427,
      "grad_norm": 10.284501075744629,
      "learning_rate": 1.9428575317666118e-05,
      "loss": 3.2386,
      "step": 4200
    },
    {
      "epoch": 0.08779631255487269,
      "grad_norm": 12.918484687805176,
      "learning_rate": 1.941496348626226e-05,
      "loss": 3.2711,
      "step": 4300
    },
    {
      "epoch": 0.08983808726545113,
      "grad_norm": 11.466767311096191,
      "learning_rate": 1.9401351654858403e-05,
      "loss": 3.2695,
      "step": 4400
    },
    {
      "epoch": 0.09187986197602957,
      "grad_norm": 12.378633499145508,
      "learning_rate": 1.938773982345455e-05,
      "loss": 3.3027,
      "step": 4500
    },
    {
      "epoch": 0.093921636686608,
      "grad_norm": 12.194940567016602,
      "learning_rate": 1.9374127992050692e-05,
      "loss": 3.3226,
      "step": 4600
    },
    {
      "epoch": 0.09596341139718643,
      "grad_norm": 11.886343002319336,
      "learning_rate": 1.9360516160646835e-05,
      "loss": 3.2553,
      "step": 4700
    },
    {
      "epoch": 0.09800518610776487,
      "grad_norm": 12.502863883972168,
      "learning_rate": 1.9347040447557016e-05,
      "loss": 3.2581,
      "step": 4800
    },
    {
      "epoch": 0.1000469608183433,
      "grad_norm": 11.692793846130371,
      "learning_rate": 1.9333428616153162e-05,
      "loss": 3.1892,
      "step": 4900
    },
    {
      "epoch": 0.10208873552892174,
      "grad_norm": 12.94906234741211,
      "learning_rate": 1.9319816784749305e-05,
      "loss": 3.2507,
      "step": 5000
    },
    {
      "epoch": 0.10413051023950018,
      "grad_norm": 10.604422569274902,
      "learning_rate": 1.9306204953345448e-05,
      "loss": 3.2317,
      "step": 5100
    },
    {
      "epoch": 0.1061722849500786,
      "grad_norm": 14.126296997070312,
      "learning_rate": 1.9292593121941594e-05,
      "loss": 3.2288,
      "step": 5200
    },
    {
      "epoch": 0.10821405966065704,
      "grad_norm": 11.025579452514648,
      "learning_rate": 1.9278981290537736e-05,
      "loss": 3.2064,
      "step": 5300
    },
    {
      "epoch": 0.11025583437123548,
      "grad_norm": 12.134109497070312,
      "learning_rate": 1.926536945913388e-05,
      "loss": 3.2273,
      "step": 5400
    },
    {
      "epoch": 0.11229760908181391,
      "grad_norm": 10.464494705200195,
      "learning_rate": 1.9251757627730025e-05,
      "loss": 3.2618,
      "step": 5500
    },
    {
      "epoch": 0.11433938379239235,
      "grad_norm": 11.406311988830566,
      "learning_rate": 1.9238145796326168e-05,
      "loss": 3.245,
      "step": 5600
    },
    {
      "epoch": 0.11638115850297079,
      "grad_norm": 12.758875846862793,
      "learning_rate": 1.922453396492231e-05,
      "loss": 3.2267,
      "step": 5700
    },
    {
      "epoch": 0.11842293321354921,
      "grad_norm": 11.72424030303955,
      "learning_rate": 1.9210922133518453e-05,
      "loss": 3.171,
      "step": 5800
    },
    {
      "epoch": 0.12046470792412765,
      "grad_norm": 12.862680435180664,
      "learning_rate": 1.91973103021146e-05,
      "loss": 3.1561,
      "step": 5900
    },
    {
      "epoch": 0.1225064826347061,
      "grad_norm": 9.683858871459961,
      "learning_rate": 1.9183698470710742e-05,
      "loss": 3.2133,
      "step": 6000
    },
    {
      "epoch": 0.12454825734528452,
      "grad_norm": 10.746879577636719,
      "learning_rate": 1.9170086639306885e-05,
      "loss": 3.203,
      "step": 6100
    },
    {
      "epoch": 0.12659003205586294,
      "grad_norm": 12.088369369506836,
      "learning_rate": 1.915647480790303e-05,
      "loss": 3.225,
      "step": 6200
    },
    {
      "epoch": 0.1286318067664414,
      "grad_norm": 11.563100814819336,
      "learning_rate": 1.9142862976499174e-05,
      "loss": 3.1365,
      "step": 6300
    },
    {
      "epoch": 0.13067358147701982,
      "grad_norm": 10.434185981750488,
      "learning_rate": 1.9129251145095317e-05,
      "loss": 3.1665,
      "step": 6400
    },
    {
      "epoch": 0.13271535618759825,
      "grad_norm": 11.186139106750488,
      "learning_rate": 1.9115639313691463e-05,
      "loss": 3.1238,
      "step": 6500
    },
    {
      "epoch": 0.1347571308981767,
      "grad_norm": 12.526570320129395,
      "learning_rate": 1.9102027482287606e-05,
      "loss": 3.2382,
      "step": 6600
    },
    {
      "epoch": 0.13679890560875513,
      "grad_norm": 12.100378036499023,
      "learning_rate": 1.908841565088375e-05,
      "loss": 3.1861,
      "step": 6700
    },
    {
      "epoch": 0.13884068031933355,
      "grad_norm": 10.982444763183594,
      "learning_rate": 1.9074803819479894e-05,
      "loss": 3.1968,
      "step": 6800
    },
    {
      "epoch": 0.140882455029912,
      "grad_norm": 13.492076873779297,
      "learning_rate": 1.9061191988076037e-05,
      "loss": 3.1828,
      "step": 6900
    },
    {
      "epoch": 0.14292422974049043,
      "grad_norm": 14.115434646606445,
      "learning_rate": 1.904771627498622e-05,
      "loss": 3.2111,
      "step": 7000
    },
    {
      "epoch": 0.14496600445106886,
      "grad_norm": 12.68421459197998,
      "learning_rate": 1.9034104443582365e-05,
      "loss": 3.1848,
      "step": 7100
    },
    {
      "epoch": 0.1470077791616473,
      "grad_norm": 14.017461776733398,
      "learning_rate": 1.9020492612178507e-05,
      "loss": 3.1638,
      "step": 7200
    },
    {
      "epoch": 0.14904955387222574,
      "grad_norm": 11.71094799041748,
      "learning_rate": 1.900688078077465e-05,
      "loss": 3.1088,
      "step": 7300
    },
    {
      "epoch": 0.15109132858280416,
      "grad_norm": 12.00625991821289,
      "learning_rate": 1.8993268949370796e-05,
      "loss": 3.1586,
      "step": 7400
    },
    {
      "epoch": 0.15313310329338262,
      "grad_norm": 10.699090003967285,
      "learning_rate": 1.897965711796694e-05,
      "loss": 3.1937,
      "step": 7500
    },
    {
      "epoch": 0.15517487800396104,
      "grad_norm": 10.434721946716309,
      "learning_rate": 1.896604528656308e-05,
      "loss": 3.1782,
      "step": 7600
    },
    {
      "epoch": 0.15721665271453947,
      "grad_norm": 9.843338012695312,
      "learning_rate": 1.8952433455159228e-05,
      "loss": 3.1602,
      "step": 7700
    },
    {
      "epoch": 0.15925842742511792,
      "grad_norm": 13.97711181640625,
      "learning_rate": 1.893882162375537e-05,
      "loss": 3.1827,
      "step": 7800
    },
    {
      "epoch": 0.16130020213569635,
      "grad_norm": 11.65129566192627,
      "learning_rate": 1.8925209792351513e-05,
      "loss": 3.154,
      "step": 7900
    },
    {
      "epoch": 0.16334197684627477,
      "grad_norm": 12.531818389892578,
      "learning_rate": 1.8911597960947656e-05,
      "loss": 3.1567,
      "step": 8000
    },
    {
      "epoch": 0.16538375155685323,
      "grad_norm": 12.932938575744629,
      "learning_rate": 1.8897986129543802e-05,
      "loss": 3.199,
      "step": 8100
    },
    {
      "epoch": 0.16742552626743165,
      "grad_norm": 9.000666618347168,
      "learning_rate": 1.8884374298139945e-05,
      "loss": 3.1578,
      "step": 8200
    },
    {
      "epoch": 0.16946730097801008,
      "grad_norm": 10.890174865722656,
      "learning_rate": 1.8870762466736088e-05,
      "loss": 3.169,
      "step": 8300
    },
    {
      "epoch": 0.17150907568858853,
      "grad_norm": 11.919278144836426,
      "learning_rate": 1.8857150635332234e-05,
      "loss": 3.1537,
      "step": 8400
    },
    {
      "epoch": 0.17355085039916696,
      "grad_norm": 10.915824890136719,
      "learning_rate": 1.8843538803928376e-05,
      "loss": 3.1485,
      "step": 8500
    },
    {
      "epoch": 0.17559262510974538,
      "grad_norm": 12.864474296569824,
      "learning_rate": 1.882992697252452e-05,
      "loss": 3.1503,
      "step": 8600
    },
    {
      "epoch": 0.17763439982032384,
      "grad_norm": 11.53228759765625,
      "learning_rate": 1.8816315141120665e-05,
      "loss": 3.1349,
      "step": 8700
    },
    {
      "epoch": 0.17967617453090226,
      "grad_norm": 12.364424705505371,
      "learning_rate": 1.8802703309716808e-05,
      "loss": 3.1715,
      "step": 8800
    },
    {
      "epoch": 0.1817179492414807,
      "grad_norm": 10.611665725708008,
      "learning_rate": 1.878909147831295e-05,
      "loss": 3.1033,
      "step": 8900
    },
    {
      "epoch": 0.18375972395205914,
      "grad_norm": 10.393767356872559,
      "learning_rate": 1.8775479646909093e-05,
      "loss": 3.1261,
      "step": 9000
    },
    {
      "epoch": 0.18580149866263757,
      "grad_norm": 11.841957092285156,
      "learning_rate": 1.8762003933819278e-05,
      "loss": 3.1465,
      "step": 9100
    },
    {
      "epoch": 0.187843273373216,
      "grad_norm": 13.529349327087402,
      "learning_rate": 1.874839210241542e-05,
      "loss": 3.144,
      "step": 9200
    },
    {
      "epoch": 0.18988504808379444,
      "grad_norm": 11.451804161071777,
      "learning_rate": 1.8734780271011567e-05,
      "loss": 3.1195,
      "step": 9300
    },
    {
      "epoch": 0.19192682279437287,
      "grad_norm": 12.557164192199707,
      "learning_rate": 1.872116843960771e-05,
      "loss": 3.1303,
      "step": 9400
    },
    {
      "epoch": 0.1939685975049513,
      "grad_norm": 10.861104011535645,
      "learning_rate": 1.8707556608203852e-05,
      "loss": 3.0975,
      "step": 9500
    },
    {
      "epoch": 0.19601037221552975,
      "grad_norm": 9.615384101867676,
      "learning_rate": 1.86939447768e-05,
      "loss": 3.087,
      "step": 9600
    },
    {
      "epoch": 0.19805214692610817,
      "grad_norm": 11.2908296585083,
      "learning_rate": 1.868033294539614e-05,
      "loss": 3.1113,
      "step": 9700
    },
    {
      "epoch": 0.2000939216366866,
      "grad_norm": 13.386640548706055,
      "learning_rate": 1.8666721113992284e-05,
      "loss": 3.136,
      "step": 9800
    },
    {
      "epoch": 0.20213569634726505,
      "grad_norm": 10.644759178161621,
      "learning_rate": 1.8653109282588427e-05,
      "loss": 3.1315,
      "step": 9900
    },
    {
      "epoch": 0.20417747105784348,
      "grad_norm": 13.24881649017334,
      "learning_rate": 1.8639497451184573e-05,
      "loss": 3.0808,
      "step": 10000
    },
    {
      "epoch": 0.2062192457684219,
      "grad_norm": 11.1624116897583,
      "learning_rate": 1.8625885619780716e-05,
      "loss": 3.1722,
      "step": 10100
    },
    {
      "epoch": 0.20826102047900036,
      "grad_norm": 12.265604972839355,
      "learning_rate": 1.861227378837686e-05,
      "loss": 3.0541,
      "step": 10200
    },
    {
      "epoch": 0.21030279518957878,
      "grad_norm": 11.237542152404785,
      "learning_rate": 1.8598661956973005e-05,
      "loss": 3.0825,
      "step": 10300
    },
    {
      "epoch": 0.2123445699001572,
      "grad_norm": 12.134686470031738,
      "learning_rate": 1.8585050125569147e-05,
      "loss": 3.0817,
      "step": 10400
    },
    {
      "epoch": 0.21438634461073566,
      "grad_norm": 11.07719612121582,
      "learning_rate": 1.857143829416529e-05,
      "loss": 3.1236,
      "step": 10500
    },
    {
      "epoch": 0.2164281193213141,
      "grad_norm": 12.196582794189453,
      "learning_rate": 1.8557826462761436e-05,
      "loss": 3.0672,
      "step": 10600
    },
    {
      "epoch": 0.21846989403189251,
      "grad_norm": 10.568984031677246,
      "learning_rate": 1.854421463135758e-05,
      "loss": 3.0985,
      "step": 10700
    },
    {
      "epoch": 0.22051166874247097,
      "grad_norm": 12.263212203979492,
      "learning_rate": 1.853060279995372e-05,
      "loss": 3.1339,
      "step": 10800
    },
    {
      "epoch": 0.2225534434530494,
      "grad_norm": 8.978233337402344,
      "learning_rate": 1.8516990968549868e-05,
      "loss": 3.0883,
      "step": 10900
    },
    {
      "epoch": 0.22459521816362782,
      "grad_norm": 12.055773735046387,
      "learning_rate": 1.850337913714601e-05,
      "loss": 3.0837,
      "step": 11000
    },
    {
      "epoch": 0.22663699287420627,
      "grad_norm": 10.172821998596191,
      "learning_rate": 1.8489767305742153e-05,
      "loss": 3.074,
      "step": 11100
    },
    {
      "epoch": 0.2286787675847847,
      "grad_norm": 10.51671028137207,
      "learning_rate": 1.8476155474338296e-05,
      "loss": 3.0787,
      "step": 11200
    },
    {
      "epoch": 0.23072054229536312,
      "grad_norm": 10.906469345092773,
      "learning_rate": 1.8462543642934442e-05,
      "loss": 3.0143,
      "step": 11300
    },
    {
      "epoch": 0.23276231700594158,
      "grad_norm": 10.728559494018555,
      "learning_rate": 1.8448931811530585e-05,
      "loss": 3.0856,
      "step": 11400
    },
    {
      "epoch": 0.23480409171652,
      "grad_norm": 10.902558326721191,
      "learning_rate": 1.8435319980126728e-05,
      "loss": 3.0263,
      "step": 11500
    },
    {
      "epoch": 0.23684586642709843,
      "grad_norm": 12.37532901763916,
      "learning_rate": 1.842184426703691e-05,
      "loss": 3.0421,
      "step": 11600
    },
    {
      "epoch": 0.23888764113767688,
      "grad_norm": 10.023582458496094,
      "learning_rate": 1.8408232435633055e-05,
      "loss": 3.0874,
      "step": 11700
    },
    {
      "epoch": 0.2409294158482553,
      "grad_norm": 11.233746528625488,
      "learning_rate": 1.8394620604229198e-05,
      "loss": 3.0829,
      "step": 11800
    },
    {
      "epoch": 0.24297119055883373,
      "grad_norm": 10.21651554107666,
      "learning_rate": 1.838100877282534e-05,
      "loss": 3.1309,
      "step": 11900
    },
    {
      "epoch": 0.2450129652694122,
      "grad_norm": 9.95112133026123,
      "learning_rate": 1.836753305973552e-05,
      "loss": 3.0739,
      "step": 12000
    },
    {
      "epoch": 0.2470547399799906,
      "grad_norm": 12.846505165100098,
      "learning_rate": 1.8353921228331668e-05,
      "loss": 3.088,
      "step": 12100
    },
    {
      "epoch": 0.24909651469056904,
      "grad_norm": 11.705755233764648,
      "learning_rate": 1.834030939692781e-05,
      "loss": 3.0275,
      "step": 12200
    },
    {
      "epoch": 0.25113828940114746,
      "grad_norm": 13.794473648071289,
      "learning_rate": 1.8326697565523953e-05,
      "loss": 3.1064,
      "step": 12300
    },
    {
      "epoch": 0.2531800641117259,
      "grad_norm": 13.002275466918945,
      "learning_rate": 1.83130857341201e-05,
      "loss": 3.0654,
      "step": 12400
    },
    {
      "epoch": 0.25522183882230437,
      "grad_norm": 10.72122859954834,
      "learning_rate": 1.8299473902716242e-05,
      "loss": 3.0838,
      "step": 12500
    },
    {
      "epoch": 0.2572636135328828,
      "grad_norm": 13.257104873657227,
      "learning_rate": 1.8285862071312385e-05,
      "loss": 3.1064,
      "step": 12600
    },
    {
      "epoch": 0.2593053882434612,
      "grad_norm": 10.28453540802002,
      "learning_rate": 1.827225023990853e-05,
      "loss": 3.082,
      "step": 12700
    },
    {
      "epoch": 0.26134716295403965,
      "grad_norm": 11.663492202758789,
      "learning_rate": 1.8258638408504674e-05,
      "loss": 3.0893,
      "step": 12800
    },
    {
      "epoch": 0.2633889376646181,
      "grad_norm": 13.075067520141602,
      "learning_rate": 1.8245026577100816e-05,
      "loss": 3.0576,
      "step": 12900
    },
    {
      "epoch": 0.2654307123751965,
      "grad_norm": 12.107151985168457,
      "learning_rate": 1.823141474569696e-05,
      "loss": 3.105,
      "step": 13000
    },
    {
      "epoch": 0.267472487085775,
      "grad_norm": 10.732147216796875,
      "learning_rate": 1.8217802914293105e-05,
      "loss": 3.0609,
      "step": 13100
    },
    {
      "epoch": 0.2695142617963534,
      "grad_norm": 10.747770309448242,
      "learning_rate": 1.8204191082889248e-05,
      "loss": 3.0892,
      "step": 13200
    },
    {
      "epoch": 0.27155603650693183,
      "grad_norm": 10.842411041259766,
      "learning_rate": 1.819057925148539e-05,
      "loss": 3.1038,
      "step": 13300
    },
    {
      "epoch": 0.27359781121751026,
      "grad_norm": 11.386565208435059,
      "learning_rate": 1.8176967420081537e-05,
      "loss": 3.021,
      "step": 13400
    },
    {
      "epoch": 0.2756395859280887,
      "grad_norm": 10.597610473632812,
      "learning_rate": 1.816335558867768e-05,
      "loss": 3.0574,
      "step": 13500
    },
    {
      "epoch": 0.2776813606386671,
      "grad_norm": 11.407858848571777,
      "learning_rate": 1.8149743757273822e-05,
      "loss": 3.0273,
      "step": 13600
    },
    {
      "epoch": 0.2797231353492456,
      "grad_norm": 11.26596736907959,
      "learning_rate": 1.813613192586997e-05,
      "loss": 3.0262,
      "step": 13700
    },
    {
      "epoch": 0.281764910059824,
      "grad_norm": 10.296380043029785,
      "learning_rate": 1.812252009446611e-05,
      "loss": 3.0427,
      "step": 13800
    },
    {
      "epoch": 0.28380668477040244,
      "grad_norm": 11.27812671661377,
      "learning_rate": 1.8108908263062254e-05,
      "loss": 3.1189,
      "step": 13900
    },
    {
      "epoch": 0.28584845948098087,
      "grad_norm": 12.045595169067383,
      "learning_rate": 1.8095296431658397e-05,
      "loss": 3.0066,
      "step": 14000
    },
    {
      "epoch": 0.2878902341915593,
      "grad_norm": 12.849969863891602,
      "learning_rate": 1.8081684600254543e-05,
      "loss": 3.0822,
      "step": 14100
    },
    {
      "epoch": 0.2899320089021377,
      "grad_norm": 10.321910858154297,
      "learning_rate": 1.8068072768850686e-05,
      "loss": 3.0415,
      "step": 14200
    },
    {
      "epoch": 0.2919737836127162,
      "grad_norm": 10.7036771774292,
      "learning_rate": 1.8054460937446828e-05,
      "loss": 3.0518,
      "step": 14300
    },
    {
      "epoch": 0.2940155583232946,
      "grad_norm": 9.15526294708252,
      "learning_rate": 1.8040849106042974e-05,
      "loss": 3.0759,
      "step": 14400
    },
    {
      "epoch": 0.29605733303387305,
      "grad_norm": 13.750367164611816,
      "learning_rate": 1.8027237274639117e-05,
      "loss": 3.0112,
      "step": 14500
    },
    {
      "epoch": 0.2980991077444515,
      "grad_norm": 11.553192138671875,
      "learning_rate": 1.801362544323526e-05,
      "loss": 2.9979,
      "step": 14600
    },
    {
      "epoch": 0.3001408824550299,
      "grad_norm": 10.52016830444336,
      "learning_rate": 1.8000013611831406e-05,
      "loss": 3.0054,
      "step": 14700
    },
    {
      "epoch": 0.3021826571656083,
      "grad_norm": 18.674070358276367,
      "learning_rate": 1.798640178042755e-05,
      "loss": 3.1086,
      "step": 14800
    },
    {
      "epoch": 0.3042244318761868,
      "grad_norm": 11.034729957580566,
      "learning_rate": 1.797278994902369e-05,
      "loss": 3.047,
      "step": 14900
    },
    {
      "epoch": 0.30626620658676523,
      "grad_norm": 10.183252334594727,
      "learning_rate": 1.7959178117619838e-05,
      "loss": 3.0336,
      "step": 15000
    },
    {
      "epoch": 0.30830798129734366,
      "grad_norm": 11.303783416748047,
      "learning_rate": 1.794556628621598e-05,
      "loss": 3.0165,
      "step": 15100
    },
    {
      "epoch": 0.3103497560079221,
      "grad_norm": 12.005227088928223,
      "learning_rate": 1.7931954454812123e-05,
      "loss": 3.0067,
      "step": 15200
    },
    {
      "epoch": 0.3123915307185005,
      "grad_norm": 10.464094161987305,
      "learning_rate": 1.7918342623408266e-05,
      "loss": 3.0533,
      "step": 15300
    },
    {
      "epoch": 0.31443330542907894,
      "grad_norm": 12.14592456817627,
      "learning_rate": 1.7904730792004412e-05,
      "loss": 3.0549,
      "step": 15400
    },
    {
      "epoch": 0.3164750801396574,
      "grad_norm": 10.312705039978027,
      "learning_rate": 1.7891118960600555e-05,
      "loss": 3.0303,
      "step": 15500
    },
    {
      "epoch": 0.31851685485023584,
      "grad_norm": 11.4214448928833,
      "learning_rate": 1.7877507129196697e-05,
      "loss": 3.0495,
      "step": 15600
    },
    {
      "epoch": 0.32055862956081427,
      "grad_norm": 11.368277549743652,
      "learning_rate": 1.7863895297792844e-05,
      "loss": 3.0392,
      "step": 15700
    },
    {
      "epoch": 0.3226004042713927,
      "grad_norm": 8.945239067077637,
      "learning_rate": 1.7850283466388986e-05,
      "loss": 3.0193,
      "step": 15800
    },
    {
      "epoch": 0.3246421789819711,
      "grad_norm": 10.19482421875,
      "learning_rate": 1.783667163498513e-05,
      "loss": 3.0164,
      "step": 15900
    },
    {
      "epoch": 0.32668395369254954,
      "grad_norm": 12.130037307739258,
      "learning_rate": 1.7823195921895314e-05,
      "loss": 3.0373,
      "step": 16000
    },
    {
      "epoch": 0.328725728403128,
      "grad_norm": 10.517560005187988,
      "learning_rate": 1.7809584090491456e-05,
      "loss": 3.0526,
      "step": 16100
    },
    {
      "epoch": 0.33076750311370645,
      "grad_norm": 9.463882446289062,
      "learning_rate": 1.77959722590876e-05,
      "loss": 3.0537,
      "step": 16200
    },
    {
      "epoch": 0.3328092778242849,
      "grad_norm": 10.832091331481934,
      "learning_rate": 1.7782360427683745e-05,
      "loss": 2.9851,
      "step": 16300
    },
    {
      "epoch": 0.3348510525348633,
      "grad_norm": 11.161176681518555,
      "learning_rate": 1.7768748596279888e-05,
      "loss": 3.0255,
      "step": 16400
    },
    {
      "epoch": 0.33689282724544173,
      "grad_norm": 9.86934757232666,
      "learning_rate": 1.775513676487603e-05,
      "loss": 3.0101,
      "step": 16500
    },
    {
      "epoch": 0.33893460195602015,
      "grad_norm": 10.494874000549316,
      "learning_rate": 1.7741524933472177e-05,
      "loss": 2.9562,
      "step": 16600
    },
    {
      "epoch": 0.34097637666659864,
      "grad_norm": 12.358827590942383,
      "learning_rate": 1.772791310206832e-05,
      "loss": 3.0246,
      "step": 16700
    },
    {
      "epoch": 0.34301815137717706,
      "grad_norm": 11.350135803222656,
      "learning_rate": 1.7714301270664462e-05,
      "loss": 2.9663,
      "step": 16800
    },
    {
      "epoch": 0.3450599260877555,
      "grad_norm": 10.381853103637695,
      "learning_rate": 1.770068943926061e-05,
      "loss": 3.0464,
      "step": 16900
    },
    {
      "epoch": 0.3471017007983339,
      "grad_norm": 10.21146297454834,
      "learning_rate": 1.768707760785675e-05,
      "loss": 3.038,
      "step": 17000
    },
    {
      "epoch": 0.34914347550891234,
      "grad_norm": 11.863410949707031,
      "learning_rate": 1.7673465776452894e-05,
      "loss": 3.0052,
      "step": 17100
    },
    {
      "epoch": 0.35118525021949076,
      "grad_norm": 11.387441635131836,
      "learning_rate": 1.765985394504904e-05,
      "loss": 3.0223,
      "step": 17200
    },
    {
      "epoch": 0.3532270249300692,
      "grad_norm": 11.016270637512207,
      "learning_rate": 1.7646242113645183e-05,
      "loss": 3.0457,
      "step": 17300
    },
    {
      "epoch": 0.35526879964064767,
      "grad_norm": 13.615669250488281,
      "learning_rate": 1.7632630282241326e-05,
      "loss": 3.058,
      "step": 17400
    },
    {
      "epoch": 0.3573105743512261,
      "grad_norm": 12.530987739562988,
      "learning_rate": 1.761915456915151e-05,
      "loss": 2.9473,
      "step": 17500
    },
    {
      "epoch": 0.3593523490618045,
      "grad_norm": 10.289320945739746,
      "learning_rate": 1.7605542737747653e-05,
      "loss": 3.0186,
      "step": 17600
    },
    {
      "epoch": 0.36139412377238295,
      "grad_norm": 9.94892406463623,
      "learning_rate": 1.7591930906343796e-05,
      "loss": 3.004,
      "step": 17700
    },
    {
      "epoch": 0.3634358984829614,
      "grad_norm": 10.673909187316895,
      "learning_rate": 1.7578319074939942e-05,
      "loss": 3.0681,
      "step": 17800
    },
    {
      "epoch": 0.3654776731935398,
      "grad_norm": 11.243976593017578,
      "learning_rate": 1.7564707243536084e-05,
      "loss": 2.9965,
      "step": 17900
    },
    {
      "epoch": 0.3675194479041183,
      "grad_norm": 11.541465759277344,
      "learning_rate": 1.7551095412132227e-05,
      "loss": 2.9639,
      "step": 18000
    },
    {
      "epoch": 0.3695612226146967,
      "grad_norm": 9.203558921813965,
      "learning_rate": 1.753748358072837e-05,
      "loss": 2.9908,
      "step": 18100
    },
    {
      "epoch": 0.37160299732527513,
      "grad_norm": 11.366724967956543,
      "learning_rate": 1.7523871749324516e-05,
      "loss": 2.9661,
      "step": 18200
    },
    {
      "epoch": 0.37364477203585356,
      "grad_norm": 13.015400886535645,
      "learning_rate": 1.751025991792066e-05,
      "loss": 2.9761,
      "step": 18300
    },
    {
      "epoch": 0.375686546746432,
      "grad_norm": 11.172626495361328,
      "learning_rate": 1.74966480865168e-05,
      "loss": 3.0168,
      "step": 18400
    },
    {
      "epoch": 0.3777283214570104,
      "grad_norm": 11.254250526428223,
      "learning_rate": 1.7483172373426983e-05,
      "loss": 3.0323,
      "step": 18500
    },
    {
      "epoch": 0.3797700961675889,
      "grad_norm": 11.87668514251709,
      "learning_rate": 1.746956054202313e-05,
      "loss": 3.0028,
      "step": 18600
    },
    {
      "epoch": 0.3818118708781673,
      "grad_norm": 9.934775352478027,
      "learning_rate": 1.745594871061927e-05,
      "loss": 2.9745,
      "step": 18700
    },
    {
      "epoch": 0.38385364558874574,
      "grad_norm": 10.792128562927246,
      "learning_rate": 1.7442336879215414e-05,
      "loss": 2.9641,
      "step": 18800
    },
    {
      "epoch": 0.38589542029932417,
      "grad_norm": 10.001476287841797,
      "learning_rate": 1.742872504781156e-05,
      "loss": 3.0489,
      "step": 18900
    },
    {
      "epoch": 0.3879371950099026,
      "grad_norm": 11.648812294006348,
      "learning_rate": 1.7415113216407703e-05,
      "loss": 2.9691,
      "step": 19000
    },
    {
      "epoch": 0.389978969720481,
      "grad_norm": 11.966570854187012,
      "learning_rate": 1.7401501385003846e-05,
      "loss": 3.0088,
      "step": 19100
    },
    {
      "epoch": 0.3920207444310595,
      "grad_norm": 9.78542423248291,
      "learning_rate": 1.738788955359999e-05,
      "loss": 2.9468,
      "step": 19200
    },
    {
      "epoch": 0.3940625191416379,
      "grad_norm": 9.883970260620117,
      "learning_rate": 1.7374277722196135e-05,
      "loss": 2.9615,
      "step": 19300
    },
    {
      "epoch": 0.39610429385221635,
      "grad_norm": 10.280396461486816,
      "learning_rate": 1.7360665890792278e-05,
      "loss": 2.9056,
      "step": 19400
    },
    {
      "epoch": 0.3981460685627948,
      "grad_norm": 11.164278984069824,
      "learning_rate": 1.734705405938842e-05,
      "loss": 2.9446,
      "step": 19500
    },
    {
      "epoch": 0.4001878432733732,
      "grad_norm": 10.460676193237305,
      "learning_rate": 1.7333442227984566e-05,
      "loss": 2.9501,
      "step": 19600
    },
    {
      "epoch": 0.4022296179839516,
      "grad_norm": 10.89653491973877,
      "learning_rate": 1.731983039658071e-05,
      "loss": 2.9345,
      "step": 19700
    },
    {
      "epoch": 0.4042713926945301,
      "grad_norm": 10.461272239685059,
      "learning_rate": 1.7306218565176852e-05,
      "loss": 2.9421,
      "step": 19800
    },
    {
      "epoch": 0.40631316740510853,
      "grad_norm": 12.17791748046875,
      "learning_rate": 1.7292606733772998e-05,
      "loss": 3.0025,
      "step": 19900
    },
    {
      "epoch": 0.40835494211568696,
      "grad_norm": 9.264399528503418,
      "learning_rate": 1.727899490236914e-05,
      "loss": 2.957,
      "step": 20000
    },
    {
      "epoch": 0.4103967168262654,
      "grad_norm": 11.76488208770752,
      "learning_rate": 1.7265383070965284e-05,
      "loss": 3.0195,
      "step": 20100
    },
    {
      "epoch": 0.4124384915368438,
      "grad_norm": 11.237529754638672,
      "learning_rate": 1.7251771239561426e-05,
      "loss": 2.9941,
      "step": 20200
    },
    {
      "epoch": 0.41448026624742224,
      "grad_norm": 9.721699714660645,
      "learning_rate": 1.7238159408157572e-05,
      "loss": 2.9946,
      "step": 20300
    },
    {
      "epoch": 0.4165220409580007,
      "grad_norm": 12.728283882141113,
      "learning_rate": 1.7224547576753715e-05,
      "loss": 3.0058,
      "step": 20400
    },
    {
      "epoch": 0.41856381566857914,
      "grad_norm": 12.194106101989746,
      "learning_rate": 1.7210935745349858e-05,
      "loss": 2.9135,
      "step": 20500
    },
    {
      "epoch": 0.42060559037915757,
      "grad_norm": 10.692459106445312,
      "learning_rate": 1.7197323913946004e-05,
      "loss": 2.965,
      "step": 20600
    },
    {
      "epoch": 0.422647365089736,
      "grad_norm": 10.651568412780762,
      "learning_rate": 1.7183712082542147e-05,
      "loss": 2.9728,
      "step": 20700
    },
    {
      "epoch": 0.4246891398003144,
      "grad_norm": 10.415714263916016,
      "learning_rate": 1.717010025113829e-05,
      "loss": 2.9564,
      "step": 20800
    },
    {
      "epoch": 0.42673091451089284,
      "grad_norm": 9.441070556640625,
      "learning_rate": 1.7156488419734436e-05,
      "loss": 2.9566,
      "step": 20900
    },
    {
      "epoch": 0.4287726892214713,
      "grad_norm": 11.36829662322998,
      "learning_rate": 1.714287658833058e-05,
      "loss": 2.8735,
      "step": 21000
    },
    {
      "epoch": 0.43081446393204975,
      "grad_norm": 11.133100509643555,
      "learning_rate": 1.712926475692672e-05,
      "loss": 2.9137,
      "step": 21100
    },
    {
      "epoch": 0.4328562386426282,
      "grad_norm": 10.876919746398926,
      "learning_rate": 1.7115652925522864e-05,
      "loss": 3.0164,
      "step": 21200
    },
    {
      "epoch": 0.4348980133532066,
      "grad_norm": 10.327024459838867,
      "learning_rate": 1.710204109411901e-05,
      "loss": 2.9521,
      "step": 21300
    },
    {
      "epoch": 0.43693978806378503,
      "grad_norm": 11.79354476928711,
      "learning_rate": 1.7088429262715153e-05,
      "loss": 2.9693,
      "step": 21400
    },
    {
      "epoch": 0.43898156277436345,
      "grad_norm": 14.764076232910156,
      "learning_rate": 1.7074817431311295e-05,
      "loss": 2.9344,
      "step": 21500
    },
    {
      "epoch": 0.44102333748494194,
      "grad_norm": 8.70622444152832,
      "learning_rate": 1.706120559990744e-05,
      "loss": 2.9512,
      "step": 21600
    },
    {
      "epoch": 0.44306511219552036,
      "grad_norm": 9.956156730651855,
      "learning_rate": 1.7047593768503584e-05,
      "loss": 2.8944,
      "step": 21700
    },
    {
      "epoch": 0.4451068869060988,
      "grad_norm": 11.21473217010498,
      "learning_rate": 1.7033981937099727e-05,
      "loss": 2.9935,
      "step": 21800
    },
    {
      "epoch": 0.4471486616166772,
      "grad_norm": 10.86817741394043,
      "learning_rate": 1.7020370105695873e-05,
      "loss": 2.9517,
      "step": 21900
    },
    {
      "epoch": 0.44919043632725564,
      "grad_norm": 9.513376235961914,
      "learning_rate": 1.7006758274292016e-05,
      "loss": 2.8804,
      "step": 22000
    },
    {
      "epoch": 0.45123221103783406,
      "grad_norm": 12.96033000946045,
      "learning_rate": 1.699314644288816e-05,
      "loss": 2.9562,
      "step": 22100
    },
    {
      "epoch": 0.45327398574841254,
      "grad_norm": 10.772687911987305,
      "learning_rate": 1.69795346114843e-05,
      "loss": 2.9092,
      "step": 22200
    },
    {
      "epoch": 0.45531576045899097,
      "grad_norm": 9.460139274597168,
      "learning_rate": 1.6965922780080447e-05,
      "loss": 2.9113,
      "step": 22300
    },
    {
      "epoch": 0.4573575351695694,
      "grad_norm": 9.54603385925293,
      "learning_rate": 1.695231094867659e-05,
      "loss": 2.9696,
      "step": 22400
    },
    {
      "epoch": 0.4593993098801478,
      "grad_norm": 10.395402908325195,
      "learning_rate": 1.6938699117272733e-05,
      "loss": 2.9398,
      "step": 22500
    },
    {
      "epoch": 0.46144108459072625,
      "grad_norm": 10.859317779541016,
      "learning_rate": 1.692508728586888e-05,
      "loss": 2.9521,
      "step": 22600
    },
    {
      "epoch": 0.4634828593013047,
      "grad_norm": 9.856734275817871,
      "learning_rate": 1.6911475454465022e-05,
      "loss": 2.8986,
      "step": 22700
    },
    {
      "epoch": 0.46552463401188315,
      "grad_norm": 9.243953704833984,
      "learning_rate": 1.6897863623061165e-05,
      "loss": 2.9385,
      "step": 22800
    },
    {
      "epoch": 0.4675664087224616,
      "grad_norm": 10.856325149536133,
      "learning_rate": 1.6884251791657307e-05,
      "loss": 2.9477,
      "step": 22900
    },
    {
      "epoch": 0.46960818343304,
      "grad_norm": 12.580735206604004,
      "learning_rate": 1.6870639960253453e-05,
      "loss": 2.9879,
      "step": 23000
    },
    {
      "epoch": 0.47164995814361843,
      "grad_norm": 10.039619445800781,
      "learning_rate": 1.6857028128849596e-05,
      "loss": 2.9204,
      "step": 23100
    },
    {
      "epoch": 0.47369173285419686,
      "grad_norm": 11.87266731262207,
      "learning_rate": 1.684341629744574e-05,
      "loss": 2.9294,
      "step": 23200
    },
    {
      "epoch": 0.4757335075647753,
      "grad_norm": 9.428146362304688,
      "learning_rate": 1.6829804466041885e-05,
      "loss": 2.9758,
      "step": 23300
    },
    {
      "epoch": 0.47777528227535376,
      "grad_norm": 12.0653076171875,
      "learning_rate": 1.6816192634638028e-05,
      "loss": 2.9058,
      "step": 23400
    },
    {
      "epoch": 0.4798170569859322,
      "grad_norm": 13.150900840759277,
      "learning_rate": 1.680258080323417e-05,
      "loss": 2.9266,
      "step": 23500
    },
    {
      "epoch": 0.4818588316965106,
      "grad_norm": 12.159170150756836,
      "learning_rate": 1.6788968971830317e-05,
      "loss": 2.9516,
      "step": 23600
    },
    {
      "epoch": 0.48390060640708904,
      "grad_norm": 10.953200340270996,
      "learning_rate": 1.677535714042646e-05,
      "loss": 2.9318,
      "step": 23700
    },
    {
      "epoch": 0.48594238111766747,
      "grad_norm": 11.581472396850586,
      "learning_rate": 1.6761745309022602e-05,
      "loss": 2.9369,
      "step": 23800
    },
    {
      "epoch": 0.4879841558282459,
      "grad_norm": 13.871171951293945,
      "learning_rate": 1.6748133477618745e-05,
      "loss": 2.8869,
      "step": 23900
    },
    {
      "epoch": 0.4900259305388244,
      "grad_norm": 9.654268264770508,
      "learning_rate": 1.673452164621489e-05,
      "loss": 2.9752,
      "step": 24000
    },
    {
      "epoch": 0.4920677052494028,
      "grad_norm": 10.292203903198242,
      "learning_rate": 1.6720909814811034e-05,
      "loss": 2.9023,
      "step": 24100
    },
    {
      "epoch": 0.4941094799599812,
      "grad_norm": 11.651091575622559,
      "learning_rate": 1.6707297983407176e-05,
      "loss": 2.9172,
      "step": 24200
    },
    {
      "epoch": 0.49615125467055965,
      "grad_norm": 14.3043212890625,
      "learning_rate": 1.6693686152003323e-05,
      "loss": 2.9278,
      "step": 24300
    },
    {
      "epoch": 0.4981930293811381,
      "grad_norm": 10.53675365447998,
      "learning_rate": 1.6680074320599465e-05,
      "loss": 2.8955,
      "step": 24400
    },
    {
      "epoch": 0.5002348040917165,
      "grad_norm": 10.663664817810059,
      "learning_rate": 1.6666462489195608e-05,
      "loss": 2.9278,
      "step": 24500
    },
    {
      "epoch": 0.5022765788022949,
      "grad_norm": 10.79678726196289,
      "learning_rate": 1.6652850657791754e-05,
      "loss": 2.9426,
      "step": 24600
    },
    {
      "epoch": 0.5043183535128734,
      "grad_norm": 11.850491523742676,
      "learning_rate": 1.6639238826387897e-05,
      "loss": 2.9685,
      "step": 24700
    },
    {
      "epoch": 0.5063601282234518,
      "grad_norm": 10.322687149047852,
      "learning_rate": 1.662562699498404e-05,
      "loss": 2.9133,
      "step": 24800
    },
    {
      "epoch": 0.5084019029340303,
      "grad_norm": 9.262397766113281,
      "learning_rate": 1.6612015163580186e-05,
      "loss": 2.9638,
      "step": 24900
    },
    {
      "epoch": 0.5104436776446087,
      "grad_norm": 11.153912544250488,
      "learning_rate": 1.659840333217633e-05,
      "loss": 2.8695,
      "step": 25000
    },
    {
      "epoch": 0.5124854523551872,
      "grad_norm": 10.225614547729492,
      "learning_rate": 1.658479150077247e-05,
      "loss": 2.9585,
      "step": 25100
    },
    {
      "epoch": 0.5145272270657656,
      "grad_norm": 12.758119583129883,
      "learning_rate": 1.6571179669368614e-05,
      "loss": 2.921,
      "step": 25200
    },
    {
      "epoch": 0.516569001776344,
      "grad_norm": 10.885286331176758,
      "learning_rate": 1.655756783796476e-05,
      "loss": 2.924,
      "step": 25300
    },
    {
      "epoch": 0.5186107764869224,
      "grad_norm": 11.124924659729004,
      "learning_rate": 1.6543956006560903e-05,
      "loss": 2.8625,
      "step": 25400
    },
    {
      "epoch": 0.5206525511975009,
      "grad_norm": 9.866616249084473,
      "learning_rate": 1.6530344175157046e-05,
      "loss": 2.8746,
      "step": 25500
    },
    {
      "epoch": 0.5226943259080793,
      "grad_norm": 11.351943969726562,
      "learning_rate": 1.651686846206723e-05,
      "loss": 2.9179,
      "step": 25600
    },
    {
      "epoch": 0.5247361006186577,
      "grad_norm": 11.474202156066895,
      "learning_rate": 1.6503256630663373e-05,
      "loss": 2.887,
      "step": 25700
    },
    {
      "epoch": 0.5267778753292361,
      "grad_norm": 10.946676254272461,
      "learning_rate": 1.648964479925952e-05,
      "loss": 2.9823,
      "step": 25800
    },
    {
      "epoch": 0.5288196500398146,
      "grad_norm": 11.323273658752441,
      "learning_rate": 1.6476032967855662e-05,
      "loss": 2.9382,
      "step": 25900
    },
    {
      "epoch": 0.530861424750393,
      "grad_norm": 11.818479537963867,
      "learning_rate": 1.6462421136451805e-05,
      "loss": 2.9383,
      "step": 26000
    },
    {
      "epoch": 0.5329031994609714,
      "grad_norm": 10.02898120880127,
      "learning_rate": 1.6448809305047947e-05,
      "loss": 2.9041,
      "step": 26100
    },
    {
      "epoch": 0.53494497417155,
      "grad_norm": 10.56084156036377,
      "learning_rate": 1.6435197473644093e-05,
      "loss": 2.8838,
      "step": 26200
    },
    {
      "epoch": 0.5369867488821284,
      "grad_norm": 11.218070983886719,
      "learning_rate": 1.6421585642240236e-05,
      "loss": 2.8624,
      "step": 26300
    },
    {
      "epoch": 0.5390285235927068,
      "grad_norm": 11.203561782836914,
      "learning_rate": 1.640797381083638e-05,
      "loss": 2.9361,
      "step": 26400
    },
    {
      "epoch": 0.5410702983032852,
      "grad_norm": 10.917593955993652,
      "learning_rate": 1.6394361979432525e-05,
      "loss": 2.9318,
      "step": 26500
    },
    {
      "epoch": 0.5431120730138637,
      "grad_norm": 11.428699493408203,
      "learning_rate": 1.6380750148028668e-05,
      "loss": 2.8101,
      "step": 26600
    },
    {
      "epoch": 0.5451538477244421,
      "grad_norm": 10.611754417419434,
      "learning_rate": 1.636713831662481e-05,
      "loss": 2.8754,
      "step": 26700
    },
    {
      "epoch": 0.5471956224350205,
      "grad_norm": 10.094626426696777,
      "learning_rate": 1.6353526485220957e-05,
      "loss": 2.9001,
      "step": 26800
    },
    {
      "epoch": 0.5492373971455989,
      "grad_norm": 9.744180679321289,
      "learning_rate": 1.63399146538171e-05,
      "loss": 2.8991,
      "step": 26900
    },
    {
      "epoch": 0.5512791718561774,
      "grad_norm": 9.674714088439941,
      "learning_rate": 1.6326302822413242e-05,
      "loss": 2.8742,
      "step": 27000
    },
    {
      "epoch": 0.5533209465667558,
      "grad_norm": 11.408663749694824,
      "learning_rate": 1.6312690991009388e-05,
      "loss": 2.9276,
      "step": 27100
    },
    {
      "epoch": 0.5553627212773342,
      "grad_norm": 11.066883087158203,
      "learning_rate": 1.629907915960553e-05,
      "loss": 2.8439,
      "step": 27200
    },
    {
      "epoch": 0.5574044959879126,
      "grad_norm": 10.813597679138184,
      "learning_rate": 1.6285467328201674e-05,
      "loss": 2.9235,
      "step": 27300
    },
    {
      "epoch": 0.5594462706984912,
      "grad_norm": 8.692071914672852,
      "learning_rate": 1.6271855496797816e-05,
      "loss": 2.8647,
      "step": 27400
    },
    {
      "epoch": 0.5614880454090696,
      "grad_norm": 9.790146827697754,
      "learning_rate": 1.6258243665393963e-05,
      "loss": 2.8974,
      "step": 27500
    },
    {
      "epoch": 0.563529820119648,
      "grad_norm": 8.638358116149902,
      "learning_rate": 1.6244631833990105e-05,
      "loss": 2.8836,
      "step": 27600
    },
    {
      "epoch": 0.5655715948302265,
      "grad_norm": 10.198934555053711,
      "learning_rate": 1.6231020002586248e-05,
      "loss": 2.9267,
      "step": 27700
    },
    {
      "epoch": 0.5676133695408049,
      "grad_norm": 13.979935646057129,
      "learning_rate": 1.6217544289496433e-05,
      "loss": 2.9234,
      "step": 27800
    },
    {
      "epoch": 0.5696551442513833,
      "grad_norm": 11.78447437286377,
      "learning_rate": 1.6203932458092575e-05,
      "loss": 2.9638,
      "step": 27900
    },
    {
      "epoch": 0.5716969189619617,
      "grad_norm": 12.073144912719727,
      "learning_rate": 1.6190320626688718e-05,
      "loss": 2.8628,
      "step": 28000
    },
    {
      "epoch": 0.5737386936725402,
      "grad_norm": 11.048003196716309,
      "learning_rate": 1.6176708795284864e-05,
      "loss": 2.9115,
      "step": 28100
    },
    {
      "epoch": 0.5757804683831186,
      "grad_norm": 12.927958488464355,
      "learning_rate": 1.6163096963881007e-05,
      "loss": 2.8295,
      "step": 28200
    },
    {
      "epoch": 0.577822243093697,
      "grad_norm": 11.839422225952148,
      "learning_rate": 1.614948513247715e-05,
      "loss": 2.8837,
      "step": 28300
    },
    {
      "epoch": 0.5798640178042754,
      "grad_norm": 10.296939849853516,
      "learning_rate": 1.6135873301073296e-05,
      "loss": 2.8874,
      "step": 28400
    },
    {
      "epoch": 0.5819057925148539,
      "grad_norm": 12.150468826293945,
      "learning_rate": 1.612226146966944e-05,
      "loss": 2.8384,
      "step": 28500
    },
    {
      "epoch": 0.5839475672254324,
      "grad_norm": 9.93922233581543,
      "learning_rate": 1.610864963826558e-05,
      "loss": 2.8875,
      "step": 28600
    },
    {
      "epoch": 0.5859893419360108,
      "grad_norm": 11.078641891479492,
      "learning_rate": 1.6095037806861727e-05,
      "loss": 2.9199,
      "step": 28700
    },
    {
      "epoch": 0.5880311166465892,
      "grad_norm": 10.687119483947754,
      "learning_rate": 1.608142597545787e-05,
      "loss": 2.8419,
      "step": 28800
    },
    {
      "epoch": 0.5900728913571677,
      "grad_norm": 9.447918891906738,
      "learning_rate": 1.6067814144054013e-05,
      "loss": 2.9276,
      "step": 28900
    },
    {
      "epoch": 0.5921146660677461,
      "grad_norm": 11.839584350585938,
      "learning_rate": 1.605420231265016e-05,
      "loss": 2.8033,
      "step": 29000
    },
    {
      "epoch": 0.5941564407783245,
      "grad_norm": 9.775238990783691,
      "learning_rate": 1.6040590481246302e-05,
      "loss": 2.8918,
      "step": 29100
    },
    {
      "epoch": 0.596198215488903,
      "grad_norm": 12.167479515075684,
      "learning_rate": 1.6026978649842445e-05,
      "loss": 2.9029,
      "step": 29200
    },
    {
      "epoch": 0.5982399901994814,
      "grad_norm": 10.792597770690918,
      "learning_rate": 1.6013366818438587e-05,
      "loss": 2.8774,
      "step": 29300
    },
    {
      "epoch": 0.6002817649100598,
      "grad_norm": 10.921173095703125,
      "learning_rate": 1.5999754987034733e-05,
      "loss": 2.8418,
      "step": 29400
    },
    {
      "epoch": 0.6023235396206382,
      "grad_norm": 9.530993461608887,
      "learning_rate": 1.5986143155630876e-05,
      "loss": 2.8636,
      "step": 29500
    },
    {
      "epoch": 0.6043653143312167,
      "grad_norm": 12.211235046386719,
      "learning_rate": 1.597253132422702e-05,
      "loss": 2.9335,
      "step": 29600
    },
    {
      "epoch": 0.6064070890417951,
      "grad_norm": 10.122101783752441,
      "learning_rate": 1.5958919492823165e-05,
      "loss": 2.8999,
      "step": 29700
    },
    {
      "epoch": 0.6084488637523736,
      "grad_norm": 9.251431465148926,
      "learning_rate": 1.5945307661419308e-05,
      "loss": 2.9238,
      "step": 29800
    },
    {
      "epoch": 0.610490638462952,
      "grad_norm": 11.609719276428223,
      "learning_rate": 1.593169583001545e-05,
      "loss": 2.9217,
      "step": 29900
    },
    {
      "epoch": 0.6125324131735305,
      "grad_norm": 11.04615592956543,
      "learning_rate": 1.5918083998611597e-05,
      "loss": 2.9023,
      "step": 30000
    },
    {
      "epoch": 0.6145741878841089,
      "grad_norm": 10.411787986755371,
      "learning_rate": 1.590447216720774e-05,
      "loss": 2.9069,
      "step": 30100
    },
    {
      "epoch": 0.6166159625946873,
      "grad_norm": 10.438640594482422,
      "learning_rate": 1.5890860335803882e-05,
      "loss": 2.9434,
      "step": 30200
    },
    {
      "epoch": 0.6186577373052657,
      "grad_norm": 10.859437942504883,
      "learning_rate": 1.5877248504400028e-05,
      "loss": 2.8942,
      "step": 30300
    },
    {
      "epoch": 0.6206995120158442,
      "grad_norm": 11.11892032623291,
      "learning_rate": 1.586363667299617e-05,
      "loss": 2.8201,
      "step": 30400
    },
    {
      "epoch": 0.6227412867264226,
      "grad_norm": 9.777860641479492,
      "learning_rate": 1.5850024841592314e-05,
      "loss": 2.8508,
      "step": 30500
    },
    {
      "epoch": 0.624783061437001,
      "grad_norm": 11.447697639465332,
      "learning_rate": 1.5836413010188456e-05,
      "loss": 2.826,
      "step": 30600
    },
    {
      "epoch": 0.6268248361475794,
      "grad_norm": 11.560345649719238,
      "learning_rate": 1.5822801178784603e-05,
      "loss": 2.9201,
      "step": 30700
    },
    {
      "epoch": 0.6288666108581579,
      "grad_norm": 10.195927619934082,
      "learning_rate": 1.5809189347380745e-05,
      "loss": 2.791,
      "step": 30800
    },
    {
      "epoch": 0.6309083855687363,
      "grad_norm": 11.182127952575684,
      "learning_rate": 1.5795577515976888e-05,
      "loss": 2.8409,
      "step": 30900
    },
    {
      "epoch": 0.6329501602793148,
      "grad_norm": 11.12010669708252,
      "learning_rate": 1.5781965684573034e-05,
      "loss": 2.8429,
      "step": 31000
    },
    {
      "epoch": 0.6349919349898933,
      "grad_norm": 10.198144912719727,
      "learning_rate": 1.5768353853169177e-05,
      "loss": 2.8424,
      "step": 31100
    },
    {
      "epoch": 0.6370337097004717,
      "grad_norm": 11.401786804199219,
      "learning_rate": 1.575474202176532e-05,
      "loss": 2.8498,
      "step": 31200
    },
    {
      "epoch": 0.6390754844110501,
      "grad_norm": 11.557000160217285,
      "learning_rate": 1.5741130190361466e-05,
      "loss": 2.8302,
      "step": 31300
    },
    {
      "epoch": 0.6411172591216285,
      "grad_norm": 10.13392448425293,
      "learning_rate": 1.572751835895761e-05,
      "loss": 2.854,
      "step": 31400
    },
    {
      "epoch": 0.643159033832207,
      "grad_norm": 11.981274604797363,
      "learning_rate": 1.571390652755375e-05,
      "loss": 2.787,
      "step": 31500
    },
    {
      "epoch": 0.6452008085427854,
      "grad_norm": 12.462830543518066,
      "learning_rate": 1.5700430814463932e-05,
      "loss": 2.92,
      "step": 31600
    },
    {
      "epoch": 0.6472425832533638,
      "grad_norm": 12.423346519470215,
      "learning_rate": 1.568681898306008e-05,
      "loss": 2.9118,
      "step": 31700
    },
    {
      "epoch": 0.6492843579639422,
      "grad_norm": 13.290043830871582,
      "learning_rate": 1.567320715165622e-05,
      "loss": 2.8985,
      "step": 31800
    },
    {
      "epoch": 0.6513261326745207,
      "grad_norm": 10.79770278930664,
      "learning_rate": 1.5659595320252364e-05,
      "loss": 2.8203,
      "step": 31900
    },
    {
      "epoch": 0.6533679073850991,
      "grad_norm": 9.36889362335205,
      "learning_rate": 1.5645983488848507e-05,
      "loss": 2.8319,
      "step": 32000
    },
    {
      "epoch": 0.6554096820956775,
      "grad_norm": 10.005398750305176,
      "learning_rate": 1.5632371657444653e-05,
      "loss": 2.8999,
      "step": 32100
    },
    {
      "epoch": 0.657451456806256,
      "grad_norm": 12.346769332885742,
      "learning_rate": 1.5618759826040796e-05,
      "loss": 2.8842,
      "step": 32200
    },
    {
      "epoch": 0.6594932315168345,
      "grad_norm": 10.263240814208984,
      "learning_rate": 1.560514799463694e-05,
      "loss": 2.9194,
      "step": 32300
    },
    {
      "epoch": 0.6615350062274129,
      "grad_norm": 11.92155647277832,
      "learning_rate": 1.5591536163233085e-05,
      "loss": 2.9029,
      "step": 32400
    },
    {
      "epoch": 0.6635767809379913,
      "grad_norm": 9.273425102233887,
      "learning_rate": 1.5577924331829227e-05,
      "loss": 2.8309,
      "step": 32500
    },
    {
      "epoch": 0.6656185556485698,
      "grad_norm": 9.780994415283203,
      "learning_rate": 1.556431250042537e-05,
      "loss": 2.9055,
      "step": 32600
    },
    {
      "epoch": 0.6676603303591482,
      "grad_norm": 10.550680160522461,
      "learning_rate": 1.5550700669021516e-05,
      "loss": 2.8351,
      "step": 32700
    },
    {
      "epoch": 0.6697021050697266,
      "grad_norm": 11.044508934020996,
      "learning_rate": 1.553708883761766e-05,
      "loss": 2.824,
      "step": 32800
    },
    {
      "epoch": 0.671743879780305,
      "grad_norm": 11.266253471374512,
      "learning_rate": 1.55234770062138e-05,
      "loss": 2.9017,
      "step": 32900
    },
    {
      "epoch": 0.6737856544908835,
      "grad_norm": 11.37059497833252,
      "learning_rate": 1.5509865174809944e-05,
      "loss": 2.841,
      "step": 33000
    },
    {
      "epoch": 0.6758274292014619,
      "grad_norm": 13.075613021850586,
      "learning_rate": 1.549625334340609e-05,
      "loss": 2.9364,
      "step": 33100
    },
    {
      "epoch": 0.6778692039120403,
      "grad_norm": 10.694477081298828,
      "learning_rate": 1.5482641512002233e-05,
      "loss": 2.7832,
      "step": 33200
    },
    {
      "epoch": 0.6799109786226187,
      "grad_norm": 9.427566528320312,
      "learning_rate": 1.5469029680598376e-05,
      "loss": 2.8351,
      "step": 33300
    },
    {
      "epoch": 0.6819527533331973,
      "grad_norm": 11.137853622436523,
      "learning_rate": 1.5455417849194522e-05,
      "loss": 2.863,
      "step": 33400
    },
    {
      "epoch": 0.6839945280437757,
      "grad_norm": 13.035489082336426,
      "learning_rate": 1.5441806017790665e-05,
      "loss": 2.8723,
      "step": 33500
    },
    {
      "epoch": 0.6860363027543541,
      "grad_norm": 11.020535469055176,
      "learning_rate": 1.5428194186386808e-05,
      "loss": 2.8326,
      "step": 33600
    },
    {
      "epoch": 0.6880780774649325,
      "grad_norm": 10.036511421203613,
      "learning_rate": 1.5414582354982954e-05,
      "loss": 2.8897,
      "step": 33700
    },
    {
      "epoch": 0.690119852175511,
      "grad_norm": 11.51386547088623,
      "learning_rate": 1.5400970523579096e-05,
      "loss": 2.8308,
      "step": 33800
    },
    {
      "epoch": 0.6921616268860894,
      "grad_norm": 9.972577095031738,
      "learning_rate": 1.538735869217524e-05,
      "loss": 2.8396,
      "step": 33900
    },
    {
      "epoch": 0.6942034015966678,
      "grad_norm": 9.192771911621094,
      "learning_rate": 1.5373746860771382e-05,
      "loss": 2.8791,
      "step": 34000
    },
    {
      "epoch": 0.6962451763072462,
      "grad_norm": 9.788721084594727,
      "learning_rate": 1.5360135029367528e-05,
      "loss": 2.84,
      "step": 34100
    },
    {
      "epoch": 0.6982869510178247,
      "grad_norm": 11.583733558654785,
      "learning_rate": 1.534652319796367e-05,
      "loss": 2.825,
      "step": 34200
    },
    {
      "epoch": 0.7003287257284031,
      "grad_norm": 11.948651313781738,
      "learning_rate": 1.5332911366559814e-05,
      "loss": 2.7953,
      "step": 34300
    },
    {
      "epoch": 0.7023705004389815,
      "grad_norm": 10.488297462463379,
      "learning_rate": 1.531929953515596e-05,
      "loss": 2.845,
      "step": 34400
    },
    {
      "epoch": 0.70441227514956,
      "grad_norm": 11.592144012451172,
      "learning_rate": 1.5305687703752102e-05,
      "loss": 2.8617,
      "step": 34500
    },
    {
      "epoch": 0.7064540498601384,
      "grad_norm": 10.849248886108398,
      "learning_rate": 1.5292075872348245e-05,
      "loss": 2.878,
      "step": 34600
    },
    {
      "epoch": 0.7084958245707169,
      "grad_norm": 10.983626365661621,
      "learning_rate": 1.5278600159258426e-05,
      "loss": 2.9167,
      "step": 34700
    },
    {
      "epoch": 0.7105375992812953,
      "grad_norm": 11.85590648651123,
      "learning_rate": 1.5264988327854572e-05,
      "loss": 2.8242,
      "step": 34800
    },
    {
      "epoch": 0.7125793739918738,
      "grad_norm": 12.25714111328125,
      "learning_rate": 1.5251376496450717e-05,
      "loss": 2.8229,
      "step": 34900
    },
    {
      "epoch": 0.7146211487024522,
      "grad_norm": 10.517977714538574,
      "learning_rate": 1.523776466504686e-05,
      "loss": 2.8072,
      "step": 35000
    },
    {
      "epoch": 0.7166629234130306,
      "grad_norm": 12.126840591430664,
      "learning_rate": 1.5224152833643004e-05,
      "loss": 2.8389,
      "step": 35100
    },
    {
      "epoch": 0.718704698123609,
      "grad_norm": 11.009828567504883,
      "learning_rate": 1.5210541002239147e-05,
      "loss": 2.8578,
      "step": 35200
    },
    {
      "epoch": 0.7207464728341875,
      "grad_norm": 11.471771240234375,
      "learning_rate": 1.5196929170835291e-05,
      "loss": 2.8301,
      "step": 35300
    },
    {
      "epoch": 0.7227882475447659,
      "grad_norm": 9.431528091430664,
      "learning_rate": 1.5183317339431436e-05,
      "loss": 2.8804,
      "step": 35400
    },
    {
      "epoch": 0.7248300222553443,
      "grad_norm": 10.173434257507324,
      "learning_rate": 1.5169705508027578e-05,
      "loss": 2.8648,
      "step": 35500
    },
    {
      "epoch": 0.7268717969659227,
      "grad_norm": 12.336457252502441,
      "learning_rate": 1.5156093676623723e-05,
      "loss": 2.8412,
      "step": 35600
    },
    {
      "epoch": 0.7289135716765012,
      "grad_norm": 10.618561744689941,
      "learning_rate": 1.5142481845219867e-05,
      "loss": 2.8091,
      "step": 35700
    },
    {
      "epoch": 0.7309553463870796,
      "grad_norm": 10.496480941772461,
      "learning_rate": 1.512887001381601e-05,
      "loss": 2.8903,
      "step": 35800
    },
    {
      "epoch": 0.7329971210976581,
      "grad_norm": 10.198619842529297,
      "learning_rate": 1.5115258182412153e-05,
      "loss": 2.8695,
      "step": 35900
    },
    {
      "epoch": 0.7350388958082366,
      "grad_norm": 10.90108585357666,
      "learning_rate": 1.5101646351008297e-05,
      "loss": 2.8158,
      "step": 36000
    },
    {
      "epoch": 0.737080670518815,
      "grad_norm": 10.072822570800781,
      "learning_rate": 1.5088034519604442e-05,
      "loss": 2.7988,
      "step": 36100
    },
    {
      "epoch": 0.7391224452293934,
      "grad_norm": 9.933211326599121,
      "learning_rate": 1.5074422688200584e-05,
      "loss": 2.8442,
      "step": 36200
    },
    {
      "epoch": 0.7411642199399718,
      "grad_norm": 10.675247192382812,
      "learning_rate": 1.5060810856796729e-05,
      "loss": 2.8339,
      "step": 36300
    },
    {
      "epoch": 0.7432059946505503,
      "grad_norm": 12.171981811523438,
      "learning_rate": 1.5047199025392873e-05,
      "loss": 2.83,
      "step": 36400
    },
    {
      "epoch": 0.7452477693611287,
      "grad_norm": 9.551069259643555,
      "learning_rate": 1.5033587193989016e-05,
      "loss": 2.8594,
      "step": 36500
    },
    {
      "epoch": 0.7472895440717071,
      "grad_norm": 10.511720657348633,
      "learning_rate": 1.501997536258516e-05,
      "loss": 2.8722,
      "step": 36600
    },
    {
      "epoch": 0.7493313187822855,
      "grad_norm": 12.233919143676758,
      "learning_rate": 1.5006363531181305e-05,
      "loss": 2.8028,
      "step": 36700
    },
    {
      "epoch": 0.751373093492864,
      "grad_norm": 10.020161628723145,
      "learning_rate": 1.4992751699777448e-05,
      "loss": 2.8663,
      "step": 36800
    },
    {
      "epoch": 0.7534148682034424,
      "grad_norm": 9.516082763671875,
      "learning_rate": 1.497913986837359e-05,
      "loss": 2.8097,
      "step": 36900
    },
    {
      "epoch": 0.7554566429140208,
      "grad_norm": 10.199578285217285,
      "learning_rate": 1.4965528036969735e-05,
      "loss": 2.8126,
      "step": 37000
    },
    {
      "epoch": 0.7574984176245994,
      "grad_norm": 9.877142906188965,
      "learning_rate": 1.495191620556588e-05,
      "loss": 2.8426,
      "step": 37100
    },
    {
      "epoch": 0.7595401923351778,
      "grad_norm": 10.570747375488281,
      "learning_rate": 1.4938304374162022e-05,
      "loss": 2.7902,
      "step": 37200
    },
    {
      "epoch": 0.7615819670457562,
      "grad_norm": 11.404998779296875,
      "learning_rate": 1.4924692542758166e-05,
      "loss": 2.8275,
      "step": 37300
    },
    {
      "epoch": 0.7636237417563346,
      "grad_norm": 9.470364570617676,
      "learning_rate": 1.491108071135431e-05,
      "loss": 2.8379,
      "step": 37400
    },
    {
      "epoch": 0.765665516466913,
      "grad_norm": 11.284467697143555,
      "learning_rate": 1.4897468879950454e-05,
      "loss": 2.7983,
      "step": 37500
    },
    {
      "epoch": 0.7677072911774915,
      "grad_norm": 9.502828598022461,
      "learning_rate": 1.4883857048546598e-05,
      "loss": 2.7998,
      "step": 37600
    },
    {
      "epoch": 0.7697490658880699,
      "grad_norm": 9.115646362304688,
      "learning_rate": 1.4870245217142742e-05,
      "loss": 2.7962,
      "step": 37700
    },
    {
      "epoch": 0.7717908405986483,
      "grad_norm": 11.348908424377441,
      "learning_rate": 1.4856633385738885e-05,
      "loss": 2.8286,
      "step": 37800
    },
    {
      "epoch": 0.7738326153092268,
      "grad_norm": 10.198492050170898,
      "learning_rate": 1.4843157672649066e-05,
      "loss": 2.8484,
      "step": 37900
    },
    {
      "epoch": 0.7758743900198052,
      "grad_norm": 11.70257568359375,
      "learning_rate": 1.4829545841245212e-05,
      "loss": 2.826,
      "step": 38000
    },
    {
      "epoch": 0.7779161647303836,
      "grad_norm": 10.321854591369629,
      "learning_rate": 1.4815934009841355e-05,
      "loss": 2.7889,
      "step": 38100
    },
    {
      "epoch": 0.779957939440962,
      "grad_norm": 11.116811752319336,
      "learning_rate": 1.4802322178437498e-05,
      "loss": 2.7854,
      "step": 38200
    },
    {
      "epoch": 0.7819997141515406,
      "grad_norm": 10.93019962310791,
      "learning_rate": 1.4788710347033644e-05,
      "loss": 2.8197,
      "step": 38300
    },
    {
      "epoch": 0.784041488862119,
      "grad_norm": 9.126121520996094,
      "learning_rate": 1.4775098515629787e-05,
      "loss": 2.8593,
      "step": 38400
    },
    {
      "epoch": 0.7860832635726974,
      "grad_norm": 11.166820526123047,
      "learning_rate": 1.476148668422593e-05,
      "loss": 2.7733,
      "step": 38500
    },
    {
      "epoch": 0.7881250382832758,
      "grad_norm": 9.972249031066895,
      "learning_rate": 1.4747874852822076e-05,
      "loss": 2.8907,
      "step": 38600
    },
    {
      "epoch": 0.7901668129938543,
      "grad_norm": 12.599896430969238,
      "learning_rate": 1.4734263021418218e-05,
      "loss": 2.8289,
      "step": 38700
    },
    {
      "epoch": 0.7922085877044327,
      "grad_norm": 10.869879722595215,
      "learning_rate": 1.4720651190014361e-05,
      "loss": 2.7941,
      "step": 38800
    },
    {
      "epoch": 0.7942503624150111,
      "grad_norm": 12.777284622192383,
      "learning_rate": 1.4707039358610507e-05,
      "loss": 2.8183,
      "step": 38900
    },
    {
      "epoch": 0.7962921371255895,
      "grad_norm": 10.493311882019043,
      "learning_rate": 1.469342752720665e-05,
      "loss": 2.8182,
      "step": 39000
    },
    {
      "epoch": 0.798333911836168,
      "grad_norm": 9.871626853942871,
      "learning_rate": 1.4679815695802793e-05,
      "loss": 2.8078,
      "step": 39100
    },
    {
      "epoch": 0.8003756865467464,
      "grad_norm": 10.157535552978516,
      "learning_rate": 1.4666203864398935e-05,
      "loss": 2.8297,
      "step": 39200
    },
    {
      "epoch": 0.8024174612573248,
      "grad_norm": 9.502848625183105,
      "learning_rate": 1.4652592032995082e-05,
      "loss": 2.8139,
      "step": 39300
    },
    {
      "epoch": 0.8044592359679033,
      "grad_norm": 12.510340690612793,
      "learning_rate": 1.4638980201591224e-05,
      "loss": 2.8318,
      "step": 39400
    },
    {
      "epoch": 0.8065010106784818,
      "grad_norm": 11.289775848388672,
      "learning_rate": 1.4625368370187367e-05,
      "loss": 2.7959,
      "step": 39500
    },
    {
      "epoch": 0.8085427853890602,
      "grad_norm": 10.874006271362305,
      "learning_rate": 1.4611756538783513e-05,
      "loss": 2.8385,
      "step": 39600
    },
    {
      "epoch": 0.8105845600996386,
      "grad_norm": 11.244144439697266,
      "learning_rate": 1.4598144707379656e-05,
      "loss": 2.8169,
      "step": 39700
    },
    {
      "epoch": 0.8126263348102171,
      "grad_norm": 10.730229377746582,
      "learning_rate": 1.4584532875975799e-05,
      "loss": 2.8493,
      "step": 39800
    },
    {
      "epoch": 0.8146681095207955,
      "grad_norm": 11.481907844543457,
      "learning_rate": 1.4570921044571945e-05,
      "loss": 2.8626,
      "step": 39900
    },
    {
      "epoch": 0.8167098842313739,
      "grad_norm": 11.711259841918945,
      "learning_rate": 1.4557309213168088e-05,
      "loss": 2.7928,
      "step": 40000
    },
    {
      "epoch": 0.8187516589419523,
      "grad_norm": 13.857295989990234,
      "learning_rate": 1.454369738176423e-05,
      "loss": 2.7874,
      "step": 40100
    },
    {
      "epoch": 0.8207934336525308,
      "grad_norm": 11.913150787353516,
      "learning_rate": 1.4530221668674413e-05,
      "loss": 2.8655,
      "step": 40200
    },
    {
      "epoch": 0.8228352083631092,
      "grad_norm": 10.429884910583496,
      "learning_rate": 1.4516609837270556e-05,
      "loss": 2.7706,
      "step": 40300
    },
    {
      "epoch": 0.8248769830736876,
      "grad_norm": 8.618163108825684,
      "learning_rate": 1.45029980058667e-05,
      "loss": 2.8009,
      "step": 40400
    },
    {
      "epoch": 0.826918757784266,
      "grad_norm": 12.116910934448242,
      "learning_rate": 1.4489386174462845e-05,
      "loss": 2.7974,
      "step": 40500
    },
    {
      "epoch": 0.8289605324948445,
      "grad_norm": 9.740158081054688,
      "learning_rate": 1.4475774343058988e-05,
      "loss": 2.7803,
      "step": 40600
    },
    {
      "epoch": 0.831002307205423,
      "grad_norm": 11.473180770874023,
      "learning_rate": 1.4462162511655132e-05,
      "loss": 2.8016,
      "step": 40700
    },
    {
      "epoch": 0.8330440819160014,
      "grad_norm": 12.766204833984375,
      "learning_rate": 1.4448550680251276e-05,
      "loss": 2.8286,
      "step": 40800
    },
    {
      "epoch": 0.8350858566265799,
      "grad_norm": 10.464903831481934,
      "learning_rate": 1.4434938848847419e-05,
      "loss": 2.7409,
      "step": 40900
    },
    {
      "epoch": 0.8371276313371583,
      "grad_norm": 9.93539047241211,
      "learning_rate": 1.4421327017443562e-05,
      "loss": 2.7765,
      "step": 41000
    },
    {
      "epoch": 0.8391694060477367,
      "grad_norm": 11.02934741973877,
      "learning_rate": 1.4407715186039708e-05,
      "loss": 2.7922,
      "step": 41100
    },
    {
      "epoch": 0.8412111807583151,
      "grad_norm": 11.068066596984863,
      "learning_rate": 1.439410335463585e-05,
      "loss": 2.8315,
      "step": 41200
    },
    {
      "epoch": 0.8432529554688936,
      "grad_norm": 9.94703197479248,
      "learning_rate": 1.4380491523231993e-05,
      "loss": 2.799,
      "step": 41300
    },
    {
      "epoch": 0.845294730179472,
      "grad_norm": 11.838541030883789,
      "learning_rate": 1.4366879691828138e-05,
      "loss": 2.8307,
      "step": 41400
    },
    {
      "epoch": 0.8473365048900504,
      "grad_norm": 12.018574714660645,
      "learning_rate": 1.4353267860424282e-05,
      "loss": 2.7931,
      "step": 41500
    },
    {
      "epoch": 0.8493782796006288,
      "grad_norm": 11.93143081665039,
      "learning_rate": 1.4339656029020425e-05,
      "loss": 2.7438,
      "step": 41600
    },
    {
      "epoch": 0.8514200543112073,
      "grad_norm": 10.52403450012207,
      "learning_rate": 1.432604419761657e-05,
      "loss": 2.7656,
      "step": 41700
    },
    {
      "epoch": 0.8534618290217857,
      "grad_norm": 14.162652969360352,
      "learning_rate": 1.4312432366212714e-05,
      "loss": 2.7594,
      "step": 41800
    },
    {
      "epoch": 0.8555036037323642,
      "grad_norm": 9.006756782531738,
      "learning_rate": 1.4298820534808857e-05,
      "loss": 2.8222,
      "step": 41900
    },
    {
      "epoch": 0.8575453784429427,
      "grad_norm": 10.597365379333496,
      "learning_rate": 1.4285208703405e-05,
      "loss": 2.8387,
      "step": 42000
    },
    {
      "epoch": 0.8595871531535211,
      "grad_norm": 9.579630851745605,
      "learning_rate": 1.4271596872001146e-05,
      "loss": 2.8368,
      "step": 42100
    },
    {
      "epoch": 0.8616289278640995,
      "grad_norm": 10.380279541015625,
      "learning_rate": 1.4257985040597288e-05,
      "loss": 2.7576,
      "step": 42200
    },
    {
      "epoch": 0.8636707025746779,
      "grad_norm": 10.933210372924805,
      "learning_rate": 1.4244373209193431e-05,
      "loss": 2.7931,
      "step": 42300
    },
    {
      "epoch": 0.8657124772852564,
      "grad_norm": 11.760411262512207,
      "learning_rate": 1.4230761377789575e-05,
      "loss": 2.8198,
      "step": 42400
    },
    {
      "epoch": 0.8677542519958348,
      "grad_norm": 11.002984046936035,
      "learning_rate": 1.421714954638572e-05,
      "loss": 2.8315,
      "step": 42500
    },
    {
      "epoch": 0.8697960267064132,
      "grad_norm": 12.295646667480469,
      "learning_rate": 1.4203537714981863e-05,
      "loss": 2.8341,
      "step": 42600
    },
    {
      "epoch": 0.8718378014169916,
      "grad_norm": 12.383460998535156,
      "learning_rate": 1.4189925883578007e-05,
      "loss": 2.7869,
      "step": 42700
    },
    {
      "epoch": 0.8738795761275701,
      "grad_norm": 11.816725730895996,
      "learning_rate": 1.4176314052174152e-05,
      "loss": 2.8312,
      "step": 42800
    },
    {
      "epoch": 0.8759213508381485,
      "grad_norm": 9.2261962890625,
      "learning_rate": 1.4162702220770294e-05,
      "loss": 2.7824,
      "step": 42900
    },
    {
      "epoch": 0.8779631255487269,
      "grad_norm": 9.158249855041504,
      "learning_rate": 1.4149090389366437e-05,
      "loss": 2.7638,
      "step": 43000
    },
    {
      "epoch": 0.8800049002593053,
      "grad_norm": 9.995166778564453,
      "learning_rate": 1.4135478557962583e-05,
      "loss": 2.7791,
      "step": 43100
    },
    {
      "epoch": 0.8820466749698839,
      "grad_norm": 14.15185260772705,
      "learning_rate": 1.4121866726558726e-05,
      "loss": 2.805,
      "step": 43200
    },
    {
      "epoch": 0.8840884496804623,
      "grad_norm": 9.627781867980957,
      "learning_rate": 1.4108254895154869e-05,
      "loss": 2.8208,
      "step": 43300
    },
    {
      "epoch": 0.8861302243910407,
      "grad_norm": 11.160595893859863,
      "learning_rate": 1.4094643063751015e-05,
      "loss": 2.826,
      "step": 43400
    },
    {
      "epoch": 0.8881719991016191,
      "grad_norm": 14.354944229125977,
      "learning_rate": 1.4081031232347157e-05,
      "loss": 2.7649,
      "step": 43500
    },
    {
      "epoch": 0.8902137738121976,
      "grad_norm": 10.669976234436035,
      "learning_rate": 1.40674194009433e-05,
      "loss": 2.7405,
      "step": 43600
    },
    {
      "epoch": 0.892255548522776,
      "grad_norm": 10.2379150390625,
      "learning_rate": 1.4053807569539445e-05,
      "loss": 2.7394,
      "step": 43700
    },
    {
      "epoch": 0.8942973232333544,
      "grad_norm": 11.476995468139648,
      "learning_rate": 1.4040195738135589e-05,
      "loss": 2.8019,
      "step": 43800
    },
    {
      "epoch": 0.8963390979439328,
      "grad_norm": 11.750161170959473,
      "learning_rate": 1.4026583906731732e-05,
      "loss": 2.7811,
      "step": 43900
    },
    {
      "epoch": 0.8983808726545113,
      "grad_norm": 11.802828788757324,
      "learning_rate": 1.4012972075327875e-05,
      "loss": 2.762,
      "step": 44000
    },
    {
      "epoch": 0.9004226473650897,
      "grad_norm": 12.407559394836426,
      "learning_rate": 1.399936024392402e-05,
      "loss": 2.7272,
      "step": 44100
    },
    {
      "epoch": 0.9024644220756681,
      "grad_norm": 9.85627555847168,
      "learning_rate": 1.3985884530834202e-05,
      "loss": 2.7828,
      "step": 44200
    },
    {
      "epoch": 0.9045061967862466,
      "grad_norm": 11.341971397399902,
      "learning_rate": 1.3972408817744385e-05,
      "loss": 2.7764,
      "step": 44300
    },
    {
      "epoch": 0.9065479714968251,
      "grad_norm": 11.377480506896973,
      "learning_rate": 1.3958796986340528e-05,
      "loss": 2.7696,
      "step": 44400
    },
    {
      "epoch": 0.9085897462074035,
      "grad_norm": 13.722583770751953,
      "learning_rate": 1.3945185154936672e-05,
      "loss": 2.7745,
      "step": 44500
    },
    {
      "epoch": 0.9106315209179819,
      "grad_norm": 11.270963668823242,
      "learning_rate": 1.3931573323532816e-05,
      "loss": 2.7129,
      "step": 44600
    },
    {
      "epoch": 0.9126732956285604,
      "grad_norm": 11.342854499816895,
      "learning_rate": 1.3917961492128959e-05,
      "loss": 2.7226,
      "step": 44700
    },
    {
      "epoch": 0.9147150703391388,
      "grad_norm": 11.399478912353516,
      "learning_rate": 1.3904349660725104e-05,
      "loss": 2.8186,
      "step": 44800
    },
    {
      "epoch": 0.9167568450497172,
      "grad_norm": 13.302679061889648,
      "learning_rate": 1.3890737829321248e-05,
      "loss": 2.8416,
      "step": 44900
    },
    {
      "epoch": 0.9187986197602956,
      "grad_norm": 11.06058120727539,
      "learning_rate": 1.387712599791739e-05,
      "loss": 2.758,
      "step": 45000
    },
    {
      "epoch": 0.9208403944708741,
      "grad_norm": 10.770044326782227,
      "learning_rate": 1.3863514166513533e-05,
      "loss": 2.7834,
      "step": 45100
    },
    {
      "epoch": 0.9228821691814525,
      "grad_norm": 8.908097267150879,
      "learning_rate": 1.384990233510968e-05,
      "loss": 2.6955,
      "step": 45200
    },
    {
      "epoch": 0.9249239438920309,
      "grad_norm": 10.590033531188965,
      "learning_rate": 1.3836290503705822e-05,
      "loss": 2.8442,
      "step": 45300
    },
    {
      "epoch": 0.9269657186026093,
      "grad_norm": 11.822956085205078,
      "learning_rate": 1.3822678672301965e-05,
      "loss": 2.7565,
      "step": 45400
    },
    {
      "epoch": 0.9290074933131878,
      "grad_norm": 9.872822761535645,
      "learning_rate": 1.380906684089811e-05,
      "loss": 2.737,
      "step": 45500
    },
    {
      "epoch": 0.9310492680237663,
      "grad_norm": 9.047992706298828,
      "learning_rate": 1.3795455009494254e-05,
      "loss": 2.7373,
      "step": 45600
    },
    {
      "epoch": 0.9330910427343447,
      "grad_norm": 11.506660461425781,
      "learning_rate": 1.3781843178090397e-05,
      "loss": 2.8,
      "step": 45700
    },
    {
      "epoch": 0.9351328174449232,
      "grad_norm": 11.280250549316406,
      "learning_rate": 1.3768367465000581e-05,
      "loss": 2.7499,
      "step": 45800
    },
    {
      "epoch": 0.9371745921555016,
      "grad_norm": 11.016416549682617,
      "learning_rate": 1.3754755633596724e-05,
      "loss": 2.8049,
      "step": 45900
    },
    {
      "epoch": 0.93921636686608,
      "grad_norm": 12.721891403198242,
      "learning_rate": 1.3741143802192867e-05,
      "loss": 2.7653,
      "step": 46000
    },
    {
      "epoch": 0.9412581415766584,
      "grad_norm": 11.721755027770996,
      "learning_rate": 1.3727531970789011e-05,
      "loss": 2.7964,
      "step": 46100
    },
    {
      "epoch": 0.9432999162872369,
      "grad_norm": 10.073653221130371,
      "learning_rate": 1.3713920139385156e-05,
      "loss": 2.7572,
      "step": 46200
    },
    {
      "epoch": 0.9453416909978153,
      "grad_norm": 11.651507377624512,
      "learning_rate": 1.3700308307981298e-05,
      "loss": 2.771,
      "step": 46300
    },
    {
      "epoch": 0.9473834657083937,
      "grad_norm": 11.566937446594238,
      "learning_rate": 1.3686696476577441e-05,
      "loss": 2.7583,
      "step": 46400
    },
    {
      "epoch": 0.9494252404189721,
      "grad_norm": 10.780101776123047,
      "learning_rate": 1.3673084645173587e-05,
      "loss": 2.74,
      "step": 46500
    },
    {
      "epoch": 0.9514670151295506,
      "grad_norm": 11.02039623260498,
      "learning_rate": 1.365947281376973e-05,
      "loss": 2.7758,
      "step": 46600
    },
    {
      "epoch": 0.953508789840129,
      "grad_norm": 10.66139030456543,
      "learning_rate": 1.3645860982365873e-05,
      "loss": 2.7077,
      "step": 46700
    },
    {
      "epoch": 0.9555505645507075,
      "grad_norm": 10.105561256408691,
      "learning_rate": 1.3632249150962019e-05,
      "loss": 2.7882,
      "step": 46800
    },
    {
      "epoch": 0.957592339261286,
      "grad_norm": 10.529783248901367,
      "learning_rate": 1.3618637319558162e-05,
      "loss": 2.7947,
      "step": 46900
    },
    {
      "epoch": 0.9596341139718644,
      "grad_norm": 14.914750099182129,
      "learning_rate": 1.3605025488154304e-05,
      "loss": 2.7726,
      "step": 47000
    },
    {
      "epoch": 0.9616758886824428,
      "grad_norm": 12.292624473571777,
      "learning_rate": 1.3591413656750449e-05,
      "loss": 2.7433,
      "step": 47100
    },
    {
      "epoch": 0.9637176633930212,
      "grad_norm": 10.854389190673828,
      "learning_rate": 1.3577801825346593e-05,
      "loss": 2.8008,
      "step": 47200
    },
    {
      "epoch": 0.9657594381035997,
      "grad_norm": 11.994743347167969,
      "learning_rate": 1.3564189993942736e-05,
      "loss": 2.7783,
      "step": 47300
    },
    {
      "epoch": 0.9678012128141781,
      "grad_norm": 9.192473411560059,
      "learning_rate": 1.3550578162538879e-05,
      "loss": 2.7419,
      "step": 47400
    },
    {
      "epoch": 0.9698429875247565,
      "grad_norm": 11.200758934020996,
      "learning_rate": 1.3536966331135025e-05,
      "loss": 2.7567,
      "step": 47500
    },
    {
      "epoch": 0.9718847622353349,
      "grad_norm": 8.372179985046387,
      "learning_rate": 1.3523354499731168e-05,
      "loss": 2.6993,
      "step": 47600
    },
    {
      "epoch": 0.9739265369459134,
      "grad_norm": 11.416936874389648,
      "learning_rate": 1.350974266832731e-05,
      "loss": 2.6853,
      "step": 47700
    },
    {
      "epoch": 0.9759683116564918,
      "grad_norm": 10.777482032775879,
      "learning_rate": 1.3496130836923456e-05,
      "loss": 2.7889,
      "step": 47800
    },
    {
      "epoch": 0.9780100863670702,
      "grad_norm": 10.662362098693848,
      "learning_rate": 1.3482519005519599e-05,
      "loss": 2.7366,
      "step": 47900
    },
    {
      "epoch": 0.9800518610776487,
      "grad_norm": 11.058036804199219,
      "learning_rate": 1.3468907174115742e-05,
      "loss": 2.7871,
      "step": 48000
    },
    {
      "epoch": 0.9820936357882272,
      "grad_norm": 10.719124794006348,
      "learning_rate": 1.3455295342711886e-05,
      "loss": 2.7317,
      "step": 48100
    },
    {
      "epoch": 0.9841354104988056,
      "grad_norm": 11.083736419677734,
      "learning_rate": 1.344168351130803e-05,
      "loss": 2.7562,
      "step": 48200
    },
    {
      "epoch": 0.986177185209384,
      "grad_norm": 11.192622184753418,
      "learning_rate": 1.3428071679904173e-05,
      "loss": 2.758,
      "step": 48300
    },
    {
      "epoch": 0.9882189599199624,
      "grad_norm": 9.956863403320312,
      "learning_rate": 1.3414459848500318e-05,
      "loss": 2.7997,
      "step": 48400
    },
    {
      "epoch": 0.9902607346305409,
      "grad_norm": 8.878944396972656,
      "learning_rate": 1.3400848017096462e-05,
      "loss": 2.7179,
      "step": 48500
    },
    {
      "epoch": 0.9923025093411193,
      "grad_norm": 10.145464897155762,
      "learning_rate": 1.3387236185692605e-05,
      "loss": 2.7763,
      "step": 48600
    },
    {
      "epoch": 0.9943442840516977,
      "grad_norm": 10.312487602233887,
      "learning_rate": 1.3373624354288748e-05,
      "loss": 2.7238,
      "step": 48700
    },
    {
      "epoch": 0.9963860587622761,
      "grad_norm": 11.584235191345215,
      "learning_rate": 1.3360012522884894e-05,
      "loss": 2.7003,
      "step": 48800
    },
    {
      "epoch": 0.9984278334728546,
      "grad_norm": 11.403764724731445,
      "learning_rate": 1.3346400691481037e-05,
      "loss": 2.8339,
      "step": 48900
    },
    {
      "epoch": 1.000469608183433,
      "grad_norm": 11.226956367492676,
      "learning_rate": 1.333278886007718e-05,
      "loss": 2.7128,
      "step": 49000
    },
    {
      "epoch": 1.0025113828940115,
      "grad_norm": 11.939264297485352,
      "learning_rate": 1.3319177028673324e-05,
      "loss": 2.6903,
      "step": 49100
    },
    {
      "epoch": 1.0045531576045899,
      "grad_norm": 10.520170211791992,
      "learning_rate": 1.3305565197269468e-05,
      "loss": 2.7153,
      "step": 49200
    },
    {
      "epoch": 1.0065949323151684,
      "grad_norm": 10.791046142578125,
      "learning_rate": 1.3291953365865611e-05,
      "loss": 2.7076,
      "step": 49300
    },
    {
      "epoch": 1.0086367070257467,
      "grad_norm": 9.752640724182129,
      "learning_rate": 1.3278341534461755e-05,
      "loss": 2.719,
      "step": 49400
    },
    {
      "epoch": 1.0106784817363252,
      "grad_norm": 9.750712394714355,
      "learning_rate": 1.32647297030579e-05,
      "loss": 2.7334,
      "step": 49500
    },
    {
      "epoch": 1.0127202564469036,
      "grad_norm": 11.784014701843262,
      "learning_rate": 1.3251117871654043e-05,
      "loss": 2.7104,
      "step": 49600
    },
    {
      "epoch": 1.014762031157482,
      "grad_norm": 8.660876274108887,
      "learning_rate": 1.3237506040250187e-05,
      "loss": 2.7843,
      "step": 49700
    },
    {
      "epoch": 1.0168038058680606,
      "grad_norm": 11.988188743591309,
      "learning_rate": 1.3223894208846331e-05,
      "loss": 2.7351,
      "step": 49800
    },
    {
      "epoch": 1.018845580578639,
      "grad_norm": 10.169512748718262,
      "learning_rate": 1.3210282377442474e-05,
      "loss": 2.7599,
      "step": 49900
    },
    {
      "epoch": 1.0208873552892175,
      "grad_norm": 11.67236614227295,
      "learning_rate": 1.3196806664352657e-05,
      "loss": 2.767,
      "step": 50000
    },
    {
      "epoch": 1.0229291299997958,
      "grad_norm": 11.439363479614258,
      "learning_rate": 1.31831948329488e-05,
      "loss": 2.7593,
      "step": 50100
    },
    {
      "epoch": 1.0249709047103743,
      "grad_norm": 12.62500286102295,
      "learning_rate": 1.3169583001544943e-05,
      "loss": 2.7348,
      "step": 50200
    },
    {
      "epoch": 1.0270126794209526,
      "grad_norm": 12.149836540222168,
      "learning_rate": 1.3155971170141089e-05,
      "loss": 2.6265,
      "step": 50300
    },
    {
      "epoch": 1.0290544541315312,
      "grad_norm": 9.57340145111084,
      "learning_rate": 1.3142359338737231e-05,
      "loss": 2.739,
      "step": 50400
    },
    {
      "epoch": 1.0310962288421095,
      "grad_norm": 10.34028148651123,
      "learning_rate": 1.3128747507333374e-05,
      "loss": 2.7289,
      "step": 50500
    },
    {
      "epoch": 1.033138003552688,
      "grad_norm": 10.444984436035156,
      "learning_rate": 1.311513567592952e-05,
      "loss": 2.7359,
      "step": 50600
    },
    {
      "epoch": 1.0351797782632663,
      "grad_norm": 14.428279876708984,
      "learning_rate": 1.3101523844525663e-05,
      "loss": 2.778,
      "step": 50700
    },
    {
      "epoch": 1.0372215529738449,
      "grad_norm": 10.046710968017578,
      "learning_rate": 1.3087912013121806e-05,
      "loss": 2.7336,
      "step": 50800
    },
    {
      "epoch": 1.0392633276844232,
      "grad_norm": 10.798172950744629,
      "learning_rate": 1.3074300181717949e-05,
      "loss": 2.7759,
      "step": 50900
    },
    {
      "epoch": 1.0413051023950017,
      "grad_norm": 8.371969223022461,
      "learning_rate": 1.3060688350314095e-05,
      "loss": 2.7251,
      "step": 51000
    },
    {
      "epoch": 1.0433468771055803,
      "grad_norm": 9.273093223571777,
      "learning_rate": 1.3047076518910237e-05,
      "loss": 2.723,
      "step": 51100
    },
    {
      "epoch": 1.0453886518161586,
      "grad_norm": 9.023242950439453,
      "learning_rate": 1.303346468750638e-05,
      "loss": 2.7267,
      "step": 51200
    },
    {
      "epoch": 1.0474304265267371,
      "grad_norm": 10.444002151489258,
      "learning_rate": 1.3019852856102526e-05,
      "loss": 2.7773,
      "step": 51300
    },
    {
      "epoch": 1.0494722012373154,
      "grad_norm": 9.401403427124023,
      "learning_rate": 1.3006241024698669e-05,
      "loss": 2.7612,
      "step": 51400
    },
    {
      "epoch": 1.051513975947894,
      "grad_norm": 9.849493026733398,
      "learning_rate": 1.2992629193294812e-05,
      "loss": 2.7138,
      "step": 51500
    },
    {
      "epoch": 1.0535557506584723,
      "grad_norm": 10.72515869140625,
      "learning_rate": 1.2979017361890958e-05,
      "loss": 2.7231,
      "step": 51600
    },
    {
      "epoch": 1.0555975253690508,
      "grad_norm": 11.40876293182373,
      "learning_rate": 1.29654055304871e-05,
      "loss": 2.7621,
      "step": 51700
    },
    {
      "epoch": 1.0576393000796291,
      "grad_norm": 11.258780479431152,
      "learning_rate": 1.2951793699083243e-05,
      "loss": 2.7596,
      "step": 51800
    },
    {
      "epoch": 1.0596810747902077,
      "grad_norm": 8.882905006408691,
      "learning_rate": 1.2938181867679386e-05,
      "loss": 2.6653,
      "step": 51900
    },
    {
      "epoch": 1.061722849500786,
      "grad_norm": 12.123570442199707,
      "learning_rate": 1.2924570036275532e-05,
      "loss": 2.6893,
      "step": 52000
    },
    {
      "epoch": 1.0637646242113645,
      "grad_norm": 9.139384269714355,
      "learning_rate": 1.2911094323185713e-05,
      "loss": 2.7718,
      "step": 52100
    },
    {
      "epoch": 1.0658063989219428,
      "grad_norm": 9.319656372070312,
      "learning_rate": 1.2897482491781858e-05,
      "loss": 2.7304,
      "step": 52200
    },
    {
      "epoch": 1.0678481736325214,
      "grad_norm": 13.010622024536133,
      "learning_rate": 1.2883870660378002e-05,
      "loss": 2.7326,
      "step": 52300
    },
    {
      "epoch": 1.0698899483431,
      "grad_norm": 11.101982116699219,
      "learning_rate": 1.2870258828974145e-05,
      "loss": 2.7417,
      "step": 52400
    },
    {
      "epoch": 1.0719317230536782,
      "grad_norm": 11.25157642364502,
      "learning_rate": 1.285664699757029e-05,
      "loss": 2.7029,
      "step": 52500
    },
    {
      "epoch": 1.0739734977642568,
      "grad_norm": 11.55439281463623,
      "learning_rate": 1.2843035166166434e-05,
      "loss": 2.7094,
      "step": 52600
    },
    {
      "epoch": 1.076015272474835,
      "grad_norm": 11.868036270141602,
      "learning_rate": 1.2829423334762577e-05,
      "loss": 2.6499,
      "step": 52700
    },
    {
      "epoch": 1.0780570471854136,
      "grad_norm": 10.449520111083984,
      "learning_rate": 1.281581150335872e-05,
      "loss": 2.703,
      "step": 52800
    },
    {
      "epoch": 1.080098821895992,
      "grad_norm": 14.623248100280762,
      "learning_rate": 1.2802335790268902e-05,
      "loss": 2.7168,
      "step": 52900
    },
    {
      "epoch": 1.0821405966065705,
      "grad_norm": 10.302390098571777,
      "learning_rate": 1.2788723958865045e-05,
      "loss": 2.7246,
      "step": 53000
    },
    {
      "epoch": 1.0841823713171488,
      "grad_norm": 10.685277938842773,
      "learning_rate": 1.2775112127461191e-05,
      "loss": 2.7267,
      "step": 53100
    },
    {
      "epoch": 1.0862241460277273,
      "grad_norm": 13.995108604431152,
      "learning_rate": 1.2761500296057334e-05,
      "loss": 2.7455,
      "step": 53200
    },
    {
      "epoch": 1.0882659207383056,
      "grad_norm": 10.684361457824707,
      "learning_rate": 1.2747888464653477e-05,
      "loss": 2.7341,
      "step": 53300
    },
    {
      "epoch": 1.0903076954488842,
      "grad_norm": 11.90002727508545,
      "learning_rate": 1.2734276633249623e-05,
      "loss": 2.676,
      "step": 53400
    },
    {
      "epoch": 1.0923494701594625,
      "grad_norm": 10.629268646240234,
      "learning_rate": 1.2720664801845766e-05,
      "loss": 2.7623,
      "step": 53500
    },
    {
      "epoch": 1.094391244870041,
      "grad_norm": 12.77210807800293,
      "learning_rate": 1.2707052970441908e-05,
      "loss": 2.7171,
      "step": 53600
    },
    {
      "epoch": 1.0964330195806196,
      "grad_norm": 9.156747817993164,
      "learning_rate": 1.2693441139038053e-05,
      "loss": 2.7768,
      "step": 53700
    },
    {
      "epoch": 1.0984747942911979,
      "grad_norm": 8.930329322814941,
      "learning_rate": 1.2679829307634197e-05,
      "loss": 2.7017,
      "step": 53800
    },
    {
      "epoch": 1.1005165690017764,
      "grad_norm": 10.686976432800293,
      "learning_rate": 1.266621747623034e-05,
      "loss": 2.7364,
      "step": 53900
    },
    {
      "epoch": 1.1025583437123547,
      "grad_norm": 9.234782218933105,
      "learning_rate": 1.2652605644826483e-05,
      "loss": 2.7267,
      "step": 54000
    },
    {
      "epoch": 1.1046001184229333,
      "grad_norm": 11.43943977355957,
      "learning_rate": 1.2638993813422629e-05,
      "loss": 2.7491,
      "step": 54100
    },
    {
      "epoch": 1.1066418931335116,
      "grad_norm": 11.56860065460205,
      "learning_rate": 1.2625381982018771e-05,
      "loss": 2.7523,
      "step": 54200
    },
    {
      "epoch": 1.1086836678440901,
      "grad_norm": 12.06158447265625,
      "learning_rate": 1.2611770150614914e-05,
      "loss": 2.7648,
      "step": 54300
    },
    {
      "epoch": 1.1107254425546684,
      "grad_norm": 14.19452953338623,
      "learning_rate": 1.259815831921106e-05,
      "loss": 2.7976,
      "step": 54400
    },
    {
      "epoch": 1.112767217265247,
      "grad_norm": 12.49577808380127,
      "learning_rate": 1.2584546487807203e-05,
      "loss": 2.6865,
      "step": 54500
    },
    {
      "epoch": 1.1148089919758255,
      "grad_norm": 14.607259750366211,
      "learning_rate": 1.2570934656403346e-05,
      "loss": 2.7258,
      "step": 54600
    },
    {
      "epoch": 1.1168507666864038,
      "grad_norm": 10.253491401672363,
      "learning_rate": 1.2557322824999492e-05,
      "loss": 2.6858,
      "step": 54700
    },
    {
      "epoch": 1.1188925413969824,
      "grad_norm": 10.281402587890625,
      "learning_rate": 1.2543710993595635e-05,
      "loss": 2.6219,
      "step": 54800
    },
    {
      "epoch": 1.1209343161075607,
      "grad_norm": 11.459051132202148,
      "learning_rate": 1.2530099162191777e-05,
      "loss": 2.7136,
      "step": 54900
    },
    {
      "epoch": 1.1229760908181392,
      "grad_norm": 11.611181259155273,
      "learning_rate": 1.251648733078792e-05,
      "loss": 2.6864,
      "step": 55000
    },
    {
      "epoch": 1.1250178655287175,
      "grad_norm": 11.443300247192383,
      "learning_rate": 1.2502875499384066e-05,
      "loss": 2.664,
      "step": 55100
    },
    {
      "epoch": 1.127059640239296,
      "grad_norm": 10.503671646118164,
      "learning_rate": 1.2489263667980209e-05,
      "loss": 2.6579,
      "step": 55200
    },
    {
      "epoch": 1.1291014149498744,
      "grad_norm": 11.057934761047363,
      "learning_rate": 1.2475651836576352e-05,
      "loss": 2.7489,
      "step": 55300
    },
    {
      "epoch": 1.131143189660453,
      "grad_norm": 11.73900318145752,
      "learning_rate": 1.2462040005172498e-05,
      "loss": 2.6993,
      "step": 55400
    },
    {
      "epoch": 1.1331849643710312,
      "grad_norm": 11.119424819946289,
      "learning_rate": 1.244842817376864e-05,
      "loss": 2.7406,
      "step": 55500
    },
    {
      "epoch": 1.1352267390816098,
      "grad_norm": 11.411421775817871,
      "learning_rate": 1.2434816342364783e-05,
      "loss": 2.7869,
      "step": 55600
    },
    {
      "epoch": 1.137268513792188,
      "grad_norm": 13.74057674407959,
      "learning_rate": 1.242120451096093e-05,
      "loss": 2.7083,
      "step": 55700
    },
    {
      "epoch": 1.1393102885027666,
      "grad_norm": 9.641105651855469,
      "learning_rate": 1.2407592679557072e-05,
      "loss": 2.7359,
      "step": 55800
    },
    {
      "epoch": 1.1413520632133451,
      "grad_norm": 12.5882568359375,
      "learning_rate": 1.2393980848153215e-05,
      "loss": 2.6718,
      "step": 55900
    },
    {
      "epoch": 1.1433938379239235,
      "grad_norm": 12.869656562805176,
      "learning_rate": 1.2380369016749361e-05,
      "loss": 2.7271,
      "step": 56000
    },
    {
      "epoch": 1.145435612634502,
      "grad_norm": 10.047717094421387,
      "learning_rate": 1.2366757185345504e-05,
      "loss": 2.6662,
      "step": 56100
    },
    {
      "epoch": 1.1474773873450803,
      "grad_norm": 10.678414344787598,
      "learning_rate": 1.2353145353941647e-05,
      "loss": 2.7437,
      "step": 56200
    },
    {
      "epoch": 1.1495191620556588,
      "grad_norm": 10.485908508300781,
      "learning_rate": 1.233953352253779e-05,
      "loss": 2.728,
      "step": 56300
    },
    {
      "epoch": 1.1515609367662372,
      "grad_norm": 13.216562271118164,
      "learning_rate": 1.2325921691133935e-05,
      "loss": 2.7416,
      "step": 56400
    },
    {
      "epoch": 1.1536027114768157,
      "grad_norm": 10.29166030883789,
      "learning_rate": 1.2312309859730078e-05,
      "loss": 2.6764,
      "step": 56500
    },
    {
      "epoch": 1.155644486187394,
      "grad_norm": 9.101388931274414,
      "learning_rate": 1.2298698028326221e-05,
      "loss": 2.7489,
      "step": 56600
    },
    {
      "epoch": 1.1576862608979726,
      "grad_norm": 13.186013221740723,
      "learning_rate": 1.2285086196922367e-05,
      "loss": 2.7255,
      "step": 56700
    },
    {
      "epoch": 1.1597280356085509,
      "grad_norm": 10.50206184387207,
      "learning_rate": 1.227147436551851e-05,
      "loss": 2.699,
      "step": 56800
    },
    {
      "epoch": 1.1617698103191294,
      "grad_norm": 11.366745948791504,
      "learning_rate": 1.2257862534114653e-05,
      "loss": 2.7287,
      "step": 56900
    },
    {
      "epoch": 1.1638115850297077,
      "grad_norm": 11.863992691040039,
      "learning_rate": 1.2244250702710799e-05,
      "loss": 2.6616,
      "step": 57000
    },
    {
      "epoch": 1.1658533597402863,
      "grad_norm": 11.893980026245117,
      "learning_rate": 1.2230638871306941e-05,
      "loss": 2.6649,
      "step": 57100
    },
    {
      "epoch": 1.1678951344508648,
      "grad_norm": 10.331381797790527,
      "learning_rate": 1.2217027039903084e-05,
      "loss": 2.6951,
      "step": 57200
    },
    {
      "epoch": 1.169936909161443,
      "grad_norm": 13.390835762023926,
      "learning_rate": 1.2203415208499227e-05,
      "loss": 2.6683,
      "step": 57300
    },
    {
      "epoch": 1.1719786838720216,
      "grad_norm": 10.469450950622559,
      "learning_rate": 1.2189803377095373e-05,
      "loss": 2.7787,
      "step": 57400
    },
    {
      "epoch": 1.1740204585826,
      "grad_norm": 12.348511695861816,
      "learning_rate": 1.2176191545691516e-05,
      "loss": 2.6794,
      "step": 57500
    },
    {
      "epoch": 1.1760622332931785,
      "grad_norm": 11.915794372558594,
      "learning_rate": 1.2162579714287658e-05,
      "loss": 2.7267,
      "step": 57600
    },
    {
      "epoch": 1.1781040080037568,
      "grad_norm": 9.683646202087402,
      "learning_rate": 1.2148967882883805e-05,
      "loss": 2.6657,
      "step": 57700
    },
    {
      "epoch": 1.1801457827143353,
      "grad_norm": 11.552889823913574,
      "learning_rate": 1.2135356051479947e-05,
      "loss": 2.7045,
      "step": 57800
    },
    {
      "epoch": 1.1821875574249137,
      "grad_norm": 10.028327941894531,
      "learning_rate": 1.212174422007609e-05,
      "loss": 2.6194,
      "step": 57900
    },
    {
      "epoch": 1.1842293321354922,
      "grad_norm": 13.675981521606445,
      "learning_rate": 1.2108132388672236e-05,
      "loss": 2.7553,
      "step": 58000
    },
    {
      "epoch": 1.1862711068460705,
      "grad_norm": 11.113093376159668,
      "learning_rate": 1.2094520557268379e-05,
      "loss": 2.7476,
      "step": 58100
    },
    {
      "epoch": 1.188312881556649,
      "grad_norm": 13.407461166381836,
      "learning_rate": 1.2080908725864522e-05,
      "loss": 2.7164,
      "step": 58200
    },
    {
      "epoch": 1.1903546562672274,
      "grad_norm": 9.613470077514648,
      "learning_rate": 1.2067296894460668e-05,
      "loss": 2.6749,
      "step": 58300
    },
    {
      "epoch": 1.192396430977806,
      "grad_norm": 11.251893997192383,
      "learning_rate": 1.205368506305681e-05,
      "loss": 2.6659,
      "step": 58400
    },
    {
      "epoch": 1.1944382056883844,
      "grad_norm": 12.113304138183594,
      "learning_rate": 1.2040073231652953e-05,
      "loss": 2.7241,
      "step": 58500
    },
    {
      "epoch": 1.1964799803989628,
      "grad_norm": 9.572315216064453,
      "learning_rate": 1.2026461400249096e-05,
      "loss": 2.7409,
      "step": 58600
    },
    {
      "epoch": 1.1985217551095413,
      "grad_norm": 11.770283699035645,
      "learning_rate": 1.201298568715928e-05,
      "loss": 2.6924,
      "step": 58700
    },
    {
      "epoch": 1.2005635298201196,
      "grad_norm": 12.047839164733887,
      "learning_rate": 1.1999373855755423e-05,
      "loss": 2.7344,
      "step": 58800
    },
    {
      "epoch": 1.2026053045306981,
      "grad_norm": 10.858489036560059,
      "learning_rate": 1.1985762024351568e-05,
      "loss": 2.717,
      "step": 58900
    },
    {
      "epoch": 1.2046470792412765,
      "grad_norm": 9.730304718017578,
      "learning_rate": 1.197215019294771e-05,
      "loss": 2.7164,
      "step": 59000
    },
    {
      "epoch": 1.206688853951855,
      "grad_norm": 11.567792892456055,
      "learning_rate": 1.1958538361543855e-05,
      "loss": 2.6579,
      "step": 59100
    },
    {
      "epoch": 1.2087306286624333,
      "grad_norm": 11.488842010498047,
      "learning_rate": 1.194492653014e-05,
      "loss": 2.6907,
      "step": 59200
    },
    {
      "epoch": 1.2107724033730118,
      "grad_norm": 8.887605667114258,
      "learning_rate": 1.1931314698736142e-05,
      "loss": 2.6956,
      "step": 59300
    },
    {
      "epoch": 1.2128141780835904,
      "grad_norm": 12.15835189819336,
      "learning_rate": 1.1917702867332287e-05,
      "loss": 2.7142,
      "step": 59400
    },
    {
      "epoch": 1.2148559527941687,
      "grad_norm": 12.312254905700684,
      "learning_rate": 1.190409103592843e-05,
      "loss": 2.6406,
      "step": 59500
    },
    {
      "epoch": 1.216897727504747,
      "grad_norm": 12.569779396057129,
      "learning_rate": 1.1890479204524574e-05,
      "loss": 2.7328,
      "step": 59600
    },
    {
      "epoch": 1.2189395022153255,
      "grad_norm": 10.212621688842773,
      "learning_rate": 1.1876867373120718e-05,
      "loss": 2.6707,
      "step": 59700
    },
    {
      "epoch": 1.220981276925904,
      "grad_norm": 8.89607048034668,
      "learning_rate": 1.1863255541716861e-05,
      "loss": 2.658,
      "step": 59800
    },
    {
      "epoch": 1.2230230516364824,
      "grad_norm": 9.88992691040039,
      "learning_rate": 1.1849643710313005e-05,
      "loss": 2.644,
      "step": 59900
    },
    {
      "epoch": 1.225064826347061,
      "grad_norm": 15.015092849731445,
      "learning_rate": 1.1836031878909148e-05,
      "loss": 2.6152,
      "step": 60000
    },
    {
      "epoch": 1.2271066010576392,
      "grad_norm": 9.628312110900879,
      "learning_rate": 1.1822420047505292e-05,
      "loss": 2.632,
      "step": 60100
    },
    {
      "epoch": 1.2291483757682178,
      "grad_norm": 10.151597023010254,
      "learning_rate": 1.1808808216101437e-05,
      "loss": 2.7613,
      "step": 60200
    },
    {
      "epoch": 1.231190150478796,
      "grad_norm": 11.064592361450195,
      "learning_rate": 1.179519638469758e-05,
      "loss": 2.6944,
      "step": 60300
    },
    {
      "epoch": 1.2332319251893746,
      "grad_norm": 12.456275939941406,
      "learning_rate": 1.1781584553293724e-05,
      "loss": 2.7021,
      "step": 60400
    },
    {
      "epoch": 1.235273699899953,
      "grad_norm": 13.101127624511719,
      "learning_rate": 1.1767972721889869e-05,
      "loss": 2.6336,
      "step": 60500
    },
    {
      "epoch": 1.2373154746105315,
      "grad_norm": 12.555645942687988,
      "learning_rate": 1.1754360890486011e-05,
      "loss": 2.6653,
      "step": 60600
    },
    {
      "epoch": 1.23935724932111,
      "grad_norm": 11.080556869506836,
      "learning_rate": 1.1740749059082156e-05,
      "loss": 2.6582,
      "step": 60700
    },
    {
      "epoch": 1.2413990240316883,
      "grad_norm": 9.6983642578125,
      "learning_rate": 1.1727137227678298e-05,
      "loss": 2.6537,
      "step": 60800
    },
    {
      "epoch": 1.2434407987422669,
      "grad_norm": 9.555248260498047,
      "learning_rate": 1.1713525396274443e-05,
      "loss": 2.6811,
      "step": 60900
    },
    {
      "epoch": 1.2454825734528452,
      "grad_norm": 9.771191596984863,
      "learning_rate": 1.1699913564870586e-05,
      "loss": 2.6844,
      "step": 61000
    },
    {
      "epoch": 1.2475243481634237,
      "grad_norm": 9.370321273803711,
      "learning_rate": 1.168630173346673e-05,
      "loss": 2.6669,
      "step": 61100
    },
    {
      "epoch": 1.249566122874002,
      "grad_norm": 9.31003189086914,
      "learning_rate": 1.1672689902062874e-05,
      "loss": 2.695,
      "step": 61200
    },
    {
      "epoch": 1.2516078975845806,
      "grad_norm": 13.148396492004395,
      "learning_rate": 1.1659078070659017e-05,
      "loss": 2.67,
      "step": 61300
    },
    {
      "epoch": 1.253649672295159,
      "grad_norm": 13.087136268615723,
      "learning_rate": 1.1645466239255162e-05,
      "loss": 2.7438,
      "step": 61400
    },
    {
      "epoch": 1.2556914470057374,
      "grad_norm": 11.522722244262695,
      "learning_rate": 1.1631990526165345e-05,
      "loss": 2.7009,
      "step": 61500
    },
    {
      "epoch": 1.2577332217163157,
      "grad_norm": 11.741305351257324,
      "learning_rate": 1.1618378694761487e-05,
      "loss": 2.7632,
      "step": 61600
    },
    {
      "epoch": 1.2597749964268943,
      "grad_norm": 12.004621505737305,
      "learning_rate": 1.160476686335763e-05,
      "loss": 2.6963,
      "step": 61700
    },
    {
      "epoch": 1.2618167711374726,
      "grad_norm": 11.227425575256348,
      "learning_rate": 1.1591155031953776e-05,
      "loss": 2.7161,
      "step": 61800
    },
    {
      "epoch": 1.2638585458480511,
      "grad_norm": 11.371803283691406,
      "learning_rate": 1.1577543200549919e-05,
      "loss": 2.6428,
      "step": 61900
    },
    {
      "epoch": 1.2659003205586297,
      "grad_norm": 10.794210433959961,
      "learning_rate": 1.1563931369146062e-05,
      "loss": 2.7518,
      "step": 62000
    },
    {
      "epoch": 1.267942095269208,
      "grad_norm": 10.07056999206543,
      "learning_rate": 1.1550319537742208e-05,
      "loss": 2.6753,
      "step": 62100
    },
    {
      "epoch": 1.2699838699797863,
      "grad_norm": 10.472249984741211,
      "learning_rate": 1.153670770633835e-05,
      "loss": 2.6436,
      "step": 62200
    },
    {
      "epoch": 1.2720256446903648,
      "grad_norm": 10.452703475952148,
      "learning_rate": 1.1523095874934493e-05,
      "loss": 2.6155,
      "step": 62300
    },
    {
      "epoch": 1.2740674194009434,
      "grad_norm": 13.385458946228027,
      "learning_rate": 1.150948404353064e-05,
      "loss": 2.6885,
      "step": 62400
    },
    {
      "epoch": 1.2761091941115217,
      "grad_norm": 11.9752779006958,
      "learning_rate": 1.1495872212126782e-05,
      "loss": 2.6411,
      "step": 62500
    },
    {
      "epoch": 1.2781509688221002,
      "grad_norm": 11.753067970275879,
      "learning_rate": 1.1482260380722925e-05,
      "loss": 2.7176,
      "step": 62600
    },
    {
      "epoch": 1.2801927435326785,
      "grad_norm": 10.690476417541504,
      "learning_rate": 1.1468648549319068e-05,
      "loss": 2.6508,
      "step": 62700
    },
    {
      "epoch": 1.282234518243257,
      "grad_norm": 11.054227828979492,
      "learning_rate": 1.1455036717915214e-05,
      "loss": 2.7386,
      "step": 62800
    },
    {
      "epoch": 1.2842762929538356,
      "grad_norm": 8.956287384033203,
      "learning_rate": 1.1441424886511356e-05,
      "loss": 2.6481,
      "step": 62900
    },
    {
      "epoch": 1.286318067664414,
      "grad_norm": 13.455086708068848,
      "learning_rate": 1.14278130551075e-05,
      "loss": 2.6968,
      "step": 63000
    },
    {
      "epoch": 1.2883598423749922,
      "grad_norm": 11.279464721679688,
      "learning_rate": 1.1414337342017682e-05,
      "loss": 2.6788,
      "step": 63100
    },
    {
      "epoch": 1.2904016170855708,
      "grad_norm": 9.60728931427002,
      "learning_rate": 1.1400725510613827e-05,
      "loss": 2.7024,
      "step": 63200
    },
    {
      "epoch": 1.2924433917961493,
      "grad_norm": 11.26557731628418,
      "learning_rate": 1.1387113679209971e-05,
      "loss": 2.663,
      "step": 63300
    },
    {
      "epoch": 1.2944851665067276,
      "grad_norm": 11.835006713867188,
      "learning_rate": 1.1373501847806114e-05,
      "loss": 2.6554,
      "step": 63400
    },
    {
      "epoch": 1.2965269412173062,
      "grad_norm": 9.76768684387207,
      "learning_rate": 1.1359890016402258e-05,
      "loss": 2.6494,
      "step": 63500
    },
    {
      "epoch": 1.2985687159278845,
      "grad_norm": 13.635590553283691,
      "learning_rate": 1.1346278184998401e-05,
      "loss": 2.6249,
      "step": 63600
    },
    {
      "epoch": 1.300610490638463,
      "grad_norm": 12.82800579071045,
      "learning_rate": 1.1332666353594545e-05,
      "loss": 2.6353,
      "step": 63700
    },
    {
      "epoch": 1.3026522653490413,
      "grad_norm": 10.669703483581543,
      "learning_rate": 1.131905452219069e-05,
      "loss": 2.7027,
      "step": 63800
    },
    {
      "epoch": 1.3046940400596199,
      "grad_norm": 10.618553161621094,
      "learning_rate": 1.1305442690786832e-05,
      "loss": 2.6859,
      "step": 63900
    },
    {
      "epoch": 1.3067358147701982,
      "grad_norm": 9.60444450378418,
      "learning_rate": 1.1291830859382977e-05,
      "loss": 2.6672,
      "step": 64000
    },
    {
      "epoch": 1.3087775894807767,
      "grad_norm": 11.81147289276123,
      "learning_rate": 1.127821902797912e-05,
      "loss": 2.6659,
      "step": 64100
    },
    {
      "epoch": 1.3108193641913553,
      "grad_norm": 11.943166732788086,
      "learning_rate": 1.1264607196575264e-05,
      "loss": 2.7164,
      "step": 64200
    },
    {
      "epoch": 1.3128611389019336,
      "grad_norm": 13.808034896850586,
      "learning_rate": 1.1250995365171409e-05,
      "loss": 2.625,
      "step": 64300
    },
    {
      "epoch": 1.3149029136125119,
      "grad_norm": 11.9350004196167,
      "learning_rate": 1.1237383533767551e-05,
      "loss": 2.6692,
      "step": 64400
    },
    {
      "epoch": 1.3169446883230904,
      "grad_norm": 12.65538215637207,
      "learning_rate": 1.1223771702363696e-05,
      "loss": 2.6776,
      "step": 64500
    },
    {
      "epoch": 1.318986463033669,
      "grad_norm": 13.072405815124512,
      "learning_rate": 1.121015987095984e-05,
      "loss": 2.7168,
      "step": 64600
    },
    {
      "epoch": 1.3210282377442473,
      "grad_norm": 10.333158493041992,
      "learning_rate": 1.1196684157870021e-05,
      "loss": 2.6182,
      "step": 64700
    },
    {
      "epoch": 1.3230700124548258,
      "grad_norm": 9.424715042114258,
      "learning_rate": 1.1183072326466164e-05,
      "loss": 2.6501,
      "step": 64800
    },
    {
      "epoch": 1.3251117871654041,
      "grad_norm": 10.606687545776367,
      "learning_rate": 1.116946049506231e-05,
      "loss": 2.6777,
      "step": 64900
    },
    {
      "epoch": 1.3271535618759827,
      "grad_norm": 11.284533500671387,
      "learning_rate": 1.1155848663658453e-05,
      "loss": 2.6592,
      "step": 65000
    },
    {
      "epoch": 1.329195336586561,
      "grad_norm": 11.088756561279297,
      "learning_rate": 1.1142236832254596e-05,
      "loss": 2.6999,
      "step": 65100
    },
    {
      "epoch": 1.3312371112971395,
      "grad_norm": 11.262408256530762,
      "learning_rate": 1.1128625000850742e-05,
      "loss": 2.6562,
      "step": 65200
    },
    {
      "epoch": 1.3332788860077178,
      "grad_norm": 10.050861358642578,
      "learning_rate": 1.1115013169446885e-05,
      "loss": 2.6434,
      "step": 65300
    },
    {
      "epoch": 1.3353206607182964,
      "grad_norm": 11.951255798339844,
      "learning_rate": 1.1101401338043027e-05,
      "loss": 2.6402,
      "step": 65400
    },
    {
      "epoch": 1.337362435428875,
      "grad_norm": 10.301958084106445,
      "learning_rate": 1.1087789506639173e-05,
      "loss": 2.7011,
      "step": 65500
    },
    {
      "epoch": 1.3394042101394532,
      "grad_norm": 11.0743408203125,
      "learning_rate": 1.1074177675235316e-05,
      "loss": 2.6581,
      "step": 65600
    },
    {
      "epoch": 1.3414459848500315,
      "grad_norm": 11.623933792114258,
      "learning_rate": 1.1060565843831459e-05,
      "loss": 2.6488,
      "step": 65700
    },
    {
      "epoch": 1.34348775956061,
      "grad_norm": 11.760126113891602,
      "learning_rate": 1.1046954012427602e-05,
      "loss": 2.6259,
      "step": 65800
    },
    {
      "epoch": 1.3455295342711886,
      "grad_norm": 11.600791931152344,
      "learning_rate": 1.1033342181023748e-05,
      "loss": 2.6193,
      "step": 65900
    },
    {
      "epoch": 1.347571308981767,
      "grad_norm": 11.846841812133789,
      "learning_rate": 1.101973034961989e-05,
      "loss": 2.6362,
      "step": 66000
    },
    {
      "epoch": 1.3496130836923454,
      "grad_norm": 11.693266868591309,
      "learning_rate": 1.1006118518216033e-05,
      "loss": 2.6884,
      "step": 66100
    },
    {
      "epoch": 1.3516548584029238,
      "grad_norm": 10.613070487976074,
      "learning_rate": 1.099250668681218e-05,
      "loss": 2.6445,
      "step": 66200
    },
    {
      "epoch": 1.3536966331135023,
      "grad_norm": 12.252334594726562,
      "learning_rate": 1.0978894855408322e-05,
      "loss": 2.6488,
      "step": 66300
    },
    {
      "epoch": 1.3557384078240806,
      "grad_norm": 10.303389549255371,
      "learning_rate": 1.0965283024004465e-05,
      "loss": 2.7006,
      "step": 66400
    },
    {
      "epoch": 1.3577801825346592,
      "grad_norm": 13.285029411315918,
      "learning_rate": 1.0951671192600611e-05,
      "loss": 2.6008,
      "step": 66500
    },
    {
      "epoch": 1.3598219572452375,
      "grad_norm": 8.796136856079102,
      "learning_rate": 1.0938059361196754e-05,
      "loss": 2.6638,
      "step": 66600
    },
    {
      "epoch": 1.361863731955816,
      "grad_norm": 10.795223236083984,
      "learning_rate": 1.0924447529792896e-05,
      "loss": 2.6701,
      "step": 66700
    },
    {
      "epoch": 1.3639055066663945,
      "grad_norm": 9.887625694274902,
      "learning_rate": 1.091083569838904e-05,
      "loss": 2.7124,
      "step": 66800
    },
    {
      "epoch": 1.3659472813769729,
      "grad_norm": 10.564854621887207,
      "learning_rate": 1.0897223866985185e-05,
      "loss": 2.6146,
      "step": 66900
    },
    {
      "epoch": 1.3679890560875512,
      "grad_norm": 9.82043170928955,
      "learning_rate": 1.0883612035581328e-05,
      "loss": 2.6674,
      "step": 67000
    },
    {
      "epoch": 1.3700308307981297,
      "grad_norm": 9.258429527282715,
      "learning_rate": 1.087000020417747e-05,
      "loss": 2.6194,
      "step": 67100
    },
    {
      "epoch": 1.3720726055087082,
      "grad_norm": 10.50685977935791,
      "learning_rate": 1.0856388372773617e-05,
      "loss": 2.6604,
      "step": 67200
    },
    {
      "epoch": 1.3741143802192866,
      "grad_norm": 11.478431701660156,
      "learning_rate": 1.084277654136976e-05,
      "loss": 2.6685,
      "step": 67300
    },
    {
      "epoch": 1.376156154929865,
      "grad_norm": 9.265799522399902,
      "learning_rate": 1.0829164709965902e-05,
      "loss": 2.6544,
      "step": 67400
    },
    {
      "epoch": 1.3781979296404434,
      "grad_norm": 10.930719375610352,
      "learning_rate": 1.0815552878562049e-05,
      "loss": 2.663,
      "step": 67500
    },
    {
      "epoch": 1.380239704351022,
      "grad_norm": 12.75462532043457,
      "learning_rate": 1.0801941047158191e-05,
      "loss": 2.697,
      "step": 67600
    },
    {
      "epoch": 1.3822814790616003,
      "grad_norm": 11.70858383178711,
      "learning_rate": 1.0788329215754334e-05,
      "loss": 2.6586,
      "step": 67700
    },
    {
      "epoch": 1.3843232537721788,
      "grad_norm": 10.375351905822754,
      "learning_rate": 1.077471738435048e-05,
      "loss": 2.5982,
      "step": 67800
    },
    {
      "epoch": 1.3863650284827571,
      "grad_norm": 10.86587142944336,
      "learning_rate": 1.0761105552946623e-05,
      "loss": 2.6371,
      "step": 67900
    },
    {
      "epoch": 1.3884068031933356,
      "grad_norm": 13.644376754760742,
      "learning_rate": 1.0747493721542766e-05,
      "loss": 2.6601,
      "step": 68000
    },
    {
      "epoch": 1.3904485779039142,
      "grad_norm": 10.014595031738281,
      "learning_rate": 1.0733881890138908e-05,
      "loss": 2.6734,
      "step": 68100
    },
    {
      "epoch": 1.3924903526144925,
      "grad_norm": 11.564864158630371,
      "learning_rate": 1.0720270058735054e-05,
      "loss": 2.6825,
      "step": 68200
    },
    {
      "epoch": 1.3945321273250708,
      "grad_norm": 9.743318557739258,
      "learning_rate": 1.0706794345645236e-05,
      "loss": 2.6394,
      "step": 68300
    },
    {
      "epoch": 1.3965739020356494,
      "grad_norm": 9.49928092956543,
      "learning_rate": 1.069318251424138e-05,
      "loss": 2.638,
      "step": 68400
    },
    {
      "epoch": 1.3986156767462279,
      "grad_norm": 9.8674898147583,
      "learning_rate": 1.0679570682837523e-05,
      "loss": 2.628,
      "step": 68500
    },
    {
      "epoch": 1.4006574514568062,
      "grad_norm": 11.708646774291992,
      "learning_rate": 1.0665958851433667e-05,
      "loss": 2.6382,
      "step": 68600
    },
    {
      "epoch": 1.4026992261673847,
      "grad_norm": 9.560049057006836,
      "learning_rate": 1.0652347020029812e-05,
      "loss": 2.6589,
      "step": 68700
    },
    {
      "epoch": 1.404741000877963,
      "grad_norm": 10.877113342285156,
      "learning_rate": 1.0638735188625954e-05,
      "loss": 2.6471,
      "step": 68800
    },
    {
      "epoch": 1.4067827755885416,
      "grad_norm": 10.933982849121094,
      "learning_rate": 1.0625123357222097e-05,
      "loss": 2.6845,
      "step": 68900
    },
    {
      "epoch": 1.4088245502991201,
      "grad_norm": 12.002400398254395,
      "learning_rate": 1.0611511525818242e-05,
      "loss": 2.6899,
      "step": 69000
    },
    {
      "epoch": 1.4108663250096984,
      "grad_norm": 11.113746643066406,
      "learning_rate": 1.0597899694414386e-05,
      "loss": 2.6702,
      "step": 69100
    },
    {
      "epoch": 1.4129080997202768,
      "grad_norm": 13.427568435668945,
      "learning_rate": 1.0584287863010529e-05,
      "loss": 2.6791,
      "step": 69200
    },
    {
      "epoch": 1.4149498744308553,
      "grad_norm": 11.475347518920898,
      "learning_rate": 1.0570676031606673e-05,
      "loss": 2.6508,
      "step": 69300
    },
    {
      "epoch": 1.4169916491414338,
      "grad_norm": 11.513496398925781,
      "learning_rate": 1.0557064200202818e-05,
      "loss": 2.7079,
      "step": 69400
    },
    {
      "epoch": 1.4190334238520121,
      "grad_norm": 11.420552253723145,
      "learning_rate": 1.054345236879896e-05,
      "loss": 2.643,
      "step": 69500
    },
    {
      "epoch": 1.4210751985625907,
      "grad_norm": 12.698193550109863,
      "learning_rate": 1.0529840537395105e-05,
      "loss": 2.6793,
      "step": 69600
    },
    {
      "epoch": 1.423116973273169,
      "grad_norm": 10.899344444274902,
      "learning_rate": 1.051622870599125e-05,
      "loss": 2.6893,
      "step": 69700
    },
    {
      "epoch": 1.4251587479837475,
      "grad_norm": 11.19398307800293,
      "learning_rate": 1.0502616874587392e-05,
      "loss": 2.6848,
      "step": 69800
    },
    {
      "epoch": 1.4272005226943258,
      "grad_norm": 9.15616226196289,
      "learning_rate": 1.0489005043183535e-05,
      "loss": 2.6864,
      "step": 69900
    },
    {
      "epoch": 1.4292422974049044,
      "grad_norm": 10.35210132598877,
      "learning_rate": 1.0475393211779681e-05,
      "loss": 2.7105,
      "step": 70000
    },
    {
      "epoch": 1.4312840721154827,
      "grad_norm": 10.404836654663086,
      "learning_rate": 1.0461781380375824e-05,
      "loss": 2.6698,
      "step": 70100
    },
    {
      "epoch": 1.4333258468260612,
      "grad_norm": 9.410874366760254,
      "learning_rate": 1.0448169548971966e-05,
      "loss": 2.6692,
      "step": 70200
    },
    {
      "epoch": 1.4353676215366398,
      "grad_norm": 11.321573257446289,
      "learning_rate": 1.043455771756811e-05,
      "loss": 2.6421,
      "step": 70300
    },
    {
      "epoch": 1.437409396247218,
      "grad_norm": 10.088655471801758,
      "learning_rate": 1.0420945886164255e-05,
      "loss": 2.6912,
      "step": 70400
    },
    {
      "epoch": 1.4394511709577964,
      "grad_norm": 10.737683296203613,
      "learning_rate": 1.0407334054760398e-05,
      "loss": 2.7081,
      "step": 70500
    },
    {
      "epoch": 1.441492945668375,
      "grad_norm": 12.927143096923828,
      "learning_rate": 1.0393722223356542e-05,
      "loss": 2.6473,
      "step": 70600
    },
    {
      "epoch": 1.4435347203789535,
      "grad_norm": 10.401389122009277,
      "learning_rate": 1.0380110391952687e-05,
      "loss": 2.6893,
      "step": 70700
    },
    {
      "epoch": 1.4455764950895318,
      "grad_norm": 10.50200080871582,
      "learning_rate": 1.036649856054883e-05,
      "loss": 2.6245,
      "step": 70800
    },
    {
      "epoch": 1.4476182698001103,
      "grad_norm": 11.601763725280762,
      "learning_rate": 1.0352886729144972e-05,
      "loss": 2.6776,
      "step": 70900
    },
    {
      "epoch": 1.4496600445106886,
      "grad_norm": 12.074234962463379,
      "learning_rate": 1.0339274897741118e-05,
      "loss": 2.621,
      "step": 71000
    },
    {
      "epoch": 1.4517018192212672,
      "grad_norm": 11.663089752197266,
      "learning_rate": 1.0325663066337261e-05,
      "loss": 2.6061,
      "step": 71100
    },
    {
      "epoch": 1.4537435939318455,
      "grad_norm": 11.827544212341309,
      "learning_rate": 1.0312051234933404e-05,
      "loss": 2.6544,
      "step": 71200
    },
    {
      "epoch": 1.455785368642424,
      "grad_norm": 11.684276580810547,
      "learning_rate": 1.0298439403529548e-05,
      "loss": 2.6705,
      "step": 71300
    },
    {
      "epoch": 1.4578271433530023,
      "grad_norm": 12.881497383117676,
      "learning_rate": 1.0284827572125693e-05,
      "loss": 2.6752,
      "step": 71400
    },
    {
      "epoch": 1.4598689180635809,
      "grad_norm": 10.295782089233398,
      "learning_rate": 1.0271215740721835e-05,
      "loss": 2.6767,
      "step": 71500
    },
    {
      "epoch": 1.4619106927741594,
      "grad_norm": 11.266791343688965,
      "learning_rate": 1.025760390931798e-05,
      "loss": 2.6591,
      "step": 71600
    },
    {
      "epoch": 1.4639524674847377,
      "grad_norm": 12.83981704711914,
      "learning_rate": 1.0243992077914124e-05,
      "loss": 2.6933,
      "step": 71700
    },
    {
      "epoch": 1.465994242195316,
      "grad_norm": 11.093117713928223,
      "learning_rate": 1.0230380246510267e-05,
      "loss": 2.6981,
      "step": 71800
    },
    {
      "epoch": 1.4680360169058946,
      "grad_norm": 12.050463676452637,
      "learning_rate": 1.021676841510641e-05,
      "loss": 2.6558,
      "step": 71900
    },
    {
      "epoch": 1.4700777916164731,
      "grad_norm": 10.843950271606445,
      "learning_rate": 1.0203156583702556e-05,
      "loss": 2.6088,
      "step": 72000
    },
    {
      "epoch": 1.4721195663270514,
      "grad_norm": 11.340019226074219,
      "learning_rate": 1.0189544752298699e-05,
      "loss": 2.6308,
      "step": 72100
    },
    {
      "epoch": 1.47416134103763,
      "grad_norm": 12.048181533813477,
      "learning_rate": 1.0175932920894841e-05,
      "loss": 2.6721,
      "step": 72200
    },
    {
      "epoch": 1.4762031157482083,
      "grad_norm": 9.875730514526367,
      "learning_rate": 1.0162321089490988e-05,
      "loss": 2.6286,
      "step": 72300
    },
    {
      "epoch": 1.4782448904587868,
      "grad_norm": 10.80497932434082,
      "learning_rate": 1.014870925808713e-05,
      "loss": 2.5589,
      "step": 72400
    },
    {
      "epoch": 1.4802866651693651,
      "grad_norm": 8.738961219787598,
      "learning_rate": 1.0135097426683273e-05,
      "loss": 2.6116,
      "step": 72500
    },
    {
      "epoch": 1.4823284398799437,
      "grad_norm": 10.585323333740234,
      "learning_rate": 1.0121485595279417e-05,
      "loss": 2.613,
      "step": 72600
    },
    {
      "epoch": 1.484370214590522,
      "grad_norm": 9.520243644714355,
      "learning_rate": 1.0107873763875562e-05,
      "loss": 2.5866,
      "step": 72700
    },
    {
      "epoch": 1.4864119893011005,
      "grad_norm": 11.548364639282227,
      "learning_rate": 1.0094261932471705e-05,
      "loss": 2.6638,
      "step": 72800
    },
    {
      "epoch": 1.488453764011679,
      "grad_norm": 10.097542762756348,
      "learning_rate": 1.0080650101067847e-05,
      "loss": 2.6638,
      "step": 72900
    },
    {
      "epoch": 1.4904955387222574,
      "grad_norm": 9.736104011535645,
      "learning_rate": 1.0067038269663994e-05,
      "loss": 2.6147,
      "step": 73000
    },
    {
      "epoch": 1.4925373134328357,
      "grad_norm": 12.369684219360352,
      "learning_rate": 1.0053426438260136e-05,
      "loss": 2.6616,
      "step": 73100
    },
    {
      "epoch": 1.4945790881434142,
      "grad_norm": 11.31441593170166,
      "learning_rate": 1.0039814606856279e-05,
      "loss": 2.632,
      "step": 73200
    },
    {
      "epoch": 1.4966208628539928,
      "grad_norm": 9.519659996032715,
      "learning_rate": 1.0026202775452425e-05,
      "loss": 2.6701,
      "step": 73300
    },
    {
      "epoch": 1.498662637564571,
      "grad_norm": 11.419496536254883,
      "learning_rate": 1.0012590944048568e-05,
      "loss": 2.6413,
      "step": 73400
    },
    {
      "epoch": 1.5007044122751496,
      "grad_norm": 11.29309368133545,
      "learning_rate": 9.99897911264471e-06,
      "loss": 2.6654,
      "step": 73500
    },
    {
      "epoch": 1.502746186985728,
      "grad_norm": 12.549348831176758,
      "learning_rate": 9.985367281240855e-06,
      "loss": 2.5802,
      "step": 73600
    },
    {
      "epoch": 1.5047879616963065,
      "grad_norm": 9.553194046020508,
      "learning_rate": 9.971755449837e-06,
      "loss": 2.6377,
      "step": 73700
    },
    {
      "epoch": 1.506829736406885,
      "grad_norm": 11.33380126953125,
      "learning_rate": 9.958143618433142e-06,
      "loss": 2.6033,
      "step": 73800
    },
    {
      "epoch": 1.5088715111174633,
      "grad_norm": 10.463532447814941,
      "learning_rate": 9.944531787029287e-06,
      "loss": 2.6601,
      "step": 73900
    },
    {
      "epoch": 1.5109132858280416,
      "grad_norm": 12.730266571044922,
      "learning_rate": 9.930919955625431e-06,
      "loss": 2.5885,
      "step": 74000
    },
    {
      "epoch": 1.5129550605386202,
      "grad_norm": 11.012474060058594,
      "learning_rate": 9.917308124221574e-06,
      "loss": 2.6176,
      "step": 74100
    },
    {
      "epoch": 1.5149968352491987,
      "grad_norm": 10.905893325805664,
      "learning_rate": 9.903832411131757e-06,
      "loss": 2.6824,
      "step": 74200
    },
    {
      "epoch": 1.517038609959777,
      "grad_norm": 8.488433837890625,
      "learning_rate": 9.890220579727901e-06,
      "loss": 2.6666,
      "step": 74300
    },
    {
      "epoch": 1.5190803846703553,
      "grad_norm": 11.61864185333252,
      "learning_rate": 9.876608748324044e-06,
      "loss": 2.6613,
      "step": 74400
    },
    {
      "epoch": 1.5211221593809339,
      "grad_norm": 11.459264755249023,
      "learning_rate": 9.862996916920188e-06,
      "loss": 2.6259,
      "step": 74500
    },
    {
      "epoch": 1.5231639340915124,
      "grad_norm": 11.364384651184082,
      "learning_rate": 9.849385085516333e-06,
      "loss": 2.7008,
      "step": 74600
    },
    {
      "epoch": 1.525205708802091,
      "grad_norm": 9.481698036193848,
      "learning_rate": 9.835773254112475e-06,
      "loss": 2.6264,
      "step": 74700
    },
    {
      "epoch": 1.5272474835126693,
      "grad_norm": 10.847887992858887,
      "learning_rate": 9.82216142270862e-06,
      "loss": 2.627,
      "step": 74800
    },
    {
      "epoch": 1.5292892582232476,
      "grad_norm": 12.87877082824707,
      "learning_rate": 9.808549591304763e-06,
      "loss": 2.6514,
      "step": 74900
    },
    {
      "epoch": 1.531331032933826,
      "grad_norm": 12.125564575195312,
      "learning_rate": 9.794937759900907e-06,
      "loss": 2.7108,
      "step": 75000
    },
    {
      "epoch": 1.5333728076444046,
      "grad_norm": 8.988897323608398,
      "learning_rate": 9.781325928497052e-06,
      "loss": 2.7007,
      "step": 75100
    },
    {
      "epoch": 1.535414582354983,
      "grad_norm": 10.0047607421875,
      "learning_rate": 9.767714097093194e-06,
      "loss": 2.6339,
      "step": 75200
    },
    {
      "epoch": 1.5374563570655613,
      "grad_norm": 10.7399320602417,
      "learning_rate": 9.754102265689339e-06,
      "loss": 2.6735,
      "step": 75300
    },
    {
      "epoch": 1.5394981317761398,
      "grad_norm": 12.777371406555176,
      "learning_rate": 9.740490434285481e-06,
      "loss": 2.6718,
      "step": 75400
    },
    {
      "epoch": 1.5415399064867183,
      "grad_norm": 13.031229019165039,
      "learning_rate": 9.726878602881626e-06,
      "loss": 2.6342,
      "step": 75500
    },
    {
      "epoch": 1.5435816811972967,
      "grad_norm": 11.371675491333008,
      "learning_rate": 9.71326677147777e-06,
      "loss": 2.6228,
      "step": 75600
    },
    {
      "epoch": 1.545623455907875,
      "grad_norm": 9.614875793457031,
      "learning_rate": 9.699654940073913e-06,
      "loss": 2.6236,
      "step": 75700
    },
    {
      "epoch": 1.5476652306184535,
      "grad_norm": 9.551836013793945,
      "learning_rate": 9.686043108670057e-06,
      "loss": 2.665,
      "step": 75800
    },
    {
      "epoch": 1.549707005329032,
      "grad_norm": 12.931951522827148,
      "learning_rate": 9.6724312772662e-06,
      "loss": 2.662,
      "step": 75900
    },
    {
      "epoch": 1.5517487800396106,
      "grad_norm": 10.03196907043457,
      "learning_rate": 9.658819445862345e-06,
      "loss": 2.662,
      "step": 76000
    },
    {
      "epoch": 1.553790554750189,
      "grad_norm": 10.041319847106934,
      "learning_rate": 9.645207614458489e-06,
      "loss": 2.6251,
      "step": 76100
    },
    {
      "epoch": 1.5558323294607672,
      "grad_norm": 11.111949920654297,
      "learning_rate": 9.631595783054632e-06,
      "loss": 2.6594,
      "step": 76200
    },
    {
      "epoch": 1.5578741041713458,
      "grad_norm": 9.124128341674805,
      "learning_rate": 9.617983951650776e-06,
      "loss": 2.7057,
      "step": 76300
    },
    {
      "epoch": 1.5599158788819243,
      "grad_norm": 12.086596488952637,
      "learning_rate": 9.604372120246919e-06,
      "loss": 2.6798,
      "step": 76400
    },
    {
      "epoch": 1.5619576535925026,
      "grad_norm": 10.432720184326172,
      "learning_rate": 9.590760288843063e-06,
      "loss": 2.6664,
      "step": 76500
    },
    {
      "epoch": 1.563999428303081,
      "grad_norm": 10.590959548950195,
      "learning_rate": 9.577148457439208e-06,
      "loss": 2.6952,
      "step": 76600
    },
    {
      "epoch": 1.5660412030136595,
      "grad_norm": 10.379766464233398,
      "learning_rate": 9.56353662603535e-06,
      "loss": 2.5647,
      "step": 76700
    },
    {
      "epoch": 1.568082977724238,
      "grad_norm": 11.48452377319336,
      "learning_rate": 9.549924794631495e-06,
      "loss": 2.5953,
      "step": 76800
    },
    {
      "epoch": 1.5701247524348163,
      "grad_norm": 9.980502128601074,
      "learning_rate": 9.536312963227638e-06,
      "loss": 2.5945,
      "step": 76900
    },
    {
      "epoch": 1.5721665271453946,
      "grad_norm": 9.540800094604492,
      "learning_rate": 9.522701131823782e-06,
      "loss": 2.6058,
      "step": 77000
    },
    {
      "epoch": 1.5742083018559732,
      "grad_norm": 13.0447998046875,
      "learning_rate": 9.509089300419927e-06,
      "loss": 2.591,
      "step": 77100
    },
    {
      "epoch": 1.5762500765665517,
      "grad_norm": 10.591147422790527,
      "learning_rate": 9.49547746901607e-06,
      "loss": 2.6286,
      "step": 77200
    },
    {
      "epoch": 1.5782918512771302,
      "grad_norm": 10.719817161560059,
      "learning_rate": 9.481865637612214e-06,
      "loss": 2.6202,
      "step": 77300
    },
    {
      "epoch": 1.5803336259877085,
      "grad_norm": 12.019288063049316,
      "learning_rate": 9.468253806208357e-06,
      "loss": 2.6332,
      "step": 77400
    },
    {
      "epoch": 1.5823754006982869,
      "grad_norm": 13.598718643188477,
      "learning_rate": 9.454641974804501e-06,
      "loss": 2.6506,
      "step": 77500
    },
    {
      "epoch": 1.5844171754088654,
      "grad_norm": 11.127206802368164,
      "learning_rate": 9.441030143400645e-06,
      "loss": 2.6381,
      "step": 77600
    },
    {
      "epoch": 1.586458950119444,
      "grad_norm": 11.200579643249512,
      "learning_rate": 9.427418311996788e-06,
      "loss": 2.6028,
      "step": 77700
    },
    {
      "epoch": 1.5885007248300222,
      "grad_norm": 11.628145217895508,
      "learning_rate": 9.413806480592933e-06,
      "loss": 2.622,
      "step": 77800
    },
    {
      "epoch": 1.5905424995406006,
      "grad_norm": 11.902078628540039,
      "learning_rate": 9.400194649189075e-06,
      "loss": 2.5729,
      "step": 77900
    },
    {
      "epoch": 1.592584274251179,
      "grad_norm": 11.414945602416992,
      "learning_rate": 9.38658281778522e-06,
      "loss": 2.6219,
      "step": 78000
    },
    {
      "epoch": 1.5946260489617576,
      "grad_norm": 11.101999282836914,
      "learning_rate": 9.372970986381364e-06,
      "loss": 2.5825,
      "step": 78100
    },
    {
      "epoch": 1.596667823672336,
      "grad_norm": 12.002384185791016,
      "learning_rate": 9.359359154977507e-06,
      "loss": 2.5981,
      "step": 78200
    },
    {
      "epoch": 1.5987095983829143,
      "grad_norm": 10.766691207885742,
      "learning_rate": 9.345747323573651e-06,
      "loss": 2.612,
      "step": 78300
    },
    {
      "epoch": 1.6007513730934928,
      "grad_norm": 11.193580627441406,
      "learning_rate": 9.332271610483833e-06,
      "loss": 2.5707,
      "step": 78400
    },
    {
      "epoch": 1.6027931478040713,
      "grad_norm": 10.51591682434082,
      "learning_rate": 9.318659779079977e-06,
      "loss": 2.6351,
      "step": 78500
    },
    {
      "epoch": 1.6048349225146499,
      "grad_norm": 10.062310218811035,
      "learning_rate": 9.305047947676121e-06,
      "loss": 2.6534,
      "step": 78600
    },
    {
      "epoch": 1.6068766972252282,
      "grad_norm": 9.447997093200684,
      "learning_rate": 9.291436116272264e-06,
      "loss": 2.6291,
      "step": 78700
    },
    {
      "epoch": 1.6089184719358065,
      "grad_norm": 10.55007553100586,
      "learning_rate": 9.277824284868409e-06,
      "loss": 2.6876,
      "step": 78800
    },
    {
      "epoch": 1.610960246646385,
      "grad_norm": 11.715755462646484,
      "learning_rate": 9.264212453464551e-06,
      "loss": 2.6688,
      "step": 78900
    },
    {
      "epoch": 1.6130020213569636,
      "grad_norm": 11.676541328430176,
      "learning_rate": 9.250600622060696e-06,
      "loss": 2.6408,
      "step": 79000
    },
    {
      "epoch": 1.615043796067542,
      "grad_norm": 9.897549629211426,
      "learning_rate": 9.23698879065684e-06,
      "loss": 2.6063,
      "step": 79100
    },
    {
      "epoch": 1.6170855707781202,
      "grad_norm": 11.932965278625488,
      "learning_rate": 9.223376959252983e-06,
      "loss": 2.6611,
      "step": 79200
    },
    {
      "epoch": 1.6191273454886987,
      "grad_norm": 9.23597526550293,
      "learning_rate": 9.209765127849127e-06,
      "loss": 2.6246,
      "step": 79300
    },
    {
      "epoch": 1.6211691201992773,
      "grad_norm": 12.709718704223633,
      "learning_rate": 9.19615329644527e-06,
      "loss": 2.6461,
      "step": 79400
    },
    {
      "epoch": 1.6232108949098556,
      "grad_norm": 10.082502365112305,
      "learning_rate": 9.182541465041415e-06,
      "loss": 2.6791,
      "step": 79500
    },
    {
      "epoch": 1.6252526696204341,
      "grad_norm": 8.623141288757324,
      "learning_rate": 9.168929633637559e-06,
      "loss": 2.6596,
      "step": 79600
    },
    {
      "epoch": 1.6272944443310124,
      "grad_norm": 11.851866722106934,
      "learning_rate": 9.155317802233702e-06,
      "loss": 2.6643,
      "step": 79700
    },
    {
      "epoch": 1.629336219041591,
      "grad_norm": 13.509984970092773,
      "learning_rate": 9.141705970829846e-06,
      "loss": 2.6015,
      "step": 79800
    },
    {
      "epoch": 1.6313779937521695,
      "grad_norm": 11.440654754638672,
      "learning_rate": 9.12809413942599e-06,
      "loss": 2.6495,
      "step": 79900
    },
    {
      "epoch": 1.6334197684627478,
      "grad_norm": 10.129413604736328,
      "learning_rate": 9.114482308022133e-06,
      "loss": 2.6225,
      "step": 80000
    },
    {
      "epoch": 1.6354615431733261,
      "grad_norm": 9.377029418945312,
      "learning_rate": 9.100870476618278e-06,
      "loss": 2.6002,
      "step": 80100
    },
    {
      "epoch": 1.6375033178839047,
      "grad_norm": 11.755199432373047,
      "learning_rate": 9.08725864521442e-06,
      "loss": 2.5917,
      "step": 80200
    },
    {
      "epoch": 1.6395450925944832,
      "grad_norm": 10.192444801330566,
      "learning_rate": 9.073646813810565e-06,
      "loss": 2.611,
      "step": 80300
    },
    {
      "epoch": 1.6415868673050615,
      "grad_norm": 9.856821060180664,
      "learning_rate": 9.06003498240671e-06,
      "loss": 2.6092,
      "step": 80400
    },
    {
      "epoch": 1.6436286420156399,
      "grad_norm": 11.120089530944824,
      "learning_rate": 9.04655926931689e-06,
      "loss": 2.6005,
      "step": 80500
    },
    {
      "epoch": 1.6456704167262184,
      "grad_norm": 9.58932113647461,
      "learning_rate": 9.032947437913035e-06,
      "loss": 2.6441,
      "step": 80600
    },
    {
      "epoch": 1.647712191436797,
      "grad_norm": 13.445854187011719,
      "learning_rate": 9.01933560650918e-06,
      "loss": 2.6543,
      "step": 80700
    },
    {
      "epoch": 1.6497539661473755,
      "grad_norm": 11.94597339630127,
      "learning_rate": 9.005723775105322e-06,
      "loss": 2.6188,
      "step": 80800
    },
    {
      "epoch": 1.6517957408579538,
      "grad_norm": 10.213778495788574,
      "learning_rate": 8.992111943701467e-06,
      "loss": 2.6379,
      "step": 80900
    },
    {
      "epoch": 1.653837515568532,
      "grad_norm": 11.17185115814209,
      "learning_rate": 8.97850011229761e-06,
      "loss": 2.6102,
      "step": 81000
    },
    {
      "epoch": 1.6558792902791106,
      "grad_norm": 12.314606666564941,
      "learning_rate": 8.964888280893754e-06,
      "loss": 2.5897,
      "step": 81100
    },
    {
      "epoch": 1.6579210649896892,
      "grad_norm": 10.724912643432617,
      "learning_rate": 8.951276449489898e-06,
      "loss": 2.6321,
      "step": 81200
    },
    {
      "epoch": 1.6599628397002675,
      "grad_norm": 11.185689926147461,
      "learning_rate": 8.937664618086041e-06,
      "loss": 2.5924,
      "step": 81300
    },
    {
      "epoch": 1.6620046144108458,
      "grad_norm": 10.680347442626953,
      "learning_rate": 8.924052786682185e-06,
      "loss": 2.6052,
      "step": 81400
    },
    {
      "epoch": 1.6640463891214243,
      "grad_norm": 12.040863037109375,
      "learning_rate": 8.910440955278328e-06,
      "loss": 2.6385,
      "step": 81500
    },
    {
      "epoch": 1.6660881638320029,
      "grad_norm": 10.489866256713867,
      "learning_rate": 8.896829123874473e-06,
      "loss": 2.6003,
      "step": 81600
    },
    {
      "epoch": 1.6681299385425812,
      "grad_norm": 10.497049331665039,
      "learning_rate": 8.883217292470617e-06,
      "loss": 2.5994,
      "step": 81700
    },
    {
      "epoch": 1.6701717132531595,
      "grad_norm": 10.845931053161621,
      "learning_rate": 8.86960546106676e-06,
      "loss": 2.6189,
      "step": 81800
    },
    {
      "epoch": 1.672213487963738,
      "grad_norm": 11.384323120117188,
      "learning_rate": 8.855993629662904e-06,
      "loss": 2.6258,
      "step": 81900
    },
    {
      "epoch": 1.6742552626743166,
      "grad_norm": 11.553805351257324,
      "learning_rate": 8.842381798259047e-06,
      "loss": 2.5872,
      "step": 82000
    },
    {
      "epoch": 1.676297037384895,
      "grad_norm": 11.966096878051758,
      "learning_rate": 8.828769966855191e-06,
      "loss": 2.6131,
      "step": 82100
    },
    {
      "epoch": 1.6783388120954734,
      "grad_norm": 11.106145858764648,
      "learning_rate": 8.815158135451336e-06,
      "loss": 2.6353,
      "step": 82200
    },
    {
      "epoch": 1.6803805868060517,
      "grad_norm": 10.714751243591309,
      "learning_rate": 8.801546304047478e-06,
      "loss": 2.6112,
      "step": 82300
    },
    {
      "epoch": 1.6824223615166303,
      "grad_norm": 11.420049667358398,
      "learning_rate": 8.787934472643623e-06,
      "loss": 2.5942,
      "step": 82400
    },
    {
      "epoch": 1.6844641362272088,
      "grad_norm": 16.623291015625,
      "learning_rate": 8.774322641239766e-06,
      "loss": 2.6672,
      "step": 82500
    },
    {
      "epoch": 1.6865059109377871,
      "grad_norm": 10.344131469726562,
      "learning_rate": 8.76071080983591e-06,
      "loss": 2.6404,
      "step": 82600
    },
    {
      "epoch": 1.6885476856483654,
      "grad_norm": 13.541666030883789,
      "learning_rate": 8.747235096746093e-06,
      "loss": 2.68,
      "step": 82700
    },
    {
      "epoch": 1.690589460358944,
      "grad_norm": 12.383222579956055,
      "learning_rate": 8.733623265342236e-06,
      "loss": 2.6635,
      "step": 82800
    },
    {
      "epoch": 1.6926312350695225,
      "grad_norm": 10.873312950134277,
      "learning_rate": 8.72001143393838e-06,
      "loss": 2.6149,
      "step": 82900
    },
    {
      "epoch": 1.6946730097801008,
      "grad_norm": 11.482292175292969,
      "learning_rate": 8.706399602534523e-06,
      "loss": 2.5925,
      "step": 83000
    },
    {
      "epoch": 1.6967147844906791,
      "grad_norm": 12.138456344604492,
      "learning_rate": 8.692787771130667e-06,
      "loss": 2.6594,
      "step": 83100
    },
    {
      "epoch": 1.6987565592012577,
      "grad_norm": 12.078683853149414,
      "learning_rate": 8.679175939726812e-06,
      "loss": 2.6575,
      "step": 83200
    },
    {
      "epoch": 1.7007983339118362,
      "grad_norm": 11.314376831054688,
      "learning_rate": 8.665564108322955e-06,
      "loss": 2.6143,
      "step": 83300
    },
    {
      "epoch": 1.7028401086224147,
      "grad_norm": 10.844033241271973,
      "learning_rate": 8.651952276919099e-06,
      "loss": 2.6471,
      "step": 83400
    },
    {
      "epoch": 1.704881883332993,
      "grad_norm": 12.865822792053223,
      "learning_rate": 8.638340445515243e-06,
      "loss": 2.6662,
      "step": 83500
    },
    {
      "epoch": 1.7069236580435714,
      "grad_norm": 14.015544891357422,
      "learning_rate": 8.624728614111386e-06,
      "loss": 2.6248,
      "step": 83600
    },
    {
      "epoch": 1.70896543275415,
      "grad_norm": 9.800848960876465,
      "learning_rate": 8.61111678270753e-06,
      "loss": 2.6116,
      "step": 83700
    },
    {
      "epoch": 1.7110072074647285,
      "grad_norm": 11.932181358337402,
      "learning_rate": 8.597504951303673e-06,
      "loss": 2.6456,
      "step": 83800
    },
    {
      "epoch": 1.7130489821753068,
      "grad_norm": 10.780303955078125,
      "learning_rate": 8.583893119899818e-06,
      "loss": 2.606,
      "step": 83900
    },
    {
      "epoch": 1.715090756885885,
      "grad_norm": 13.662392616271973,
      "learning_rate": 8.570281288495962e-06,
      "loss": 2.5857,
      "step": 84000
    },
    {
      "epoch": 1.7171325315964636,
      "grad_norm": 10.507661819458008,
      "learning_rate": 8.556669457092105e-06,
      "loss": 2.6171,
      "step": 84100
    },
    {
      "epoch": 1.7191743063070422,
      "grad_norm": 11.590235710144043,
      "learning_rate": 8.54305762568825e-06,
      "loss": 2.6097,
      "step": 84200
    },
    {
      "epoch": 1.7212160810176205,
      "grad_norm": 11.00464916229248,
      "learning_rate": 8.529445794284392e-06,
      "loss": 2.6046,
      "step": 84300
    },
    {
      "epoch": 1.7232578557281988,
      "grad_norm": 10.455134391784668,
      "learning_rate": 8.515833962880537e-06,
      "loss": 2.569,
      "step": 84400
    },
    {
      "epoch": 1.7252996304387773,
      "grad_norm": 9.981289863586426,
      "learning_rate": 8.502222131476681e-06,
      "loss": 2.5944,
      "step": 84500
    },
    {
      "epoch": 1.7273414051493559,
      "grad_norm": 11.763368606567383,
      "learning_rate": 8.488610300072824e-06,
      "loss": 2.5941,
      "step": 84600
    },
    {
      "epoch": 1.7293831798599344,
      "grad_norm": 10.51573657989502,
      "learning_rate": 8.474998468668968e-06,
      "loss": 2.6051,
      "step": 84700
    },
    {
      "epoch": 1.7314249545705127,
      "grad_norm": 9.615549087524414,
      "learning_rate": 8.46138663726511e-06,
      "loss": 2.6084,
      "step": 84800
    },
    {
      "epoch": 1.733466729281091,
      "grad_norm": 10.927197456359863,
      "learning_rate": 8.447774805861255e-06,
      "loss": 2.623,
      "step": 84900
    },
    {
      "epoch": 1.7355085039916696,
      "grad_norm": 11.18294906616211,
      "learning_rate": 8.434299092771438e-06,
      "loss": 2.5339,
      "step": 85000
    },
    {
      "epoch": 1.737550278702248,
      "grad_norm": 10.735185623168945,
      "learning_rate": 8.420687261367581e-06,
      "loss": 2.6453,
      "step": 85100
    },
    {
      "epoch": 1.7395920534128264,
      "grad_norm": 10.243576049804688,
      "learning_rate": 8.407075429963725e-06,
      "loss": 2.6105,
      "step": 85200
    },
    {
      "epoch": 1.7416338281234047,
      "grad_norm": 11.717653274536133,
      "learning_rate": 8.393463598559868e-06,
      "loss": 2.6432,
      "step": 85300
    },
    {
      "epoch": 1.7436756028339833,
      "grad_norm": 10.84801959991455,
      "learning_rate": 8.379851767156013e-06,
      "loss": 2.5734,
      "step": 85400
    },
    {
      "epoch": 1.7457173775445618,
      "grad_norm": 11.217552185058594,
      "learning_rate": 8.366239935752157e-06,
      "loss": 2.6095,
      "step": 85500
    },
    {
      "epoch": 1.7477591522551401,
      "grad_norm": 11.279112815856934,
      "learning_rate": 8.3526281043483e-06,
      "loss": 2.6625,
      "step": 85600
    },
    {
      "epoch": 1.7498009269657187,
      "grad_norm": 11.803088188171387,
      "learning_rate": 8.339016272944444e-06,
      "loss": 2.684,
      "step": 85700
    },
    {
      "epoch": 1.751842701676297,
      "grad_norm": 10.321946144104004,
      "learning_rate": 8.325404441540587e-06,
      "loss": 2.6117,
      "step": 85800
    },
    {
      "epoch": 1.7538844763868755,
      "grad_norm": 12.983062744140625,
      "learning_rate": 8.311792610136731e-06,
      "loss": 2.6505,
      "step": 85900
    },
    {
      "epoch": 1.755926251097454,
      "grad_norm": 10.016766548156738,
      "learning_rate": 8.298180778732876e-06,
      "loss": 2.5966,
      "step": 86000
    },
    {
      "epoch": 1.7579680258080324,
      "grad_norm": 12.313535690307617,
      "learning_rate": 8.284568947329018e-06,
      "loss": 2.6778,
      "step": 86100
    },
    {
      "epoch": 1.7600098005186107,
      "grad_norm": 10.605435371398926,
      "learning_rate": 8.270957115925163e-06,
      "loss": 2.5915,
      "step": 86200
    },
    {
      "epoch": 1.7620515752291892,
      "grad_norm": 11.266799926757812,
      "learning_rate": 8.257345284521306e-06,
      "loss": 2.6127,
      "step": 86300
    },
    {
      "epoch": 1.7640933499397677,
      "grad_norm": 12.041533470153809,
      "learning_rate": 8.24373345311745e-06,
      "loss": 2.5587,
      "step": 86400
    },
    {
      "epoch": 1.766135124650346,
      "grad_norm": 12.206639289855957,
      "learning_rate": 8.230121621713595e-06,
      "loss": 2.6721,
      "step": 86500
    },
    {
      "epoch": 1.7681768993609244,
      "grad_norm": 11.343413352966309,
      "learning_rate": 8.216509790309737e-06,
      "loss": 2.6668,
      "step": 86600
    },
    {
      "epoch": 1.770218674071503,
      "grad_norm": 9.84118938446045,
      "learning_rate": 8.202897958905882e-06,
      "loss": 2.5901,
      "step": 86700
    },
    {
      "epoch": 1.7722604487820814,
      "grad_norm": 13.285736083984375,
      "learning_rate": 8.189286127502024e-06,
      "loss": 2.5916,
      "step": 86800
    },
    {
      "epoch": 1.77430222349266,
      "grad_norm": 13.75151538848877,
      "learning_rate": 8.175674296098169e-06,
      "loss": 2.6616,
      "step": 86900
    },
    {
      "epoch": 1.7763439982032383,
      "grad_norm": 13.206014633178711,
      "learning_rate": 8.162062464694313e-06,
      "loss": 2.6182,
      "step": 87000
    },
    {
      "epoch": 1.7783857729138166,
      "grad_norm": 11.262364387512207,
      "learning_rate": 8.148586751604494e-06,
      "loss": 2.6359,
      "step": 87100
    },
    {
      "epoch": 1.7804275476243951,
      "grad_norm": 12.911859512329102,
      "learning_rate": 8.134974920200639e-06,
      "loss": 2.5685,
      "step": 87200
    },
    {
      "epoch": 1.7824693223349737,
      "grad_norm": 11.246148109436035,
      "learning_rate": 8.121363088796783e-06,
      "loss": 2.6266,
      "step": 87300
    },
    {
      "epoch": 1.784511097045552,
      "grad_norm": 11.497746467590332,
      "learning_rate": 8.107751257392926e-06,
      "loss": 2.6026,
      "step": 87400
    },
    {
      "epoch": 1.7865528717561303,
      "grad_norm": 8.499737739562988,
      "learning_rate": 8.09413942598907e-06,
      "loss": 2.5688,
      "step": 87500
    },
    {
      "epoch": 1.7885946464667088,
      "grad_norm": 10.719858169555664,
      "learning_rate": 8.080527594585215e-06,
      "loss": 2.5827,
      "step": 87600
    },
    {
      "epoch": 1.7906364211772874,
      "grad_norm": 9.540788650512695,
      "learning_rate": 8.066915763181358e-06,
      "loss": 2.5903,
      "step": 87700
    },
    {
      "epoch": 1.7926781958878657,
      "grad_norm": 12.243597030639648,
      "learning_rate": 8.053303931777502e-06,
      "loss": 2.5982,
      "step": 87800
    },
    {
      "epoch": 1.794719970598444,
      "grad_norm": 12.948505401611328,
      "learning_rate": 8.039692100373645e-06,
      "loss": 2.6254,
      "step": 87900
    },
    {
      "epoch": 1.7967617453090226,
      "grad_norm": 10.563081741333008,
      "learning_rate": 8.02608026896979e-06,
      "loss": 2.6143,
      "step": 88000
    },
    {
      "epoch": 1.798803520019601,
      "grad_norm": 11.260083198547363,
      "learning_rate": 8.012468437565934e-06,
      "loss": 2.6366,
      "step": 88100
    },
    {
      "epoch": 1.8008452947301796,
      "grad_norm": 10.192233085632324,
      "learning_rate": 7.998856606162076e-06,
      "loss": 2.6341,
      "step": 88200
    },
    {
      "epoch": 1.802887069440758,
      "grad_norm": 10.000632286071777,
      "learning_rate": 7.985244774758221e-06,
      "loss": 2.6856,
      "step": 88300
    },
    {
      "epoch": 1.8049288441513363,
      "grad_norm": 11.260689735412598,
      "learning_rate": 7.971632943354364e-06,
      "loss": 2.6325,
      "step": 88400
    },
    {
      "epoch": 1.8069706188619148,
      "grad_norm": 11.405555725097656,
      "learning_rate": 7.958021111950508e-06,
      "loss": 2.6006,
      "step": 88500
    },
    {
      "epoch": 1.8090123935724933,
      "grad_norm": 11.416410446166992,
      "learning_rate": 7.944409280546653e-06,
      "loss": 2.5452,
      "step": 88600
    },
    {
      "epoch": 1.8110541682830716,
      "grad_norm": 11.91140365600586,
      "learning_rate": 7.930797449142795e-06,
      "loss": 2.5992,
      "step": 88700
    },
    {
      "epoch": 1.81309594299365,
      "grad_norm": 13.750996589660645,
      "learning_rate": 7.91718561773894e-06,
      "loss": 2.633,
      "step": 88800
    },
    {
      "epoch": 1.8151377177042285,
      "grad_norm": 11.877262115478516,
      "learning_rate": 7.903573786335084e-06,
      "loss": 2.6185,
      "step": 88900
    },
    {
      "epoch": 1.817179492414807,
      "grad_norm": 10.897893905639648,
      "learning_rate": 7.889961954931227e-06,
      "loss": 2.6099,
      "step": 89000
    },
    {
      "epoch": 1.8192212671253853,
      "grad_norm": 10.543606758117676,
      "learning_rate": 7.876350123527371e-06,
      "loss": 2.5569,
      "step": 89100
    },
    {
      "epoch": 1.8212630418359637,
      "grad_norm": 11.146397590637207,
      "learning_rate": 7.862738292123514e-06,
      "loss": 2.6116,
      "step": 89200
    },
    {
      "epoch": 1.8233048165465422,
      "grad_norm": 11.487772941589355,
      "learning_rate": 7.849126460719658e-06,
      "loss": 2.5991,
      "step": 89300
    },
    {
      "epoch": 1.8253465912571207,
      "grad_norm": 10.625533103942871,
      "learning_rate": 7.83565074762984e-06,
      "loss": 2.6113,
      "step": 89400
    },
    {
      "epoch": 1.8273883659676993,
      "grad_norm": 10.748183250427246,
      "learning_rate": 7.822038916225984e-06,
      "loss": 2.6353,
      "step": 89500
    },
    {
      "epoch": 1.8294301406782776,
      "grad_norm": 10.765034675598145,
      "learning_rate": 7.808427084822129e-06,
      "loss": 2.5929,
      "step": 89600
    },
    {
      "epoch": 1.831471915388856,
      "grad_norm": 10.109850883483887,
      "learning_rate": 7.794815253418271e-06,
      "loss": 2.5962,
      "step": 89700
    },
    {
      "epoch": 1.8335136900994344,
      "grad_norm": 11.299808502197266,
      "learning_rate": 7.781203422014416e-06,
      "loss": 2.5396,
      "step": 89800
    },
    {
      "epoch": 1.835555464810013,
      "grad_norm": 11.348334312438965,
      "learning_rate": 7.767591590610558e-06,
      "loss": 2.6597,
      "step": 89900
    },
    {
      "epoch": 1.8375972395205913,
      "grad_norm": 10.276451110839844,
      "learning_rate": 7.753979759206703e-06,
      "loss": 2.5748,
      "step": 90000
    },
    {
      "epoch": 1.8396390142311696,
      "grad_norm": 10.881850242614746,
      "learning_rate": 7.740367927802847e-06,
      "loss": 2.5842,
      "step": 90100
    },
    {
      "epoch": 1.8416807889417481,
      "grad_norm": 10.010656356811523,
      "learning_rate": 7.72675609639899e-06,
      "loss": 2.6132,
      "step": 90200
    },
    {
      "epoch": 1.8437225636523267,
      "grad_norm": 12.658552169799805,
      "learning_rate": 7.713144264995134e-06,
      "loss": 2.6332,
      "step": 90300
    },
    {
      "epoch": 1.845764338362905,
      "grad_norm": 10.090927124023438,
      "learning_rate": 7.699532433591277e-06,
      "loss": 2.6358,
      "step": 90400
    },
    {
      "epoch": 1.8478061130734835,
      "grad_norm": 8.726634979248047,
      "learning_rate": 7.685920602187422e-06,
      "loss": 2.6133,
      "step": 90500
    },
    {
      "epoch": 1.8498478877840618,
      "grad_norm": 10.7212495803833,
      "learning_rate": 7.672308770783566e-06,
      "loss": 2.6398,
      "step": 90600
    },
    {
      "epoch": 1.8518896624946404,
      "grad_norm": 9.759001731872559,
      "learning_rate": 7.658696939379709e-06,
      "loss": 2.5609,
      "step": 90700
    },
    {
      "epoch": 1.853931437205219,
      "grad_norm": 11.227263450622559,
      "learning_rate": 7.645085107975853e-06,
      "loss": 2.5966,
      "step": 90800
    },
    {
      "epoch": 1.8559732119157972,
      "grad_norm": 11.093302726745605,
      "learning_rate": 7.631473276571996e-06,
      "loss": 2.6415,
      "step": 90900
    },
    {
      "epoch": 1.8580149866263755,
      "grad_norm": 11.157711029052734,
      "learning_rate": 7.617997563482179e-06,
      "loss": 2.5884,
      "step": 91000
    },
    {
      "epoch": 1.860056761336954,
      "grad_norm": 10.933414459228516,
      "learning_rate": 7.604385732078323e-06,
      "loss": 2.5927,
      "step": 91100
    },
    {
      "epoch": 1.8620985360475326,
      "grad_norm": 10.303767204284668,
      "learning_rate": 7.590773900674468e-06,
      "loss": 2.5521,
      "step": 91200
    },
    {
      "epoch": 1.864140310758111,
      "grad_norm": 11.972208976745605,
      "learning_rate": 7.5771620692706105e-06,
      "loss": 2.5925,
      "step": 91300
    },
    {
      "epoch": 1.8661820854686892,
      "grad_norm": 12.978121757507324,
      "learning_rate": 7.563550237866755e-06,
      "loss": 2.5558,
      "step": 91400
    },
    {
      "epoch": 1.8682238601792678,
      "grad_norm": 11.996984481811523,
      "learning_rate": 7.549938406462898e-06,
      "loss": 2.6075,
      "step": 91500
    },
    {
      "epoch": 1.8702656348898463,
      "grad_norm": 11.25528621673584,
      "learning_rate": 7.536326575059042e-06,
      "loss": 2.5772,
      "step": 91600
    },
    {
      "epoch": 1.8723074096004249,
      "grad_norm": 10.227395057678223,
      "learning_rate": 7.5227147436551866e-06,
      "loss": 2.579,
      "step": 91700
    },
    {
      "epoch": 1.8743491843110032,
      "grad_norm": 10.550793647766113,
      "learning_rate": 7.509102912251329e-06,
      "loss": 2.5736,
      "step": 91800
    },
    {
      "epoch": 1.8763909590215815,
      "grad_norm": 10.04298210144043,
      "learning_rate": 7.495491080847474e-06,
      "loss": 2.6056,
      "step": 91900
    },
    {
      "epoch": 1.87843273373216,
      "grad_norm": 11.371847152709961,
      "learning_rate": 7.4818792494436165e-06,
      "loss": 2.5894,
      "step": 92000
    },
    {
      "epoch": 1.8804745084427386,
      "grad_norm": 10.780743598937988,
      "learning_rate": 7.468267418039761e-06,
      "loss": 2.6201,
      "step": 92100
    },
    {
      "epoch": 1.8825162831533169,
      "grad_norm": 12.757431983947754,
      "learning_rate": 7.454655586635905e-06,
      "loss": 2.5894,
      "step": 92200
    },
    {
      "epoch": 1.8845580578638952,
      "grad_norm": 9.051477432250977,
      "learning_rate": 7.441043755232048e-06,
      "loss": 2.6579,
      "step": 92300
    },
    {
      "epoch": 1.8865998325744737,
      "grad_norm": 11.6783447265625,
      "learning_rate": 7.4274319238281925e-06,
      "loss": 2.6121,
      "step": 92400
    },
    {
      "epoch": 1.8886416072850523,
      "grad_norm": 10.279067039489746,
      "learning_rate": 7.413820092424335e-06,
      "loss": 2.615,
      "step": 92500
    },
    {
      "epoch": 1.8906833819956306,
      "grad_norm": 10.480125427246094,
      "learning_rate": 7.40020826102048e-06,
      "loss": 2.6074,
      "step": 92600
    },
    {
      "epoch": 1.8927251567062089,
      "grad_norm": 11.391327857971191,
      "learning_rate": 7.386596429616624e-06,
      "loss": 2.5817,
      "step": 92700
    },
    {
      "epoch": 1.8947669314167874,
      "grad_norm": 9.445056915283203,
      "learning_rate": 7.372984598212767e-06,
      "loss": 2.5663,
      "step": 92800
    },
    {
      "epoch": 1.896808706127366,
      "grad_norm": 11.741661071777344,
      "learning_rate": 7.359372766808911e-06,
      "loss": 2.5564,
      "step": 92900
    },
    {
      "epoch": 1.8988504808379445,
      "grad_norm": 11.495274543762207,
      "learning_rate": 7.345760935405055e-06,
      "loss": 2.517,
      "step": 93000
    },
    {
      "epoch": 1.9008922555485228,
      "grad_norm": 11.524697303771973,
      "learning_rate": 7.3321491040011984e-06,
      "loss": 2.6393,
      "step": 93100
    },
    {
      "epoch": 1.9029340302591011,
      "grad_norm": 11.43239688873291,
      "learning_rate": 7.318537272597343e-06,
      "loss": 2.5279,
      "step": 93200
    },
    {
      "epoch": 1.9049758049696797,
      "grad_norm": 11.700078964233398,
      "learning_rate": 7.304925441193486e-06,
      "loss": 2.5511,
      "step": 93300
    },
    {
      "epoch": 1.9070175796802582,
      "grad_norm": 9.489102363586426,
      "learning_rate": 7.29131360978963e-06,
      "loss": 2.5553,
      "step": 93400
    },
    {
      "epoch": 1.9090593543908365,
      "grad_norm": 9.477877616882324,
      "learning_rate": 7.277701778385774e-06,
      "loss": 2.5598,
      "step": 93500
    },
    {
      "epoch": 1.9111011291014148,
      "grad_norm": 9.241299629211426,
      "learning_rate": 7.264089946981917e-06,
      "loss": 2.4888,
      "step": 93600
    },
    {
      "epoch": 1.9131429038119934,
      "grad_norm": 10.484249114990234,
      "learning_rate": 7.250478115578062e-06,
      "loss": 2.5799,
      "step": 93700
    },
    {
      "epoch": 1.915184678522572,
      "grad_norm": 10.384160041809082,
      "learning_rate": 7.236866284174204e-06,
      "loss": 2.629,
      "step": 93800
    },
    {
      "epoch": 1.9172264532331502,
      "grad_norm": 9.756368637084961,
      "learning_rate": 7.223254452770349e-06,
      "loss": 2.5721,
      "step": 93900
    },
    {
      "epoch": 1.9192682279437285,
      "grad_norm": 11.641777038574219,
      "learning_rate": 7.209642621366492e-06,
      "loss": 2.5839,
      "step": 94000
    },
    {
      "epoch": 1.921310002654307,
      "grad_norm": 11.347509384155273,
      "learning_rate": 7.196030789962636e-06,
      "loss": 2.6123,
      "step": 94100
    },
    {
      "epoch": 1.9233517773648856,
      "grad_norm": 10.112963676452637,
      "learning_rate": 7.1824189585587804e-06,
      "loss": 2.6463,
      "step": 94200
    },
    {
      "epoch": 1.9253935520754641,
      "grad_norm": 10.581140518188477,
      "learning_rate": 7.168807127154923e-06,
      "loss": 2.5938,
      "step": 94300
    },
    {
      "epoch": 1.9274353267860425,
      "grad_norm": 10.661408424377441,
      "learning_rate": 7.155195295751068e-06,
      "loss": 2.6527,
      "step": 94400
    },
    {
      "epoch": 1.9294771014966208,
      "grad_norm": 12.350921630859375,
      "learning_rate": 7.141583464347211e-06,
      "loss": 2.5999,
      "step": 94500
    },
    {
      "epoch": 1.9315188762071993,
      "grad_norm": 11.602177619934082,
      "learning_rate": 7.127971632943355e-06,
      "loss": 2.5244,
      "step": 94600
    },
    {
      "epoch": 1.9335606509177778,
      "grad_norm": 11.902315139770508,
      "learning_rate": 7.114359801539499e-06,
      "loss": 2.5684,
      "step": 94700
    },
    {
      "epoch": 1.9356024256283562,
      "grad_norm": 12.261818885803223,
      "learning_rate": 7.100747970135643e-06,
      "loss": 2.6073,
      "step": 94800
    },
    {
      "epoch": 1.9376442003389345,
      "grad_norm": 10.255610466003418,
      "learning_rate": 7.087136138731786e-06,
      "loss": 2.5738,
      "step": 94900
    },
    {
      "epoch": 1.939685975049513,
      "grad_norm": 15.479598045349121,
      "learning_rate": 7.07352430732793e-06,
      "loss": 2.5543,
      "step": 95000
    },
    {
      "epoch": 1.9417277497600915,
      "grad_norm": 12.645683288574219,
      "learning_rate": 7.0599124759240735e-06,
      "loss": 2.6379,
      "step": 95100
    },
    {
      "epoch": 1.9437695244706699,
      "grad_norm": 10.330577850341797,
      "learning_rate": 7.046436762834256e-06,
      "loss": 2.5667,
      "step": 95200
    },
    {
      "epoch": 1.9458112991812482,
      "grad_norm": 11.892088890075684,
      "learning_rate": 7.0328249314304e-06,
      "loss": 2.6512,
      "step": 95300
    },
    {
      "epoch": 1.9478530738918267,
      "grad_norm": 11.291723251342773,
      "learning_rate": 7.019213100026544e-06,
      "loss": 2.6223,
      "step": 95400
    },
    {
      "epoch": 1.9498948486024053,
      "grad_norm": 14.841714859008789,
      "learning_rate": 7.005601268622687e-06,
      "loss": 2.5622,
      "step": 95500
    },
    {
      "epoch": 1.9519366233129838,
      "grad_norm": 11.21119499206543,
      "learning_rate": 6.991989437218832e-06,
      "loss": 2.5661,
      "step": 95600
    },
    {
      "epoch": 1.953978398023562,
      "grad_norm": 10.961710929870605,
      "learning_rate": 6.978377605814975e-06,
      "loss": 2.5788,
      "step": 95700
    },
    {
      "epoch": 1.9560201727341404,
      "grad_norm": 11.025312423706055,
      "learning_rate": 6.964765774411119e-06,
      "loss": 2.5893,
      "step": 95800
    },
    {
      "epoch": 1.958061947444719,
      "grad_norm": 10.343206405639648,
      "learning_rate": 6.951153943007262e-06,
      "loss": 2.5476,
      "step": 95900
    },
    {
      "epoch": 1.9601037221552975,
      "grad_norm": 10.710308074951172,
      "learning_rate": 6.937542111603406e-06,
      "loss": 2.5966,
      "step": 96000
    },
    {
      "epoch": 1.9621454968658758,
      "grad_norm": 11.169610977172852,
      "learning_rate": 6.92393028019955e-06,
      "loss": 2.5966,
      "step": 96100
    },
    {
      "epoch": 1.9641872715764541,
      "grad_norm": 12.085794448852539,
      "learning_rate": 6.910318448795694e-06,
      "loss": 2.5835,
      "step": 96200
    },
    {
      "epoch": 1.9662290462870327,
      "grad_norm": 10.053152084350586,
      "learning_rate": 6.896706617391838e-06,
      "loss": 2.5896,
      "step": 96300
    },
    {
      "epoch": 1.9682708209976112,
      "grad_norm": 9.94864559173584,
      "learning_rate": 6.883094785987981e-06,
      "loss": 2.6083,
      "step": 96400
    },
    {
      "epoch": 1.9703125957081895,
      "grad_norm": 10.54466724395752,
      "learning_rate": 6.869482954584125e-06,
      "loss": 2.5385,
      "step": 96500
    },
    {
      "epoch": 1.972354370418768,
      "grad_norm": 11.947863578796387,
      "learning_rate": 6.856007241494308e-06,
      "loss": 2.5933,
      "step": 96600
    },
    {
      "epoch": 1.9743961451293464,
      "grad_norm": 10.241364479064941,
      "learning_rate": 6.842395410090451e-06,
      "loss": 2.5834,
      "step": 96700
    },
    {
      "epoch": 1.976437919839925,
      "grad_norm": 10.46712589263916,
      "learning_rate": 6.828783578686595e-06,
      "loss": 2.6254,
      "step": 96800
    },
    {
      "epoch": 1.9784796945505034,
      "grad_norm": 10.881546020507812,
      "learning_rate": 6.8151717472827384e-06,
      "loss": 2.5748,
      "step": 96900
    },
    {
      "epoch": 1.9805214692610817,
      "grad_norm": 11.263747215270996,
      "learning_rate": 6.801559915878883e-06,
      "loss": 2.5901,
      "step": 97000
    },
    {
      "epoch": 1.98256324397166,
      "grad_norm": 11.476866722106934,
      "learning_rate": 6.7879480844750264e-06,
      "loss": 2.6162,
      "step": 97100
    },
    {
      "epoch": 1.9846050186822386,
      "grad_norm": 11.070782661437988,
      "learning_rate": 6.774472371385209e-06,
      "loss": 2.6167,
      "step": 97200
    },
    {
      "epoch": 1.9866467933928171,
      "grad_norm": 9.521352767944336,
      "learning_rate": 6.760860539981352e-06,
      "loss": 2.5889,
      "step": 97300
    },
    {
      "epoch": 1.9886885681033954,
      "grad_norm": 12.536177635192871,
      "learning_rate": 6.7472487085774965e-06,
      "loss": 2.5618,
      "step": 97400
    },
    {
      "epoch": 1.9907303428139738,
      "grad_norm": 13.088337898254395,
      "learning_rate": 6.733636877173639e-06,
      "loss": 2.6383,
      "step": 97500
    },
    {
      "epoch": 1.9927721175245523,
      "grad_norm": 11.8411226272583,
      "learning_rate": 6.720025045769784e-06,
      "loss": 2.5642,
      "step": 97600
    },
    {
      "epoch": 1.9948138922351308,
      "grad_norm": 9.72670841217041,
      "learning_rate": 6.706413214365928e-06,
      "loss": 2.6336,
      "step": 97700
    },
    {
      "epoch": 1.9968556669457094,
      "grad_norm": 9.912701606750488,
      "learning_rate": 6.692801382962071e-06,
      "loss": 2.5432,
      "step": 97800
    },
    {
      "epoch": 1.9988974416562877,
      "grad_norm": 12.126779556274414,
      "learning_rate": 6.679189551558215e-06,
      "loss": 2.6511,
      "step": 97900
    },
    {
      "epoch": 2.000939216366866,
      "grad_norm": 12.086454391479492,
      "learning_rate": 6.665577720154359e-06,
      "loss": 2.6347,
      "step": 98000
    },
    {
      "epoch": 2.0029809910774445,
      "grad_norm": 10.204973220825195,
      "learning_rate": 6.6519658887505025e-06,
      "loss": 2.5385,
      "step": 98100
    },
    {
      "epoch": 2.005022765788023,
      "grad_norm": 9.738165855407715,
      "learning_rate": 6.638354057346647e-06,
      "loss": 2.5771,
      "step": 98200
    },
    {
      "epoch": 2.007064540498601,
      "grad_norm": 12.58811092376709,
      "learning_rate": 6.62474222594279e-06,
      "loss": 2.5736,
      "step": 98300
    },
    {
      "epoch": 2.0091063152091797,
      "grad_norm": 11.731009483337402,
      "learning_rate": 6.611130394538934e-06,
      "loss": 2.5894,
      "step": 98400
    },
    {
      "epoch": 2.0111480899197582,
      "grad_norm": 10.920024871826172,
      "learning_rate": 6.597518563135078e-06,
      "loss": 2.4985,
      "step": 98500
    },
    {
      "epoch": 2.013189864630337,
      "grad_norm": 12.844220161437988,
      "learning_rate": 6.583906731731221e-06,
      "loss": 2.5655,
      "step": 98600
    },
    {
      "epoch": 2.0152316393409153,
      "grad_norm": 10.888656616210938,
      "learning_rate": 6.570294900327366e-06,
      "loss": 2.5983,
      "step": 98700
    },
    {
      "epoch": 2.0172734140514934,
      "grad_norm": 12.837547302246094,
      "learning_rate": 6.556683068923508e-06,
      "loss": 2.6025,
      "step": 98800
    },
    {
      "epoch": 2.019315188762072,
      "grad_norm": 10.157808303833008,
      "learning_rate": 6.543071237519653e-06,
      "loss": 2.5884,
      "step": 98900
    },
    {
      "epoch": 2.0213569634726505,
      "grad_norm": 12.331965446472168,
      "learning_rate": 6.5294594061157964e-06,
      "loss": 2.5995,
      "step": 99000
    },
    {
      "epoch": 2.023398738183229,
      "grad_norm": 11.941227912902832,
      "learning_rate": 6.51584757471194e-06,
      "loss": 2.5659,
      "step": 99100
    },
    {
      "epoch": 2.025440512893807,
      "grad_norm": 10.905472755432129,
      "learning_rate": 6.5022357433080845e-06,
      "loss": 2.6208,
      "step": 99200
    },
    {
      "epoch": 2.0274822876043856,
      "grad_norm": 11.175403594970703,
      "learning_rate": 6.488623911904227e-06,
      "loss": 2.5734,
      "step": 99300
    },
    {
      "epoch": 2.029524062314964,
      "grad_norm": 13.735386848449707,
      "learning_rate": 6.475012080500372e-06,
      "loss": 2.5988,
      "step": 99400
    },
    {
      "epoch": 2.0315658370255427,
      "grad_norm": 10.350663185119629,
      "learning_rate": 6.461400249096515e-06,
      "loss": 2.5623,
      "step": 99500
    },
    {
      "epoch": 2.0336076117361213,
      "grad_norm": 11.764420509338379,
      "learning_rate": 6.447788417692659e-06,
      "loss": 2.6288,
      "step": 99600
    },
    {
      "epoch": 2.0356493864466993,
      "grad_norm": 12.48563003540039,
      "learning_rate": 6.434176586288803e-06,
      "loss": 2.5649,
      "step": 99700
    },
    {
      "epoch": 2.037691161157278,
      "grad_norm": 9.47916030883789,
      "learning_rate": 6.420564754884947e-06,
      "loss": 2.5604,
      "step": 99800
    },
    {
      "epoch": 2.0397329358678564,
      "grad_norm": 10.638737678527832,
      "learning_rate": 6.40695292348109e-06,
      "loss": 2.5404,
      "step": 99900
    },
    {
      "epoch": 2.041774710578435,
      "grad_norm": 8.84792423248291,
      "learning_rate": 6.3934772103912725e-06,
      "loss": 2.5579,
      "step": 100000
    },
    {
      "epoch": 2.043816485289013,
      "grad_norm": 10.010199546813965,
      "learning_rate": 6.379865378987417e-06,
      "loss": 2.5508,
      "step": 100100
    },
    {
      "epoch": 2.0458582599995916,
      "grad_norm": 11.932083129882812,
      "learning_rate": 6.36625354758356e-06,
      "loss": 2.5808,
      "step": 100200
    },
    {
      "epoch": 2.04790003471017,
      "grad_norm": 10.474514961242676,
      "learning_rate": 6.352641716179704e-06,
      "loss": 2.6212,
      "step": 100300
    },
    {
      "epoch": 2.0499418094207487,
      "grad_norm": 10.186236381530762,
      "learning_rate": 6.339029884775848e-06,
      "loss": 2.6081,
      "step": 100400
    },
    {
      "epoch": 2.0519835841313268,
      "grad_norm": 11.789562225341797,
      "learning_rate": 6.325418053371991e-06,
      "loss": 2.5718,
      "step": 100500
    },
    {
      "epoch": 2.0540253588419053,
      "grad_norm": 11.533388137817383,
      "learning_rate": 6.311806221968136e-06,
      "loss": 2.5568,
      "step": 100600
    },
    {
      "epoch": 2.056067133552484,
      "grad_norm": 10.630912780761719,
      "learning_rate": 6.298194390564279e-06,
      "loss": 2.5206,
      "step": 100700
    },
    {
      "epoch": 2.0581089082630624,
      "grad_norm": 9.515546798706055,
      "learning_rate": 6.284582559160423e-06,
      "loss": 2.555,
      "step": 100800
    },
    {
      "epoch": 2.060150682973641,
      "grad_norm": 11.974932670593262,
      "learning_rate": 6.270970727756566e-06,
      "loss": 2.555,
      "step": 100900
    },
    {
      "epoch": 2.062192457684219,
      "grad_norm": 10.891433715820312,
      "learning_rate": 6.25735889635271e-06,
      "loss": 2.5614,
      "step": 101000
    },
    {
      "epoch": 2.0642342323947975,
      "grad_norm": 10.276455879211426,
      "learning_rate": 6.2437470649488544e-06,
      "loss": 2.5444,
      "step": 101100
    },
    {
      "epoch": 2.066276007105376,
      "grad_norm": 11.303669929504395,
      "learning_rate": 6.230135233544998e-06,
      "loss": 2.5529,
      "step": 101200
    },
    {
      "epoch": 2.0683177818159546,
      "grad_norm": 10.978082656860352,
      "learning_rate": 6.216523402141142e-06,
      "loss": 2.6494,
      "step": 101300
    },
    {
      "epoch": 2.0703595565265327,
      "grad_norm": 13.426325798034668,
      "learning_rate": 6.202911570737285e-06,
      "loss": 2.5871,
      "step": 101400
    },
    {
      "epoch": 2.0724013312371112,
      "grad_norm": 11.165181159973145,
      "learning_rate": 6.189299739333429e-06,
      "loss": 2.6072,
      "step": 101500
    },
    {
      "epoch": 2.0744431059476898,
      "grad_norm": 10.793072700500488,
      "learning_rate": 6.175687907929573e-06,
      "loss": 2.5929,
      "step": 101600
    },
    {
      "epoch": 2.0764848806582683,
      "grad_norm": 9.325873374938965,
      "learning_rate": 6.162076076525717e-06,
      "loss": 2.5635,
      "step": 101700
    },
    {
      "epoch": 2.0785266553688464,
      "grad_norm": 12.16700267791748,
      "learning_rate": 6.14846424512186e-06,
      "loss": 2.5562,
      "step": 101800
    },
    {
      "epoch": 2.080568430079425,
      "grad_norm": 10.837450981140137,
      "learning_rate": 6.134852413718004e-06,
      "loss": 2.5563,
      "step": 101900
    },
    {
      "epoch": 2.0826102047900035,
      "grad_norm": 13.036561012268066,
      "learning_rate": 6.1212405823141476e-06,
      "loss": 2.5723,
      "step": 102000
    },
    {
      "epoch": 2.084651979500582,
      "grad_norm": 11.561304092407227,
      "learning_rate": 6.107628750910292e-06,
      "loss": 2.5965,
      "step": 102100
    },
    {
      "epoch": 2.0866937542111605,
      "grad_norm": 12.728561401367188,
      "learning_rate": 6.094016919506436e-06,
      "loss": 2.5609,
      "step": 102200
    },
    {
      "epoch": 2.0887355289217386,
      "grad_norm": 11.02652645111084,
      "learning_rate": 6.080405088102579e-06,
      "loss": 2.574,
      "step": 102300
    },
    {
      "epoch": 2.090777303632317,
      "grad_norm": 9.704285621643066,
      "learning_rate": 6.066793256698723e-06,
      "loss": 2.5011,
      "step": 102400
    },
    {
      "epoch": 2.0928190783428957,
      "grad_norm": 12.534551620483398,
      "learning_rate": 6.053181425294867e-06,
      "loss": 2.5942,
      "step": 102500
    },
    {
      "epoch": 2.0948608530534742,
      "grad_norm": 12.139619827270508,
      "learning_rate": 6.039569593891011e-06,
      "loss": 2.6183,
      "step": 102600
    },
    {
      "epoch": 2.0969026277640523,
      "grad_norm": 11.858942031860352,
      "learning_rate": 6.025957762487154e-06,
      "loss": 2.5562,
      "step": 102700
    },
    {
      "epoch": 2.098944402474631,
      "grad_norm": 10.610158920288086,
      "learning_rate": 6.012345931083298e-06,
      "loss": 2.5097,
      "step": 102800
    },
    {
      "epoch": 2.1009861771852094,
      "grad_norm": 14.06202220916748,
      "learning_rate": 5.9987340996794415e-06,
      "loss": 2.5765,
      "step": 102900
    },
    {
      "epoch": 2.103027951895788,
      "grad_norm": 13.214534759521484,
      "learning_rate": 5.985122268275586e-06,
      "loss": 2.61,
      "step": 103000
    },
    {
      "epoch": 2.105069726606366,
      "grad_norm": 12.010106086730957,
      "learning_rate": 5.9715104368717295e-06,
      "loss": 2.6222,
      "step": 103100
    },
    {
      "epoch": 2.1071115013169446,
      "grad_norm": 12.150196075439453,
      "learning_rate": 5.957898605467873e-06,
      "loss": 2.5806,
      "step": 103200
    },
    {
      "epoch": 2.109153276027523,
      "grad_norm": 11.194938659667969,
      "learning_rate": 5.944286774064017e-06,
      "loss": 2.5487,
      "step": 103300
    },
    {
      "epoch": 2.1111950507381017,
      "grad_norm": 9.211871147155762,
      "learning_rate": 5.93067494266016e-06,
      "loss": 2.5299,
      "step": 103400
    },
    {
      "epoch": 2.11323682544868,
      "grad_norm": 9.973949432373047,
      "learning_rate": 5.917063111256305e-06,
      "loss": 2.5624,
      "step": 103500
    },
    {
      "epoch": 2.1152786001592583,
      "grad_norm": 10.053662300109863,
      "learning_rate": 5.903451279852448e-06,
      "loss": 2.5402,
      "step": 103600
    },
    {
      "epoch": 2.117320374869837,
      "grad_norm": 11.20262336730957,
      "learning_rate": 5.889839448448592e-06,
      "loss": 2.5063,
      "step": 103700
    },
    {
      "epoch": 2.1193621495804154,
      "grad_norm": 10.910954475402832,
      "learning_rate": 5.8762276170447355e-06,
      "loss": 2.5551,
      "step": 103800
    },
    {
      "epoch": 2.121403924290994,
      "grad_norm": 11.741373062133789,
      "learning_rate": 5.862615785640879e-06,
      "loss": 2.5132,
      "step": 103900
    },
    {
      "epoch": 2.123445699001572,
      "grad_norm": 11.53327465057373,
      "learning_rate": 5.8490039542370235e-06,
      "loss": 2.5183,
      "step": 104000
    },
    {
      "epoch": 2.1254874737121505,
      "grad_norm": 12.209637641906738,
      "learning_rate": 5.835392122833167e-06,
      "loss": 2.5593,
      "step": 104100
    },
    {
      "epoch": 2.127529248422729,
      "grad_norm": 12.890981674194336,
      "learning_rate": 5.821780291429311e-06,
      "loss": 2.5346,
      "step": 104200
    },
    {
      "epoch": 2.1295710231333076,
      "grad_norm": 12.667555809020996,
      "learning_rate": 5.808168460025455e-06,
      "loss": 2.5913,
      "step": 104300
    },
    {
      "epoch": 2.1316127978438857,
      "grad_norm": 10.6153564453125,
      "learning_rate": 5.794556628621598e-06,
      "loss": 2.5698,
      "step": 104400
    },
    {
      "epoch": 2.1336545725544642,
      "grad_norm": 10.754093170166016,
      "learning_rate": 5.780944797217742e-06,
      "loss": 2.5671,
      "step": 104500
    },
    {
      "epoch": 2.1356963472650428,
      "grad_norm": 14.92758560180664,
      "learning_rate": 5.767332965813886e-06,
      "loss": 2.5934,
      "step": 104600
    },
    {
      "epoch": 2.1377381219756213,
      "grad_norm": 12.012107849121094,
      "learning_rate": 5.7537211344100295e-06,
      "loss": 2.5289,
      "step": 104700
    },
    {
      "epoch": 2.1397798966862,
      "grad_norm": 11.931899070739746,
      "learning_rate": 5.740109303006174e-06,
      "loss": 2.5744,
      "step": 104800
    },
    {
      "epoch": 2.141821671396778,
      "grad_norm": 11.235074996948242,
      "learning_rate": 5.726497471602317e-06,
      "loss": 2.5809,
      "step": 104900
    },
    {
      "epoch": 2.1438634461073565,
      "grad_norm": 11.262310028076172,
      "learning_rate": 5.712885640198461e-06,
      "loss": 2.4949,
      "step": 105000
    },
    {
      "epoch": 2.145905220817935,
      "grad_norm": 9.817715644836426,
      "learning_rate": 5.699273808794605e-06,
      "loss": 2.5943,
      "step": 105100
    },
    {
      "epoch": 2.1479469955285135,
      "grad_norm": 11.841328620910645,
      "learning_rate": 5.685661977390748e-06,
      "loss": 2.6144,
      "step": 105200
    },
    {
      "epoch": 2.1499887702390916,
      "grad_norm": 10.90384578704834,
      "learning_rate": 5.672050145986893e-06,
      "loss": 2.4922,
      "step": 105300
    },
    {
      "epoch": 2.15203054494967,
      "grad_norm": 10.449528694152832,
      "learning_rate": 5.658438314583035e-06,
      "loss": 2.5599,
      "step": 105400
    },
    {
      "epoch": 2.1540723196602487,
      "grad_norm": 12.059882164001465,
      "learning_rate": 5.64482648317918e-06,
      "loss": 2.5863,
      "step": 105500
    },
    {
      "epoch": 2.1561140943708272,
      "grad_norm": 12.793550491333008,
      "learning_rate": 5.631214651775324e-06,
      "loss": 2.6259,
      "step": 105600
    },
    {
      "epoch": 2.1581558690814058,
      "grad_norm": 11.655621528625488,
      "learning_rate": 5.617602820371467e-06,
      "loss": 2.5166,
      "step": 105700
    },
    {
      "epoch": 2.160197643791984,
      "grad_norm": 13.014147758483887,
      "learning_rate": 5.6039909889676114e-06,
      "loss": 2.5862,
      "step": 105800
    },
    {
      "epoch": 2.1622394185025624,
      "grad_norm": 10.832436561584473,
      "learning_rate": 5.590379157563754e-06,
      "loss": 2.5349,
      "step": 105900
    },
    {
      "epoch": 2.164281193213141,
      "grad_norm": 11.45637035369873,
      "learning_rate": 5.576767326159899e-06,
      "loss": 2.53,
      "step": 106000
    },
    {
      "epoch": 2.1663229679237195,
      "grad_norm": 9.645577430725098,
      "learning_rate": 5.563291613070081e-06,
      "loss": 2.5617,
      "step": 106100
    },
    {
      "epoch": 2.1683647426342976,
      "grad_norm": 9.902259826660156,
      "learning_rate": 5.549679781666225e-06,
      "loss": 2.5674,
      "step": 106200
    },
    {
      "epoch": 2.170406517344876,
      "grad_norm": 12.217846870422363,
      "learning_rate": 5.536067950262368e-06,
      "loss": 2.596,
      "step": 106300
    },
    {
      "epoch": 2.1724482920554546,
      "grad_norm": 12.21853256225586,
      "learning_rate": 5.522456118858512e-06,
      "loss": 2.589,
      "step": 106400
    },
    {
      "epoch": 2.174490066766033,
      "grad_norm": 10.939715385437012,
      "learning_rate": 5.508844287454656e-06,
      "loss": 2.5348,
      "step": 106500
    },
    {
      "epoch": 2.1765318414766113,
      "grad_norm": 10.973237037658691,
      "learning_rate": 5.4952324560507994e-06,
      "loss": 2.5776,
      "step": 106600
    },
    {
      "epoch": 2.17857361618719,
      "grad_norm": 11.154557228088379,
      "learning_rate": 5.481620624646944e-06,
      "loss": 2.5395,
      "step": 106700
    },
    {
      "epoch": 2.1806153908977683,
      "grad_norm": 11.330836296081543,
      "learning_rate": 5.468008793243087e-06,
      "loss": 2.5809,
      "step": 106800
    },
    {
      "epoch": 2.182657165608347,
      "grad_norm": 10.262587547302246,
      "learning_rate": 5.454396961839231e-06,
      "loss": 2.5953,
      "step": 106900
    },
    {
      "epoch": 2.184698940318925,
      "grad_norm": 10.201886177062988,
      "learning_rate": 5.4407851304353755e-06,
      "loss": 2.507,
      "step": 107000
    },
    {
      "epoch": 2.1867407150295035,
      "grad_norm": 9.675328254699707,
      "learning_rate": 5.427173299031518e-06,
      "loss": 2.5394,
      "step": 107100
    },
    {
      "epoch": 2.188782489740082,
      "grad_norm": 11.76754379272461,
      "learning_rate": 5.413561467627663e-06,
      "loss": 2.499,
      "step": 107200
    },
    {
      "epoch": 2.1908242644506606,
      "grad_norm": 10.328984260559082,
      "learning_rate": 5.399949636223805e-06,
      "loss": 2.5846,
      "step": 107300
    },
    {
      "epoch": 2.192866039161239,
      "grad_norm": 11.115782737731934,
      "learning_rate": 5.38633780481995e-06,
      "loss": 2.5616,
      "step": 107400
    },
    {
      "epoch": 2.194907813871817,
      "grad_norm": 10.534768104553223,
      "learning_rate": 5.372725973416094e-06,
      "loss": 2.5984,
      "step": 107500
    },
    {
      "epoch": 2.1969495885823958,
      "grad_norm": 10.450400352478027,
      "learning_rate": 5.359114142012237e-06,
      "loss": 2.528,
      "step": 107600
    },
    {
      "epoch": 2.1989913632929743,
      "grad_norm": 11.966853141784668,
      "learning_rate": 5.3455023106083814e-06,
      "loss": 2.5884,
      "step": 107700
    },
    {
      "epoch": 2.201033138003553,
      "grad_norm": 10.15141773223877,
      "learning_rate": 5.331890479204524e-06,
      "loss": 2.5337,
      "step": 107800
    },
    {
      "epoch": 2.203074912714131,
      "grad_norm": 13.253941535949707,
      "learning_rate": 5.318278647800669e-06,
      "loss": 2.535,
      "step": 107900
    },
    {
      "epoch": 2.2051166874247095,
      "grad_norm": 11.433598518371582,
      "learning_rate": 5.304666816396813e-06,
      "loss": 2.5916,
      "step": 108000
    },
    {
      "epoch": 2.207158462135288,
      "grad_norm": 8.757061004638672,
      "learning_rate": 5.291054984992956e-06,
      "loss": 2.5187,
      "step": 108100
    },
    {
      "epoch": 2.2092002368458665,
      "grad_norm": 11.789551734924316,
      "learning_rate": 5.2774431535891e-06,
      "loss": 2.5342,
      "step": 108200
    },
    {
      "epoch": 2.211242011556445,
      "grad_norm": 11.472132682800293,
      "learning_rate": 5.263831322185245e-06,
      "loss": 2.5308,
      "step": 108300
    },
    {
      "epoch": 2.213283786267023,
      "grad_norm": 13.003255844116211,
      "learning_rate": 5.250219490781387e-06,
      "loss": 2.5373,
      "step": 108400
    },
    {
      "epoch": 2.2153255609776017,
      "grad_norm": 12.099569320678711,
      "learning_rate": 5.236607659377532e-06,
      "loss": 2.5399,
      "step": 108500
    },
    {
      "epoch": 2.2173673356881802,
      "grad_norm": 11.70564079284668,
      "learning_rate": 5.2229958279736746e-06,
      "loss": 2.6038,
      "step": 108600
    },
    {
      "epoch": 2.2194091103987588,
      "grad_norm": 12.894611358642578,
      "learning_rate": 5.209383996569819e-06,
      "loss": 2.575,
      "step": 108700
    },
    {
      "epoch": 2.221450885109337,
      "grad_norm": 9.920766830444336,
      "learning_rate": 5.195772165165963e-06,
      "loss": 2.5035,
      "step": 108800
    },
    {
      "epoch": 2.2234926598199154,
      "grad_norm": 11.925071716308594,
      "learning_rate": 5.182160333762106e-06,
      "loss": 2.4655,
      "step": 108900
    },
    {
      "epoch": 2.225534434530494,
      "grad_norm": 11.193290710449219,
      "learning_rate": 5.168548502358251e-06,
      "loss": 2.5823,
      "step": 109000
    },
    {
      "epoch": 2.2275762092410725,
      "grad_norm": 12.24231243133545,
      "learning_rate": 5.154936670954393e-06,
      "loss": 2.4797,
      "step": 109100
    },
    {
      "epoch": 2.229617983951651,
      "grad_norm": 9.439088821411133,
      "learning_rate": 5.141324839550538e-06,
      "loss": 2.6069,
      "step": 109200
    },
    {
      "epoch": 2.231659758662229,
      "grad_norm": 9.69235897064209,
      "learning_rate": 5.127713008146682e-06,
      "loss": 2.5414,
      "step": 109300
    },
    {
      "epoch": 2.2337015333728076,
      "grad_norm": 12.545675277709961,
      "learning_rate": 5.114101176742825e-06,
      "loss": 2.5576,
      "step": 109400
    },
    {
      "epoch": 2.235743308083386,
      "grad_norm": 10.755013465881348,
      "learning_rate": 5.100625463653007e-06,
      "loss": 2.6452,
      "step": 109500
    },
    {
      "epoch": 2.2377850827939647,
      "grad_norm": 9.979573249816895,
      "learning_rate": 5.087013632249151e-06,
      "loss": 2.5565,
      "step": 109600
    },
    {
      "epoch": 2.239826857504543,
      "grad_norm": 11.841898918151855,
      "learning_rate": 5.073401800845296e-06,
      "loss": 2.5321,
      "step": 109700
    },
    {
      "epoch": 2.2418686322151213,
      "grad_norm": 12.999750137329102,
      "learning_rate": 5.059789969441439e-06,
      "loss": 2.594,
      "step": 109800
    },
    {
      "epoch": 2.2439104069257,
      "grad_norm": 9.977295875549316,
      "learning_rate": 5.046178138037583e-06,
      "loss": 2.5243,
      "step": 109900
    },
    {
      "epoch": 2.2459521816362784,
      "grad_norm": 11.600701332092285,
      "learning_rate": 5.032566306633726e-06,
      "loss": 2.5936,
      "step": 110000
    },
    {
      "epoch": 2.2479939563468565,
      "grad_norm": 10.846293449401855,
      "learning_rate": 5.01895447522987e-06,
      "loss": 2.5316,
      "step": 110100
    },
    {
      "epoch": 2.250035731057435,
      "grad_norm": 10.484006881713867,
      "learning_rate": 5.005342643826015e-06,
      "loss": 2.5654,
      "step": 110200
    },
    {
      "epoch": 2.2520775057680136,
      "grad_norm": 11.365856170654297,
      "learning_rate": 4.991730812422157e-06,
      "loss": 2.5029,
      "step": 110300
    },
    {
      "epoch": 2.254119280478592,
      "grad_norm": 11.825139045715332,
      "learning_rate": 4.978118981018302e-06,
      "loss": 2.4948,
      "step": 110400
    },
    {
      "epoch": 2.25616105518917,
      "grad_norm": 9.551410675048828,
      "learning_rate": 4.964507149614445e-06,
      "loss": 2.5035,
      "step": 110500
    },
    {
      "epoch": 2.2582028298997487,
      "grad_norm": 10.648364067077637,
      "learning_rate": 4.950895318210589e-06,
      "loss": 2.5862,
      "step": 110600
    },
    {
      "epoch": 2.2602446046103273,
      "grad_norm": 10.335558891296387,
      "learning_rate": 4.9372834868067326e-06,
      "loss": 2.5896,
      "step": 110700
    },
    {
      "epoch": 2.262286379320906,
      "grad_norm": 10.081901550292969,
      "learning_rate": 4.923671655402877e-06,
      "loss": 2.5803,
      "step": 110800
    },
    {
      "epoch": 2.2643281540314844,
      "grad_norm": 13.527256965637207,
      "learning_rate": 4.910059823999021e-06,
      "loss": 2.5363,
      "step": 110900
    },
    {
      "epoch": 2.2663699287420624,
      "grad_norm": 12.707674026489258,
      "learning_rate": 4.896447992595164e-06,
      "loss": 2.5325,
      "step": 111000
    },
    {
      "epoch": 2.268411703452641,
      "grad_norm": 9.734227180480957,
      "learning_rate": 4.882836161191308e-06,
      "loss": 2.5182,
      "step": 111100
    },
    {
      "epoch": 2.2704534781632195,
      "grad_norm": 12.949769973754883,
      "learning_rate": 4.869224329787451e-06,
      "loss": 2.5535,
      "step": 111200
    },
    {
      "epoch": 2.272495252873798,
      "grad_norm": 14.348569869995117,
      "learning_rate": 4.855612498383596e-06,
      "loss": 2.5479,
      "step": 111300
    },
    {
      "epoch": 2.274537027584376,
      "grad_norm": 13.474895477294922,
      "learning_rate": 4.842000666979739e-06,
      "loss": 2.5497,
      "step": 111400
    },
    {
      "epoch": 2.2765788022949547,
      "grad_norm": 10.124951362609863,
      "learning_rate": 4.828388835575883e-06,
      "loss": 2.4916,
      "step": 111500
    },
    {
      "epoch": 2.278620577005533,
      "grad_norm": 11.284317016601562,
      "learning_rate": 4.8147770041720265e-06,
      "loss": 2.5249,
      "step": 111600
    },
    {
      "epoch": 2.2806623517161118,
      "grad_norm": 11.634319305419922,
      "learning_rate": 4.801165172768171e-06,
      "loss": 2.5209,
      "step": 111700
    },
    {
      "epoch": 2.2827041264266903,
      "grad_norm": 12.679792404174805,
      "learning_rate": 4.7875533413643145e-06,
      "loss": 2.5295,
      "step": 111800
    },
    {
      "epoch": 2.2847459011372684,
      "grad_norm": 12.885224342346191,
      "learning_rate": 4.773941509960458e-06,
      "loss": 2.5765,
      "step": 111900
    },
    {
      "epoch": 2.286787675847847,
      "grad_norm": 10.430499076843262,
      "learning_rate": 4.76046579687064e-06,
      "loss": 2.5568,
      "step": 112000
    },
    {
      "epoch": 2.2888294505584255,
      "grad_norm": 12.874178886413574,
      "learning_rate": 4.746853965466784e-06,
      "loss": 2.5691,
      "step": 112100
    },
    {
      "epoch": 2.290871225269004,
      "grad_norm": 11.252232551574707,
      "learning_rate": 4.733242134062928e-06,
      "loss": 2.5392,
      "step": 112200
    },
    {
      "epoch": 2.292912999979582,
      "grad_norm": 11.135685920715332,
      "learning_rate": 4.719630302659072e-06,
      "loss": 2.4957,
      "step": 112300
    },
    {
      "epoch": 2.2949547746901606,
      "grad_norm": 10.34978199005127,
      "learning_rate": 4.706018471255215e-06,
      "loss": 2.5713,
      "step": 112400
    },
    {
      "epoch": 2.296996549400739,
      "grad_norm": 11.050960540771484,
      "learning_rate": 4.692406639851359e-06,
      "loss": 2.5907,
      "step": 112500
    },
    {
      "epoch": 2.2990383241113177,
      "grad_norm": 10.262489318847656,
      "learning_rate": 4.678794808447503e-06,
      "loss": 2.5447,
      "step": 112600
    },
    {
      "epoch": 2.3010800988218962,
      "grad_norm": 12.32262897491455,
      "learning_rate": 4.665182977043647e-06,
      "loss": 2.581,
      "step": 112700
    },
    {
      "epoch": 2.3031218735324743,
      "grad_norm": 11.194007873535156,
      "learning_rate": 4.6515711456397906e-06,
      "loss": 2.6076,
      "step": 112800
    },
    {
      "epoch": 2.305163648243053,
      "grad_norm": 9.516161918640137,
      "learning_rate": 4.637959314235934e-06,
      "loss": 2.5436,
      "step": 112900
    },
    {
      "epoch": 2.3072054229536314,
      "grad_norm": 10.283811569213867,
      "learning_rate": 4.624347482832078e-06,
      "loss": 2.5969,
      "step": 113000
    },
    {
      "epoch": 2.3092471976642095,
      "grad_norm": 11.06340503692627,
      "learning_rate": 4.610735651428222e-06,
      "loss": 2.5077,
      "step": 113100
    },
    {
      "epoch": 2.311288972374788,
      "grad_norm": 11.945902824401855,
      "learning_rate": 4.597123820024366e-06,
      "loss": 2.547,
      "step": 113200
    },
    {
      "epoch": 2.3133307470853666,
      "grad_norm": 9.924964904785156,
      "learning_rate": 4.583511988620509e-06,
      "loss": 2.5559,
      "step": 113300
    },
    {
      "epoch": 2.315372521795945,
      "grad_norm": 12.554821014404297,
      "learning_rate": 4.569900157216653e-06,
      "loss": 2.5735,
      "step": 113400
    },
    {
      "epoch": 2.3174142965065236,
      "grad_norm": 13.357717514038086,
      "learning_rate": 4.556288325812797e-06,
      "loss": 2.5012,
      "step": 113500
    },
    {
      "epoch": 2.3194560712171017,
      "grad_norm": 13.871496200561523,
      "learning_rate": 4.542676494408941e-06,
      "loss": 2.5354,
      "step": 113600
    },
    {
      "epoch": 2.3214978459276803,
      "grad_norm": 10.430380821228027,
      "learning_rate": 4.5290646630050845e-06,
      "loss": 2.4948,
      "step": 113700
    },
    {
      "epoch": 2.323539620638259,
      "grad_norm": 10.86793327331543,
      "learning_rate": 4.515452831601228e-06,
      "loss": 2.4681,
      "step": 113800
    },
    {
      "epoch": 2.3255813953488373,
      "grad_norm": 11.537545204162598,
      "learning_rate": 4.501841000197372e-06,
      "loss": 2.5153,
      "step": 113900
    },
    {
      "epoch": 2.3276231700594154,
      "grad_norm": 10.555373191833496,
      "learning_rate": 4.488229168793516e-06,
      "loss": 2.5742,
      "step": 114000
    },
    {
      "epoch": 2.329664944769994,
      "grad_norm": 9.989119529724121,
      "learning_rate": 4.47461733738966e-06,
      "loss": 2.4816,
      "step": 114100
    },
    {
      "epoch": 2.3317067194805725,
      "grad_norm": 10.738941192626953,
      "learning_rate": 4.461005505985803e-06,
      "loss": 2.5626,
      "step": 114200
    },
    {
      "epoch": 2.333748494191151,
      "grad_norm": 11.834014892578125,
      "learning_rate": 4.447393674581947e-06,
      "loss": 2.5374,
      "step": 114300
    },
    {
      "epoch": 2.3357902689017296,
      "grad_norm": 12.015923500061035,
      "learning_rate": 4.433781843178091e-06,
      "loss": 2.5354,
      "step": 114400
    },
    {
      "epoch": 2.3378320436123077,
      "grad_norm": 10.477693557739258,
      "learning_rate": 4.420170011774235e-06,
      "loss": 2.483,
      "step": 114500
    },
    {
      "epoch": 2.339873818322886,
      "grad_norm": 11.706559181213379,
      "learning_rate": 4.4065581803703785e-06,
      "loss": 2.5312,
      "step": 114600
    },
    {
      "epoch": 2.3419155930334647,
      "grad_norm": 10.577510833740234,
      "learning_rate": 4.392946348966522e-06,
      "loss": 2.5078,
      "step": 114700
    },
    {
      "epoch": 2.3439573677440433,
      "grad_norm": 13.039522171020508,
      "learning_rate": 4.379334517562666e-06,
      "loss": 2.5777,
      "step": 114800
    },
    {
      "epoch": 2.3459991424546214,
      "grad_norm": 10.455872535705566,
      "learning_rate": 4.36572268615881e-06,
      "loss": 2.5584,
      "step": 114900
    },
    {
      "epoch": 2.3480409171652,
      "grad_norm": 11.412222862243652,
      "learning_rate": 4.352110854754954e-06,
      "loss": 2.5518,
      "step": 115000
    },
    {
      "epoch": 2.3500826918757785,
      "grad_norm": 12.101985931396484,
      "learning_rate": 4.338499023351097e-06,
      "loss": 2.5201,
      "step": 115100
    },
    {
      "epoch": 2.352124466586357,
      "grad_norm": 9.395153999328613,
      "learning_rate": 4.324887191947241e-06,
      "loss": 2.5201,
      "step": 115200
    },
    {
      "epoch": 2.3541662412969355,
      "grad_norm": 10.892369270324707,
      "learning_rate": 4.311275360543385e-06,
      "loss": 2.5398,
      "step": 115300
    },
    {
      "epoch": 2.3562080160075136,
      "grad_norm": 8.508861541748047,
      "learning_rate": 4.297663529139529e-06,
      "loss": 2.5955,
      "step": 115400
    },
    {
      "epoch": 2.358249790718092,
      "grad_norm": 11.095959663391113,
      "learning_rate": 4.2840516977356725e-06,
      "loss": 2.5944,
      "step": 115500
    },
    {
      "epoch": 2.3602915654286707,
      "grad_norm": 12.953155517578125,
      "learning_rate": 4.270439866331816e-06,
      "loss": 2.505,
      "step": 115600
    },
    {
      "epoch": 2.3623333401392492,
      "grad_norm": 13.67667007446289,
      "learning_rate": 4.25682803492796e-06,
      "loss": 2.5476,
      "step": 115700
    },
    {
      "epoch": 2.3643751148498273,
      "grad_norm": 12.61252498626709,
      "learning_rate": 4.243216203524104e-06,
      "loss": 2.5642,
      "step": 115800
    },
    {
      "epoch": 2.366416889560406,
      "grad_norm": 10.879603385925293,
      "learning_rate": 4.229604372120248e-06,
      "loss": 2.5777,
      "step": 115900
    },
    {
      "epoch": 2.3684586642709844,
      "grad_norm": 10.512609481811523,
      "learning_rate": 4.21612865903043e-06,
      "loss": 2.5747,
      "step": 116000
    },
    {
      "epoch": 2.370500438981563,
      "grad_norm": 10.275594711303711,
      "learning_rate": 4.202516827626573e-06,
      "loss": 2.538,
      "step": 116100
    },
    {
      "epoch": 2.372542213692141,
      "grad_norm": 11.307558059692383,
      "learning_rate": 4.188904996222718e-06,
      "loss": 2.5305,
      "step": 116200
    },
    {
      "epoch": 2.3745839884027196,
      "grad_norm": 11.01069450378418,
      "learning_rate": 4.175293164818861e-06,
      "loss": 2.5358,
      "step": 116300
    },
    {
      "epoch": 2.376625763113298,
      "grad_norm": 10.824387550354004,
      "learning_rate": 4.161681333415005e-06,
      "loss": 2.5452,
      "step": 116400
    },
    {
      "epoch": 2.3786675378238766,
      "grad_norm": 9.319280624389648,
      "learning_rate": 4.1480695020111485e-06,
      "loss": 2.5722,
      "step": 116500
    },
    {
      "epoch": 2.3807093125344547,
      "grad_norm": 9.702244758605957,
      "learning_rate": 4.134457670607292e-06,
      "loss": 2.5727,
      "step": 116600
    },
    {
      "epoch": 2.3827510872450333,
      "grad_norm": 12.022186279296875,
      "learning_rate": 4.120981957517475e-06,
      "loss": 2.5623,
      "step": 116700
    },
    {
      "epoch": 2.384792861955612,
      "grad_norm": 9.161751747131348,
      "learning_rate": 4.1073701261136186e-06,
      "loss": 2.5177,
      "step": 116800
    },
    {
      "epoch": 2.3868346366661903,
      "grad_norm": 10.302237510681152,
      "learning_rate": 4.093758294709762e-06,
      "loss": 2.5307,
      "step": 116900
    },
    {
      "epoch": 2.388876411376769,
      "grad_norm": 10.26708984375,
      "learning_rate": 4.080146463305906e-06,
      "loss": 2.5018,
      "step": 117000
    },
    {
      "epoch": 2.390918186087347,
      "grad_norm": 9.398957252502441,
      "learning_rate": 4.066534631902049e-06,
      "loss": 2.5641,
      "step": 117100
    },
    {
      "epoch": 2.3929599607979255,
      "grad_norm": 11.192980766296387,
      "learning_rate": 4.052922800498194e-06,
      "loss": 2.5199,
      "step": 117200
    },
    {
      "epoch": 2.395001735508504,
      "grad_norm": 11.10905647277832,
      "learning_rate": 4.039310969094337e-06,
      "loss": 2.5763,
      "step": 117300
    },
    {
      "epoch": 2.3970435102190826,
      "grad_norm": 11.577380180358887,
      "learning_rate": 4.025699137690481e-06,
      "loss": 2.5562,
      "step": 117400
    },
    {
      "epoch": 2.3990852849296607,
      "grad_norm": 11.010472297668457,
      "learning_rate": 4.0120873062866245e-06,
      "loss": 2.5316,
      "step": 117500
    },
    {
      "epoch": 2.401127059640239,
      "grad_norm": 10.678616523742676,
      "learning_rate": 3.998475474882769e-06,
      "loss": 2.4985,
      "step": 117600
    },
    {
      "epoch": 2.4031688343508177,
      "grad_norm": 13.615570068359375,
      "learning_rate": 3.9848636434789125e-06,
      "loss": 2.4984,
      "step": 117700
    },
    {
      "epoch": 2.4052106090613963,
      "grad_norm": 11.668013572692871,
      "learning_rate": 3.971251812075056e-06,
      "loss": 2.561,
      "step": 117800
    },
    {
      "epoch": 2.407252383771975,
      "grad_norm": 11.956711769104004,
      "learning_rate": 3.9576399806712e-06,
      "loss": 2.5273,
      "step": 117900
    },
    {
      "epoch": 2.409294158482553,
      "grad_norm": 11.370319366455078,
      "learning_rate": 3.944028149267343e-06,
      "loss": 2.5937,
      "step": 118000
    },
    {
      "epoch": 2.4113359331931314,
      "grad_norm": 12.057043075561523,
      "learning_rate": 3.930416317863488e-06,
      "loss": 2.4616,
      "step": 118100
    },
    {
      "epoch": 2.41337770790371,
      "grad_norm": 13.048416137695312,
      "learning_rate": 3.916804486459631e-06,
      "loss": 2.483,
      "step": 118200
    },
    {
      "epoch": 2.4154194826142885,
      "grad_norm": 9.575398445129395,
      "learning_rate": 3.903192655055775e-06,
      "loss": 2.5192,
      "step": 118300
    },
    {
      "epoch": 2.4174612573248666,
      "grad_norm": 11.120023727416992,
      "learning_rate": 3.8895808236519185e-06,
      "loss": 2.5862,
      "step": 118400
    },
    {
      "epoch": 2.419503032035445,
      "grad_norm": 11.803633689880371,
      "learning_rate": 3.875968992248063e-06,
      "loss": 2.4661,
      "step": 118500
    },
    {
      "epoch": 2.4215448067460237,
      "grad_norm": 11.553836822509766,
      "learning_rate": 3.8623571608442065e-06,
      "loss": 2.5689,
      "step": 118600
    },
    {
      "epoch": 2.423586581456602,
      "grad_norm": 14.69987678527832,
      "learning_rate": 3.8488814477543886e-06,
      "loss": 2.5038,
      "step": 118700
    },
    {
      "epoch": 2.4256283561671808,
      "grad_norm": 10.954833030700684,
      "learning_rate": 3.835269616350532e-06,
      "loss": 2.4601,
      "step": 118800
    },
    {
      "epoch": 2.427670130877759,
      "grad_norm": 12.809349060058594,
      "learning_rate": 3.821657784946676e-06,
      "loss": 2.5157,
      "step": 118900
    },
    {
      "epoch": 2.4297119055883374,
      "grad_norm": 12.32655143737793,
      "learning_rate": 3.8080459535428197e-06,
      "loss": 2.5011,
      "step": 119000
    },
    {
      "epoch": 2.431753680298916,
      "grad_norm": 10.278943061828613,
      "learning_rate": 3.7944341221389637e-06,
      "loss": 2.5392,
      "step": 119100
    },
    {
      "epoch": 2.433795455009494,
      "grad_norm": 13.347545623779297,
      "learning_rate": 3.7808222907351073e-06,
      "loss": 2.5587,
      "step": 119200
    },
    {
      "epoch": 2.4358372297200725,
      "grad_norm": 9.51596736907959,
      "learning_rate": 3.767210459331251e-06,
      "loss": 2.4313,
      "step": 119300
    },
    {
      "epoch": 2.437879004430651,
      "grad_norm": 11.12389850616455,
      "learning_rate": 3.753598627927395e-06,
      "loss": 2.5329,
      "step": 119400
    },
    {
      "epoch": 2.4399207791412296,
      "grad_norm": 14.20007038116455,
      "learning_rate": 3.7399867965235385e-06,
      "loss": 2.6116,
      "step": 119500
    },
    {
      "epoch": 2.441962553851808,
      "grad_norm": 10.05749797821045,
      "learning_rate": 3.7263749651196825e-06,
      "loss": 2.5418,
      "step": 119600
    },
    {
      "epoch": 2.4440043285623863,
      "grad_norm": 11.538064956665039,
      "learning_rate": 3.712763133715826e-06,
      "loss": 2.5862,
      "step": 119700
    },
    {
      "epoch": 2.446046103272965,
      "grad_norm": 10.980047225952148,
      "learning_rate": 3.6991513023119697e-06,
      "loss": 2.5391,
      "step": 119800
    },
    {
      "epoch": 2.4480878779835433,
      "grad_norm": 10.235145568847656,
      "learning_rate": 3.6856755892221526e-06,
      "loss": 2.5138,
      "step": 119900
    },
    {
      "epoch": 2.450129652694122,
      "grad_norm": 9.407654762268066,
      "learning_rate": 3.672063757818296e-06,
      "loss": 2.4572,
      "step": 120000
    },
    {
      "epoch": 2.4521714274047,
      "grad_norm": 12.577969551086426,
      "learning_rate": 3.6584519264144398e-06,
      "loss": 2.4893,
      "step": 120100
    },
    {
      "epoch": 2.4542132021152785,
      "grad_norm": 11.296112060546875,
      "learning_rate": 3.6448400950105834e-06,
      "loss": 2.483,
      "step": 120200
    },
    {
      "epoch": 2.456254976825857,
      "grad_norm": 10.913347244262695,
      "learning_rate": 3.6312282636067274e-06,
      "loss": 2.532,
      "step": 120300
    },
    {
      "epoch": 2.4582967515364356,
      "grad_norm": 9.211339950561523,
      "learning_rate": 3.6176164322028714e-06,
      "loss": 2.5766,
      "step": 120400
    },
    {
      "epoch": 2.460338526247014,
      "grad_norm": 10.271697998046875,
      "learning_rate": 3.604004600799015e-06,
      "loss": 2.5061,
      "step": 120500
    },
    {
      "epoch": 2.462380300957592,
      "grad_norm": 12.285507202148438,
      "learning_rate": 3.5903927693951585e-06,
      "loss": 2.512,
      "step": 120600
    },
    {
      "epoch": 2.4644220756681707,
      "grad_norm": 11.419960975646973,
      "learning_rate": 3.576780937991302e-06,
      "loss": 2.541,
      "step": 120700
    },
    {
      "epoch": 2.4664638503787493,
      "grad_norm": 12.343499183654785,
      "learning_rate": 3.563169106587446e-06,
      "loss": 2.5578,
      "step": 120800
    },
    {
      "epoch": 2.468505625089328,
      "grad_norm": 10.285083770751953,
      "learning_rate": 3.54955727518359e-06,
      "loss": 2.5159,
      "step": 120900
    },
    {
      "epoch": 2.470547399799906,
      "grad_norm": 10.979265213012695,
      "learning_rate": 3.5359454437797337e-06,
      "loss": 2.5647,
      "step": 121000
    },
    {
      "epoch": 2.4725891745104844,
      "grad_norm": 10.334498405456543,
      "learning_rate": 3.5223336123758773e-06,
      "loss": 2.4946,
      "step": 121100
    },
    {
      "epoch": 2.474630949221063,
      "grad_norm": 11.120248794555664,
      "learning_rate": 3.5087217809720213e-06,
      "loss": 2.524,
      "step": 121200
    },
    {
      "epoch": 2.4766727239316415,
      "grad_norm": 9.841797828674316,
      "learning_rate": 3.495109949568165e-06,
      "loss": 2.516,
      "step": 121300
    },
    {
      "epoch": 2.47871449864222,
      "grad_norm": 11.527850151062012,
      "learning_rate": 3.481498118164309e-06,
      "loss": 2.509,
      "step": 121400
    },
    {
      "epoch": 2.480756273352798,
      "grad_norm": 10.353568077087402,
      "learning_rate": 3.4678862867604525e-06,
      "loss": 2.5263,
      "step": 121500
    },
    {
      "epoch": 2.4827980480633767,
      "grad_norm": 11.159709930419922,
      "learning_rate": 3.454274455356596e-06,
      "loss": 2.5311,
      "step": 121600
    },
    {
      "epoch": 2.484839822773955,
      "grad_norm": 12.54347038269043,
      "learning_rate": 3.44066262395274e-06,
      "loss": 2.5846,
      "step": 121700
    },
    {
      "epoch": 2.4868815974845337,
      "grad_norm": 11.890620231628418,
      "learning_rate": 3.4270507925488837e-06,
      "loss": 2.4914,
      "step": 121800
    },
    {
      "epoch": 2.488923372195112,
      "grad_norm": 12.098621368408203,
      "learning_rate": 3.4134389611450273e-06,
      "loss": 2.5225,
      "step": 121900
    },
    {
      "epoch": 2.4909651469056904,
      "grad_norm": 11.236416816711426,
      "learning_rate": 3.3998271297411713e-06,
      "loss": 2.553,
      "step": 122000
    },
    {
      "epoch": 2.493006921616269,
      "grad_norm": 11.97818374633789,
      "learning_rate": 3.3862152983373153e-06,
      "loss": 2.5378,
      "step": 122100
    },
    {
      "epoch": 2.4950486963268474,
      "grad_norm": 12.224698066711426,
      "learning_rate": 3.372603466933459e-06,
      "loss": 2.5255,
      "step": 122200
    },
    {
      "epoch": 2.4970904710374255,
      "grad_norm": 8.596482276916504,
      "learning_rate": 3.3589916355296025e-06,
      "loss": 2.5326,
      "step": 122300
    },
    {
      "epoch": 2.499132245748004,
      "grad_norm": 13.198668479919434,
      "learning_rate": 3.345379804125746e-06,
      "loss": 2.5171,
      "step": 122400
    },
    {
      "epoch": 2.5011740204585826,
      "grad_norm": 11.847162246704102,
      "learning_rate": 3.33176797272189e-06,
      "loss": 2.4811,
      "step": 122500
    },
    {
      "epoch": 2.503215795169161,
      "grad_norm": 14.146305084228516,
      "learning_rate": 3.318156141318034e-06,
      "loss": 2.4534,
      "step": 122600
    },
    {
      "epoch": 2.5052575698797392,
      "grad_norm": 11.521560668945312,
      "learning_rate": 3.3045443099141777e-06,
      "loss": 2.4575,
      "step": 122700
    },
    {
      "epoch": 2.507299344590318,
      "grad_norm": 13.834558486938477,
      "learning_rate": 3.2909324785103212e-06,
      "loss": 2.5848,
      "step": 122800
    },
    {
      "epoch": 2.5093411193008963,
      "grad_norm": 11.884876251220703,
      "learning_rate": 3.277320647106465e-06,
      "loss": 2.548,
      "step": 122900
    },
    {
      "epoch": 2.511382894011475,
      "grad_norm": 13.171670913696289,
      "learning_rate": 3.2637088157026093e-06,
      "loss": 2.5343,
      "step": 123000
    },
    {
      "epoch": 2.5134246687220534,
      "grad_norm": 11.737882614135742,
      "learning_rate": 3.250096984298753e-06,
      "loss": 2.5215,
      "step": 123100
    },
    {
      "epoch": 2.5154664434326315,
      "grad_norm": 11.3795804977417,
      "learning_rate": 3.2364851528948964e-06,
      "loss": 2.5011,
      "step": 123200
    },
    {
      "epoch": 2.51750821814321,
      "grad_norm": 12.703862190246582,
      "learning_rate": 3.22287332149104e-06,
      "loss": 2.5242,
      "step": 123300
    },
    {
      "epoch": 2.5195499928537886,
      "grad_norm": 9.695761680603027,
      "learning_rate": 3.2092614900871836e-06,
      "loss": 2.5665,
      "step": 123400
    },
    {
      "epoch": 2.521591767564367,
      "grad_norm": 9.867826461791992,
      "learning_rate": 3.195649658683328e-06,
      "loss": 2.465,
      "step": 123500
    },
    {
      "epoch": 2.523633542274945,
      "grad_norm": 10.910633087158203,
      "learning_rate": 3.1820378272794716e-06,
      "loss": 2.5217,
      "step": 123600
    },
    {
      "epoch": 2.5256753169855237,
      "grad_norm": 11.814021110534668,
      "learning_rate": 3.168425995875615e-06,
      "loss": 2.504,
      "step": 123700
    },
    {
      "epoch": 2.5277170916961023,
      "grad_norm": 10.546056747436523,
      "learning_rate": 3.154814164471759e-06,
      "loss": 2.5181,
      "step": 123800
    },
    {
      "epoch": 2.529758866406681,
      "grad_norm": 10.27556324005127,
      "learning_rate": 3.1412023330679032e-06,
      "loss": 2.5437,
      "step": 123900
    },
    {
      "epoch": 2.5318006411172593,
      "grad_norm": 10.888051986694336,
      "learning_rate": 3.127590501664047e-06,
      "loss": 2.5923,
      "step": 124000
    },
    {
      "epoch": 2.5338424158278374,
      "grad_norm": 11.990087509155273,
      "learning_rate": 3.114114788574229e-06,
      "loss": 2.6144,
      "step": 124100
    },
    {
      "epoch": 2.535884190538416,
      "grad_norm": 9.555392265319824,
      "learning_rate": 3.1005029571703725e-06,
      "loss": 2.4949,
      "step": 124200
    },
    {
      "epoch": 2.5379259652489945,
      "grad_norm": 12.239853858947754,
      "learning_rate": 3.0868911257665165e-06,
      "loss": 2.5297,
      "step": 124300
    },
    {
      "epoch": 2.5399677399595726,
      "grad_norm": 10.575701713562012,
      "learning_rate": 3.0732792943626605e-06,
      "loss": 2.5282,
      "step": 124400
    },
    {
      "epoch": 2.542009514670151,
      "grad_norm": 10.222051620483398,
      "learning_rate": 3.059667462958804e-06,
      "loss": 2.5164,
      "step": 124500
    },
    {
      "epoch": 2.5440512893807297,
      "grad_norm": 9.87734603881836,
      "learning_rate": 3.0460556315549476e-06,
      "loss": 2.5088,
      "step": 124600
    },
    {
      "epoch": 2.546093064091308,
      "grad_norm": 9.971848487854004,
      "learning_rate": 3.0324438001510912e-06,
      "loss": 2.4852,
      "step": 124700
    },
    {
      "epoch": 2.5481348388018867,
      "grad_norm": 11.738367080688477,
      "learning_rate": 3.0188319687472357e-06,
      "loss": 2.5229,
      "step": 124800
    },
    {
      "epoch": 2.5501766135124653,
      "grad_norm": 9.562053680419922,
      "learning_rate": 3.0052201373433792e-06,
      "loss": 2.4951,
      "step": 124900
    },
    {
      "epoch": 2.5522183882230434,
      "grad_norm": 9.240439414978027,
      "learning_rate": 2.991608305939523e-06,
      "loss": 2.534,
      "step": 125000
    },
    {
      "epoch": 2.554260162933622,
      "grad_norm": 11.4157075881958,
      "learning_rate": 2.9779964745356664e-06,
      "loss": 2.5169,
      "step": 125100
    },
    {
      "epoch": 2.5563019376442004,
      "grad_norm": 11.920050621032715,
      "learning_rate": 2.96438464313181e-06,
      "loss": 2.5032,
      "step": 125200
    },
    {
      "epoch": 2.5583437123547785,
      "grad_norm": 11.381786346435547,
      "learning_rate": 2.9507728117279544e-06,
      "loss": 2.496,
      "step": 125300
    },
    {
      "epoch": 2.560385487065357,
      "grad_norm": 11.985398292541504,
      "learning_rate": 2.937160980324098e-06,
      "loss": 2.5468,
      "step": 125400
    },
    {
      "epoch": 2.5624272617759356,
      "grad_norm": 10.536351203918457,
      "learning_rate": 2.9235491489202416e-06,
      "loss": 2.5235,
      "step": 125500
    },
    {
      "epoch": 2.564469036486514,
      "grad_norm": 10.389965057373047,
      "learning_rate": 2.909937317516385e-06,
      "loss": 2.5639,
      "step": 125600
    },
    {
      "epoch": 2.5665108111970927,
      "grad_norm": 13.292454719543457,
      "learning_rate": 2.8963254861125296e-06,
      "loss": 2.5386,
      "step": 125700
    },
    {
      "epoch": 2.568552585907671,
      "grad_norm": 11.841717720031738,
      "learning_rate": 2.8827136547086732e-06,
      "loss": 2.4795,
      "step": 125800
    },
    {
      "epoch": 2.5705943606182493,
      "grad_norm": 9.95061206817627,
      "learning_rate": 2.869101823304817e-06,
      "loss": 2.3992,
      "step": 125900
    },
    {
      "epoch": 2.572636135328828,
      "grad_norm": 12.29510498046875,
      "learning_rate": 2.8554899919009604e-06,
      "loss": 2.5551,
      "step": 126000
    },
    {
      "epoch": 2.5746779100394064,
      "grad_norm": 11.667339324951172,
      "learning_rate": 2.841878160497104e-06,
      "loss": 2.5453,
      "step": 126100
    },
    {
      "epoch": 2.5767196847499845,
      "grad_norm": 12.778297424316406,
      "learning_rate": 2.8282663290932484e-06,
      "loss": 2.4864,
      "step": 126200
    },
    {
      "epoch": 2.578761459460563,
      "grad_norm": 10.921343803405762,
      "learning_rate": 2.814654497689392e-06,
      "loss": 2.5004,
      "step": 126300
    },
    {
      "epoch": 2.5808032341711415,
      "grad_norm": 11.317898750305176,
      "learning_rate": 2.801178784599574e-06,
      "loss": 2.5041,
      "step": 126400
    },
    {
      "epoch": 2.58284500888172,
      "grad_norm": 10.989433288574219,
      "learning_rate": 2.7875669531957176e-06,
      "loss": 2.4862,
      "step": 126500
    },
    {
      "epoch": 2.5848867835922986,
      "grad_norm": 10.688600540161133,
      "learning_rate": 2.7739551217918616e-06,
      "loss": 2.575,
      "step": 126600
    },
    {
      "epoch": 2.5869285583028767,
      "grad_norm": 11.172359466552734,
      "learning_rate": 2.7603432903880057e-06,
      "loss": 2.5736,
      "step": 126700
    },
    {
      "epoch": 2.5889703330134552,
      "grad_norm": 12.794337272644043,
      "learning_rate": 2.7467314589841492e-06,
      "loss": 2.5176,
      "step": 126800
    },
    {
      "epoch": 2.591012107724034,
      "grad_norm": 11.24972152709961,
      "learning_rate": 2.733119627580293e-06,
      "loss": 2.5845,
      "step": 126900
    },
    {
      "epoch": 2.5930538824346123,
      "grad_norm": 12.715592384338379,
      "learning_rate": 2.7195077961764364e-06,
      "loss": 2.5661,
      "step": 127000
    },
    {
      "epoch": 2.5950956571451904,
      "grad_norm": 11.389493942260742,
      "learning_rate": 2.705895964772581e-06,
      "loss": 2.5495,
      "step": 127100
    },
    {
      "epoch": 2.597137431855769,
      "grad_norm": 11.69753646850586,
      "learning_rate": 2.6922841333687244e-06,
      "loss": 2.5645,
      "step": 127200
    },
    {
      "epoch": 2.5991792065663475,
      "grad_norm": 12.114255905151367,
      "learning_rate": 2.678672301964868e-06,
      "loss": 2.5306,
      "step": 127300
    },
    {
      "epoch": 2.601220981276926,
      "grad_norm": 10.002840042114258,
      "learning_rate": 2.6650604705610116e-06,
      "loss": 2.5261,
      "step": 127400
    },
    {
      "epoch": 2.6032627559875046,
      "grad_norm": 12.201254844665527,
      "learning_rate": 2.651448639157156e-06,
      "loss": 2.5416,
      "step": 127500
    },
    {
      "epoch": 2.6053045306980827,
      "grad_norm": 11.273972511291504,
      "learning_rate": 2.6378368077532996e-06,
      "loss": 2.563,
      "step": 127600
    },
    {
      "epoch": 2.607346305408661,
      "grad_norm": 11.368623733520508,
      "learning_rate": 2.624224976349443e-06,
      "loss": 2.4789,
      "step": 127700
    },
    {
      "epoch": 2.6093880801192397,
      "grad_norm": 9.629552841186523,
      "learning_rate": 2.610613144945587e-06,
      "loss": 2.4966,
      "step": 127800
    },
    {
      "epoch": 2.611429854829818,
      "grad_norm": 11.983728408813477,
      "learning_rate": 2.5970013135417304e-06,
      "loss": 2.5074,
      "step": 127900
    },
    {
      "epoch": 2.6134716295403964,
      "grad_norm": 10.256733894348145,
      "learning_rate": 2.583389482137875e-06,
      "loss": 2.5301,
      "step": 128000
    },
    {
      "epoch": 2.615513404250975,
      "grad_norm": 12.600557327270508,
      "learning_rate": 2.569913769048057e-06,
      "loss": 2.5811,
      "step": 128100
    },
    {
      "epoch": 2.6175551789615534,
      "grad_norm": 11.499430656433105,
      "learning_rate": 2.5563019376442005e-06,
      "loss": 2.5765,
      "step": 128200
    },
    {
      "epoch": 2.619596953672132,
      "grad_norm": 12.32287311553955,
      "learning_rate": 2.542690106240344e-06,
      "loss": 2.5489,
      "step": 128300
    },
    {
      "epoch": 2.6216387283827105,
      "grad_norm": 12.663899421691895,
      "learning_rate": 2.5290782748364876e-06,
      "loss": 2.4931,
      "step": 128400
    },
    {
      "epoch": 2.6236805030932886,
      "grad_norm": 11.842065811157227,
      "learning_rate": 2.515466443432632e-06,
      "loss": 2.5383,
      "step": 128500
    },
    {
      "epoch": 2.625722277803867,
      "grad_norm": 13.241072654724121,
      "learning_rate": 2.5018546120287756e-06,
      "loss": 2.5123,
      "step": 128600
    },
    {
      "epoch": 2.6277640525144457,
      "grad_norm": 14.46715259552002,
      "learning_rate": 2.4882427806249192e-06,
      "loss": 2.5489,
      "step": 128700
    },
    {
      "epoch": 2.6298058272250238,
      "grad_norm": 10.530211448669434,
      "learning_rate": 2.4746309492210632e-06,
      "loss": 2.528,
      "step": 128800
    },
    {
      "epoch": 2.6318476019356023,
      "grad_norm": 11.562005996704102,
      "learning_rate": 2.461019117817207e-06,
      "loss": 2.5035,
      "step": 128900
    },
    {
      "epoch": 2.633889376646181,
      "grad_norm": 12.29176139831543,
      "learning_rate": 2.447407286413351e-06,
      "loss": 2.5513,
      "step": 129000
    },
    {
      "epoch": 2.6359311513567594,
      "grad_norm": 14.729581832885742,
      "learning_rate": 2.4337954550094944e-06,
      "loss": 2.4983,
      "step": 129100
    },
    {
      "epoch": 2.637972926067338,
      "grad_norm": 14.328934669494629,
      "learning_rate": 2.4201836236056384e-06,
      "loss": 2.4944,
      "step": 129200
    },
    {
      "epoch": 2.640014700777916,
      "grad_norm": 10.131916046142578,
      "learning_rate": 2.406571792201782e-06,
      "loss": 2.5495,
      "step": 129300
    },
    {
      "epoch": 2.6420564754884945,
      "grad_norm": 11.336530685424805,
      "learning_rate": 2.3929599607979256e-06,
      "loss": 2.4959,
      "step": 129400
    },
    {
      "epoch": 2.644098250199073,
      "grad_norm": 12.336703300476074,
      "learning_rate": 2.3793481293940696e-06,
      "loss": 2.5539,
      "step": 129500
    },
    {
      "epoch": 2.6461400249096516,
      "grad_norm": 11.2036771774292,
      "learning_rate": 2.365736297990213e-06,
      "loss": 2.4728,
      "step": 129600
    },
    {
      "epoch": 2.6481817996202297,
      "grad_norm": 10.814988136291504,
      "learning_rate": 2.352124466586357e-06,
      "loss": 2.554,
      "step": 129700
    },
    {
      "epoch": 2.6502235743308082,
      "grad_norm": 10.77356243133545,
      "learning_rate": 2.338512635182501e-06,
      "loss": 2.5168,
      "step": 129800
    },
    {
      "epoch": 2.6522653490413868,
      "grad_norm": 10.764315605163574,
      "learning_rate": 2.324900803778645e-06,
      "loss": 2.5984,
      "step": 129900
    },
    {
      "epoch": 2.6543071237519653,
      "grad_norm": 11.193501472473145,
      "learning_rate": 2.3112889723747884e-06,
      "loss": 2.5091,
      "step": 130000
    },
    {
      "epoch": 2.656348898462544,
      "grad_norm": 11.821698188781738,
      "learning_rate": 2.2976771409709324e-06,
      "loss": 2.5818,
      "step": 130100
    },
    {
      "epoch": 2.658390673173122,
      "grad_norm": 10.501285552978516,
      "learning_rate": 2.284065309567076e-06,
      "loss": 2.4959,
      "step": 130200
    },
    {
      "epoch": 2.6604324478837005,
      "grad_norm": 12.260783195495605,
      "learning_rate": 2.2704534781632196e-06,
      "loss": 2.5424,
      "step": 130300
    },
    {
      "epoch": 2.662474222594279,
      "grad_norm": 10.274372100830078,
      "learning_rate": 2.2568416467593636e-06,
      "loss": 2.5314,
      "step": 130400
    },
    {
      "epoch": 2.664515997304857,
      "grad_norm": 11.064814567565918,
      "learning_rate": 2.243229815355507e-06,
      "loss": 2.5138,
      "step": 130500
    },
    {
      "epoch": 2.6665577720154356,
      "grad_norm": 9.298161506652832,
      "learning_rate": 2.229617983951651e-06,
      "loss": 2.5544,
      "step": 130600
    },
    {
      "epoch": 2.668599546726014,
      "grad_norm": 11.228350639343262,
      "learning_rate": 2.2160061525477948e-06,
      "loss": 2.4797,
      "step": 130700
    },
    {
      "epoch": 2.6706413214365927,
      "grad_norm": 11.80848217010498,
      "learning_rate": 2.2023943211439388e-06,
      "loss": 2.5208,
      "step": 130800
    },
    {
      "epoch": 2.6726830961471713,
      "grad_norm": 15.971631050109863,
      "learning_rate": 2.1887824897400824e-06,
      "loss": 2.4953,
      "step": 130900
    },
    {
      "epoch": 2.67472487085775,
      "grad_norm": 10.086109161376953,
      "learning_rate": 2.1751706583362264e-06,
      "loss": 2.5046,
      "step": 131000
    },
    {
      "epoch": 2.676766645568328,
      "grad_norm": 11.824028015136719,
      "learning_rate": 2.16155882693237e-06,
      "loss": 2.5394,
      "step": 131100
    },
    {
      "epoch": 2.6788084202789064,
      "grad_norm": 11.678210258483887,
      "learning_rate": 2.1479469955285135e-06,
      "loss": 2.4917,
      "step": 131200
    },
    {
      "epoch": 2.680850194989485,
      "grad_norm": 10.468450546264648,
      "learning_rate": 2.1343351641246575e-06,
      "loss": 2.4605,
      "step": 131300
    },
    {
      "epoch": 2.682891969700063,
      "grad_norm": 11.706595420837402,
      "learning_rate": 2.120723332720801e-06,
      "loss": 2.5016,
      "step": 131400
    },
    {
      "epoch": 2.6849337444106416,
      "grad_norm": 11.09096908569336,
      "learning_rate": 2.1071115013169447e-06,
      "loss": 2.4754,
      "step": 131500
    },
    {
      "epoch": 2.68697551912122,
      "grad_norm": 11.052014350891113,
      "learning_rate": 2.0934996699130887e-06,
      "loss": 2.5436,
      "step": 131600
    },
    {
      "epoch": 2.6890172938317987,
      "grad_norm": 12.528172492980957,
      "learning_rate": 2.0798878385092323e-06,
      "loss": 2.5306,
      "step": 131700
    },
    {
      "epoch": 2.691059068542377,
      "grad_norm": 11.768275260925293,
      "learning_rate": 2.0664121254194148e-06,
      "loss": 2.4716,
      "step": 131800
    },
    {
      "epoch": 2.6931008432529557,
      "grad_norm": 12.742101669311523,
      "learning_rate": 2.0528002940155584e-06,
      "loss": 2.5036,
      "step": 131900
    },
    {
      "epoch": 2.695142617963534,
      "grad_norm": 10.979228019714355,
      "learning_rate": 2.0391884626117024e-06,
      "loss": 2.5343,
      "step": 132000
    },
    {
      "epoch": 2.6971843926741124,
      "grad_norm": 11.48460865020752,
      "learning_rate": 2.025576631207846e-06,
      "loss": 2.5381,
      "step": 132100
    },
    {
      "epoch": 2.699226167384691,
      "grad_norm": 9.202483177185059,
      "learning_rate": 2.01196479980399e-06,
      "loss": 2.5394,
      "step": 132200
    },
    {
      "epoch": 2.701267942095269,
      "grad_norm": 10.600570678710938,
      "learning_rate": 1.9983529684001336e-06,
      "loss": 2.5178,
      "step": 132300
    },
    {
      "epoch": 2.7033097168058475,
      "grad_norm": 12.169815063476562,
      "learning_rate": 1.9847411369962776e-06,
      "loss": 2.4902,
      "step": 132400
    },
    {
      "epoch": 2.705351491516426,
      "grad_norm": 13.245137214660645,
      "learning_rate": 1.971129305592421e-06,
      "loss": 2.5116,
      "step": 132500
    },
    {
      "epoch": 2.7073932662270046,
      "grad_norm": 10.938136100769043,
      "learning_rate": 1.957517474188565e-06,
      "loss": 2.5962,
      "step": 132600
    },
    {
      "epoch": 2.709435040937583,
      "grad_norm": 11.9147310256958,
      "learning_rate": 1.9439056427847088e-06,
      "loss": 2.4774,
      "step": 132700
    },
    {
      "epoch": 2.7114768156481612,
      "grad_norm": 11.921369552612305,
      "learning_rate": 1.9302938113808523e-06,
      "loss": 2.5107,
      "step": 132800
    },
    {
      "epoch": 2.7135185903587398,
      "grad_norm": 11.085768699645996,
      "learning_rate": 1.9166819799769963e-06,
      "loss": 2.5105,
      "step": 132900
    },
    {
      "epoch": 2.7155603650693183,
      "grad_norm": 12.403414726257324,
      "learning_rate": 1.9030701485731397e-06,
      "loss": 2.4484,
      "step": 133000
    },
    {
      "epoch": 2.717602139779897,
      "grad_norm": 11.625782012939453,
      "learning_rate": 1.8894583171692837e-06,
      "loss": 2.5564,
      "step": 133100
    },
    {
      "epoch": 2.719643914490475,
      "grad_norm": 11.261124610900879,
      "learning_rate": 1.8758464857654273e-06,
      "loss": 2.475,
      "step": 133200
    },
    {
      "epoch": 2.7216856892010535,
      "grad_norm": 10.696845054626465,
      "learning_rate": 1.8622346543615713e-06,
      "loss": 2.4644,
      "step": 133300
    },
    {
      "epoch": 2.723727463911632,
      "grad_norm": 13.033003807067871,
      "learning_rate": 1.848622822957715e-06,
      "loss": 2.5121,
      "step": 133400
    },
    {
      "epoch": 2.7257692386222105,
      "grad_norm": 11.319334983825684,
      "learning_rate": 1.835010991553859e-06,
      "loss": 2.5448,
      "step": 133500
    },
    {
      "epoch": 2.727811013332789,
      "grad_norm": 12.494309425354004,
      "learning_rate": 1.8213991601500025e-06,
      "loss": 2.5398,
      "step": 133600
    },
    {
      "epoch": 2.729852788043367,
      "grad_norm": 11.48712158203125,
      "learning_rate": 1.8077873287461465e-06,
      "loss": 2.522,
      "step": 133700
    },
    {
      "epoch": 2.7318945627539457,
      "grad_norm": 12.039661407470703,
      "learning_rate": 1.79417549734229e-06,
      "loss": 2.5046,
      "step": 133800
    },
    {
      "epoch": 2.7339363374645242,
      "grad_norm": 10.444840431213379,
      "learning_rate": 1.7805636659384337e-06,
      "loss": 2.5012,
      "step": 133900
    },
    {
      "epoch": 2.7359781121751023,
      "grad_norm": 11.003961563110352,
      "learning_rate": 1.7669518345345777e-06,
      "loss": 2.5302,
      "step": 134000
    },
    {
      "epoch": 2.738019886885681,
      "grad_norm": 11.58273983001709,
      "learning_rate": 1.7533400031307213e-06,
      "loss": 2.5466,
      "step": 134100
    },
    {
      "epoch": 2.7400616615962594,
      "grad_norm": 11.599047660827637,
      "learning_rate": 1.7397281717268653e-06,
      "loss": 2.5292,
      "step": 134200
    },
    {
      "epoch": 2.742103436306838,
      "grad_norm": 13.503471374511719,
      "learning_rate": 1.7261163403230089e-06,
      "loss": 2.5203,
      "step": 134300
    },
    {
      "epoch": 2.7441452110174165,
      "grad_norm": 11.47864818572998,
      "learning_rate": 1.7125045089191527e-06,
      "loss": 2.5012,
      "step": 134400
    },
    {
      "epoch": 2.746186985727995,
      "grad_norm": 12.10765266418457,
      "learning_rate": 1.6988926775152965e-06,
      "loss": 2.5246,
      "step": 134500
    },
    {
      "epoch": 2.748228760438573,
      "grad_norm": 11.029584884643555,
      "learning_rate": 1.6852808461114403e-06,
      "loss": 2.5534,
      "step": 134600
    },
    {
      "epoch": 2.7502705351491517,
      "grad_norm": 11.535530090332031,
      "learning_rate": 1.671669014707584e-06,
      "loss": 2.5221,
      "step": 134700
    },
    {
      "epoch": 2.75231230985973,
      "grad_norm": 11.2266206741333,
      "learning_rate": 1.6580571833037277e-06,
      "loss": 2.5243,
      "step": 134800
    },
    {
      "epoch": 2.7543540845703083,
      "grad_norm": 12.524577140808105,
      "learning_rate": 1.6444453518998715e-06,
      "loss": 2.4673,
      "step": 134900
    },
    {
      "epoch": 2.756395859280887,
      "grad_norm": 11.419313430786133,
      "learning_rate": 1.6308335204960153e-06,
      "loss": 2.4971,
      "step": 135000
    },
    {
      "epoch": 2.7584376339914654,
      "grad_norm": 12.33151912689209,
      "learning_rate": 1.617221689092159e-06,
      "loss": 2.5157,
      "step": 135100
    },
    {
      "epoch": 2.760479408702044,
      "grad_norm": 12.186675071716309,
      "learning_rate": 1.6036098576883028e-06,
      "loss": 2.5841,
      "step": 135200
    },
    {
      "epoch": 2.7625211834126224,
      "grad_norm": 12.014427185058594,
      "learning_rate": 1.5899980262844466e-06,
      "loss": 2.5666,
      "step": 135300
    },
    {
      "epoch": 2.7645629581232005,
      "grad_norm": 10.424745559692383,
      "learning_rate": 1.5763861948805902e-06,
      "loss": 2.5196,
      "step": 135400
    },
    {
      "epoch": 2.766604732833779,
      "grad_norm": 13.062411308288574,
      "learning_rate": 1.5627743634767342e-06,
      "loss": 2.5969,
      "step": 135500
    },
    {
      "epoch": 2.7686465075443576,
      "grad_norm": 12.782362937927246,
      "learning_rate": 1.5491625320728778e-06,
      "loss": 2.5296,
      "step": 135600
    },
    {
      "epoch": 2.770688282254936,
      "grad_norm": 10.49001407623291,
      "learning_rate": 1.5355507006690214e-06,
      "loss": 2.4685,
      "step": 135700
    },
    {
      "epoch": 2.7727300569655142,
      "grad_norm": 12.350069999694824,
      "learning_rate": 1.5219388692651654e-06,
      "loss": 2.5846,
      "step": 135800
    },
    {
      "epoch": 2.7747718316760928,
      "grad_norm": 11.573190689086914,
      "learning_rate": 1.508327037861309e-06,
      "loss": 2.4529,
      "step": 135900
    },
    {
      "epoch": 2.7768136063866713,
      "grad_norm": 12.518597602844238,
      "learning_rate": 1.494715206457453e-06,
      "loss": 2.5343,
      "step": 136000
    },
    {
      "epoch": 2.77885538109725,
      "grad_norm": 10.931106567382812,
      "learning_rate": 1.4811033750535966e-06,
      "loss": 2.5337,
      "step": 136100
    },
    {
      "epoch": 2.7808971558078284,
      "grad_norm": 9.927188873291016,
      "learning_rate": 1.4674915436497406e-06,
      "loss": 2.5308,
      "step": 136200
    },
    {
      "epoch": 2.7829389305184065,
      "grad_norm": 15.127434730529785,
      "learning_rate": 1.4538797122458842e-06,
      "loss": 2.4857,
      "step": 136300
    },
    {
      "epoch": 2.784980705228985,
      "grad_norm": 9.65871524810791,
      "learning_rate": 1.4404039991560665e-06,
      "loss": 2.5119,
      "step": 136400
    },
    {
      "epoch": 2.7870224799395635,
      "grad_norm": 10.293863296508789,
      "learning_rate": 1.4267921677522103e-06,
      "loss": 2.4852,
      "step": 136500
    },
    {
      "epoch": 2.7890642546501416,
      "grad_norm": 12.603757858276367,
      "learning_rate": 1.413180336348354e-06,
      "loss": 2.5468,
      "step": 136600
    },
    {
      "epoch": 2.79110602936072,
      "grad_norm": 11.892060279846191,
      "learning_rate": 1.3995685049444979e-06,
      "loss": 2.4926,
      "step": 136700
    },
    {
      "epoch": 2.7931478040712987,
      "grad_norm": 14.072787284851074,
      "learning_rate": 1.3859566735406417e-06,
      "loss": 2.5107,
      "step": 136800
    },
    {
      "epoch": 2.7951895787818772,
      "grad_norm": 10.694106101989746,
      "learning_rate": 1.3723448421367855e-06,
      "loss": 2.4708,
      "step": 136900
    },
    {
      "epoch": 2.7972313534924558,
      "grad_norm": 9.956501960754395,
      "learning_rate": 1.358733010732929e-06,
      "loss": 2.4963,
      "step": 137000
    },
    {
      "epoch": 2.7992731282030343,
      "grad_norm": 11.133196830749512,
      "learning_rate": 1.345121179329073e-06,
      "loss": 2.5018,
      "step": 137100
    },
    {
      "epoch": 2.8013149029136124,
      "grad_norm": 10.423912048339844,
      "learning_rate": 1.3315093479252166e-06,
      "loss": 2.4416,
      "step": 137200
    },
    {
      "epoch": 2.803356677624191,
      "grad_norm": 11.408185958862305,
      "learning_rate": 1.3178975165213606e-06,
      "loss": 2.5266,
      "step": 137300
    },
    {
      "epoch": 2.8053984523347695,
      "grad_norm": 12.32462215423584,
      "learning_rate": 1.3042856851175042e-06,
      "loss": 2.539,
      "step": 137400
    },
    {
      "epoch": 2.8074402270453476,
      "grad_norm": 12.993631362915039,
      "learning_rate": 1.2906738537136478e-06,
      "loss": 2.4696,
      "step": 137500
    },
    {
      "epoch": 2.809482001755926,
      "grad_norm": 11.310327529907227,
      "learning_rate": 1.2770620223097918e-06,
      "loss": 2.538,
      "step": 137600
    },
    {
      "epoch": 2.8115237764665046,
      "grad_norm": 11.661127090454102,
      "learning_rate": 1.2634501909059354e-06,
      "loss": 2.5413,
      "step": 137700
    },
    {
      "epoch": 2.813565551177083,
      "grad_norm": 11.578876495361328,
      "learning_rate": 1.2498383595020792e-06,
      "loss": 2.5186,
      "step": 137800
    },
    {
      "epoch": 2.8156073258876617,
      "grad_norm": 11.864676475524902,
      "learning_rate": 1.236226528098223e-06,
      "loss": 2.5174,
      "step": 137900
    },
    {
      "epoch": 2.8176491005982403,
      "grad_norm": 11.667328834533691,
      "learning_rate": 1.2226146966943668e-06,
      "loss": 2.5115,
      "step": 138000
    },
    {
      "epoch": 2.8196908753088183,
      "grad_norm": 10.322450637817383,
      "learning_rate": 1.2090028652905106e-06,
      "loss": 2.4833,
      "step": 138100
    },
    {
      "epoch": 2.821732650019397,
      "grad_norm": 11.706233024597168,
      "learning_rate": 1.1953910338866544e-06,
      "loss": 2.5192,
      "step": 138200
    },
    {
      "epoch": 2.8237744247299754,
      "grad_norm": 11.744293212890625,
      "learning_rate": 1.1817792024827982e-06,
      "loss": 2.509,
      "step": 138300
    },
    {
      "epoch": 2.8258161994405535,
      "grad_norm": 12.100618362426758,
      "learning_rate": 1.1683034893929805e-06,
      "loss": 2.5601,
      "step": 138400
    },
    {
      "epoch": 2.827857974151132,
      "grad_norm": 12.1827392578125,
      "learning_rate": 1.1546916579891243e-06,
      "loss": 2.4799,
      "step": 138500
    },
    {
      "epoch": 2.8298997488617106,
      "grad_norm": 11.605539321899414,
      "learning_rate": 1.1410798265852678e-06,
      "loss": 2.5316,
      "step": 138600
    },
    {
      "epoch": 2.831941523572289,
      "grad_norm": 10.95483112335205,
      "learning_rate": 1.1274679951814116e-06,
      "loss": 2.4945,
      "step": 138700
    },
    {
      "epoch": 2.8339832982828677,
      "grad_norm": 11.036602020263672,
      "learning_rate": 1.1138561637775554e-06,
      "loss": 2.4421,
      "step": 138800
    },
    {
      "epoch": 2.8360250729934458,
      "grad_norm": 10.498315811157227,
      "learning_rate": 1.1002443323736992e-06,
      "loss": 2.5116,
      "step": 138900
    },
    {
      "epoch": 2.8380668477040243,
      "grad_norm": 14.136528968811035,
      "learning_rate": 1.086632500969843e-06,
      "loss": 2.5185,
      "step": 139000
    },
    {
      "epoch": 2.840108622414603,
      "grad_norm": 10.739221572875977,
      "learning_rate": 1.0730206695659868e-06,
      "loss": 2.525,
      "step": 139100
    },
    {
      "epoch": 2.8421503971251814,
      "grad_norm": 11.946866035461426,
      "learning_rate": 1.0594088381621306e-06,
      "loss": 2.5341,
      "step": 139200
    },
    {
      "epoch": 2.8441921718357595,
      "grad_norm": 13.73058795928955,
      "learning_rate": 1.0457970067582744e-06,
      "loss": 2.448,
      "step": 139300
    },
    {
      "epoch": 2.846233946546338,
      "grad_norm": 10.972211837768555,
      "learning_rate": 1.0321851753544182e-06,
      "loss": 2.501,
      "step": 139400
    },
    {
      "epoch": 2.8482757212569165,
      "grad_norm": 11.925576210021973,
      "learning_rate": 1.0185733439505618e-06,
      "loss": 2.5333,
      "step": 139500
    },
    {
      "epoch": 2.850317495967495,
      "grad_norm": 9.90451431274414,
      "learning_rate": 1.0049615125467056e-06,
      "loss": 2.523,
      "step": 139600
    },
    {
      "epoch": 2.8523592706780736,
      "grad_norm": 11.0416841506958,
      "learning_rate": 9.913496811428494e-07,
      "loss": 2.4245,
      "step": 139700
    },
    {
      "epoch": 2.8544010453886517,
      "grad_norm": 12.248516082763672,
      "learning_rate": 9.777378497389932e-07,
      "loss": 2.5031,
      "step": 139800
    },
    {
      "epoch": 2.8564428200992302,
      "grad_norm": 11.41672420501709,
      "learning_rate": 9.64126018335137e-07,
      "loss": 2.501,
      "step": 139900
    },
    {
      "epoch": 2.8584845948098088,
      "grad_norm": 13.721906661987305,
      "learning_rate": 9.505141869312808e-07,
      "loss": 2.4791,
      "step": 140000
    },
    {
      "epoch": 2.860526369520387,
      "grad_norm": 10.764596939086914,
      "learning_rate": 9.369023555274245e-07,
      "loss": 2.5158,
      "step": 140100
    },
    {
      "epoch": 2.8625681442309654,
      "grad_norm": 11.743295669555664,
      "learning_rate": 9.232905241235683e-07,
      "loss": 2.54,
      "step": 140200
    },
    {
      "epoch": 2.864609918941544,
      "grad_norm": 12.357276916503906,
      "learning_rate": 9.096786927197121e-07,
      "loss": 2.5412,
      "step": 140300
    },
    {
      "epoch": 2.8666516936521225,
      "grad_norm": 11.478506088256836,
      "learning_rate": 8.960668613158559e-07,
      "loss": 2.5267,
      "step": 140400
    },
    {
      "epoch": 2.868693468362701,
      "grad_norm": 13.438267707824707,
      "learning_rate": 8.824550299119995e-07,
      "loss": 2.5091,
      "step": 140500
    },
    {
      "epoch": 2.8707352430732795,
      "grad_norm": 11.395661354064941,
      "learning_rate": 8.688431985081433e-07,
      "loss": 2.5023,
      "step": 140600
    },
    {
      "epoch": 2.8727770177838576,
      "grad_norm": 10.702506065368652,
      "learning_rate": 8.552313671042871e-07,
      "loss": 2.4918,
      "step": 140700
    },
    {
      "epoch": 2.874818792494436,
      "grad_norm": 9.796712875366211,
      "learning_rate": 8.416195357004309e-07,
      "loss": 2.3998,
      "step": 140800
    },
    {
      "epoch": 2.8768605672050147,
      "grad_norm": 10.375988960266113,
      "learning_rate": 8.281438226106132e-07,
      "loss": 2.5629,
      "step": 140900
    },
    {
      "epoch": 2.878902341915593,
      "grad_norm": 13.122040748596191,
      "learning_rate": 8.14531991206757e-07,
      "loss": 2.5233,
      "step": 141000
    },
    {
      "epoch": 2.8809441166261713,
      "grad_norm": 12.40261459350586,
      "learning_rate": 8.009201598029008e-07,
      "loss": 2.4988,
      "step": 141100
    },
    {
      "epoch": 2.88298589133675,
      "grad_norm": 11.533076286315918,
      "learning_rate": 7.873083283990444e-07,
      "loss": 2.5313,
      "step": 141200
    },
    {
      "epoch": 2.8850276660473284,
      "grad_norm": 9.862324714660645,
      "learning_rate": 7.736964969951882e-07,
      "loss": 2.4195,
      "step": 141300
    },
    {
      "epoch": 2.887069440757907,
      "grad_norm": 13.020647048950195,
      "learning_rate": 7.60084665591332e-07,
      "loss": 2.5369,
      "step": 141400
    },
    {
      "epoch": 2.8891112154684855,
      "grad_norm": 11.232259750366211,
      "learning_rate": 7.464728341874758e-07,
      "loss": 2.4869,
      "step": 141500
    },
    {
      "epoch": 2.8911529901790636,
      "grad_norm": 10.628355979919434,
      "learning_rate": 7.328610027836196e-07,
      "loss": 2.4426,
      "step": 141600
    },
    {
      "epoch": 2.893194764889642,
      "grad_norm": 12.257620811462402,
      "learning_rate": 7.192491713797634e-07,
      "loss": 2.5225,
      "step": 141700
    },
    {
      "epoch": 2.8952365396002206,
      "grad_norm": 10.561981201171875,
      "learning_rate": 7.056373399759071e-07,
      "loss": 2.5334,
      "step": 141800
    },
    {
      "epoch": 2.8972783143107987,
      "grad_norm": 12.690499305725098,
      "learning_rate": 6.920255085720509e-07,
      "loss": 2.492,
      "step": 141900
    },
    {
      "epoch": 2.8993200890213773,
      "grad_norm": 11.081511497497559,
      "learning_rate": 6.784136771681947e-07,
      "loss": 2.5552,
      "step": 142000
    },
    {
      "epoch": 2.901361863731956,
      "grad_norm": 12.82912540435791,
      "learning_rate": 6.648018457643385e-07,
      "loss": 2.5414,
      "step": 142100
    },
    {
      "epoch": 2.9034036384425344,
      "grad_norm": 12.77222728729248,
      "learning_rate": 6.511900143604821e-07,
      "loss": 2.4786,
      "step": 142200
    },
    {
      "epoch": 2.905445413153113,
      "grad_norm": 12.690450668334961,
      "learning_rate": 6.375781829566259e-07,
      "loss": 2.4722,
      "step": 142300
    },
    {
      "epoch": 2.907487187863691,
      "grad_norm": 12.512415885925293,
      "learning_rate": 6.239663515527697e-07,
      "loss": 2.5401,
      "step": 142400
    },
    {
      "epoch": 2.9095289625742695,
      "grad_norm": 11.198054313659668,
      "learning_rate": 6.103545201489135e-07,
      "loss": 2.4987,
      "step": 142500
    },
    {
      "epoch": 2.911570737284848,
      "grad_norm": 12.886202812194824,
      "learning_rate": 5.967426887450573e-07,
      "loss": 2.5276,
      "step": 142600
    },
    {
      "epoch": 2.9136125119954266,
      "grad_norm": 8.853809356689453,
      "learning_rate": 5.831308573412011e-07,
      "loss": 2.5459,
      "step": 142700
    },
    {
      "epoch": 2.9156542867060047,
      "grad_norm": 13.163164138793945,
      "learning_rate": 5.695190259373449e-07,
      "loss": 2.5686,
      "step": 142800
    },
    {
      "epoch": 2.917696061416583,
      "grad_norm": 12.223258972167969,
      "learning_rate": 5.559071945334886e-07,
      "loss": 2.511,
      "step": 142900
    },
    {
      "epoch": 2.9197378361271618,
      "grad_norm": 10.960702896118164,
      "learning_rate": 5.422953631296324e-07,
      "loss": 2.4972,
      "step": 143000
    },
    {
      "epoch": 2.9217796108377403,
      "grad_norm": 9.923859596252441,
      "learning_rate": 5.288196500398147e-07,
      "loss": 2.4824,
      "step": 143100
    },
    {
      "epoch": 2.923821385548319,
      "grad_norm": 11.607508659362793,
      "learning_rate": 5.152078186359584e-07,
      "loss": 2.4701,
      "step": 143200
    },
    {
      "epoch": 2.925863160258897,
      "grad_norm": 9.812458038330078,
      "learning_rate": 5.015959872321022e-07,
      "loss": 2.5325,
      "step": 143300
    },
    {
      "epoch": 2.9279049349694755,
      "grad_norm": 9.520132064819336,
      "learning_rate": 4.87984155828246e-07,
      "loss": 2.4976,
      "step": 143400
    },
    {
      "epoch": 2.929946709680054,
      "grad_norm": 11.75566577911377,
      "learning_rate": 4.7437232442438976e-07,
      "loss": 2.4952,
      "step": 143500
    },
    {
      "epoch": 2.931988484390632,
      "grad_norm": 16.803638458251953,
      "learning_rate": 4.6076049302053345e-07,
      "loss": 2.5148,
      "step": 143600
    },
    {
      "epoch": 2.9340302591012106,
      "grad_norm": 13.634774208068848,
      "learning_rate": 4.4714866161667725e-07,
      "loss": 2.5039,
      "step": 143700
    },
    {
      "epoch": 2.936072033811789,
      "grad_norm": 11.485950469970703,
      "learning_rate": 4.33536830212821e-07,
      "loss": 2.5294,
      "step": 143800
    },
    {
      "epoch": 2.9381138085223677,
      "grad_norm": 11.735077857971191,
      "learning_rate": 4.199249988089648e-07,
      "loss": 2.4601,
      "step": 143900
    },
    {
      "epoch": 2.9401555832329462,
      "grad_norm": 13.27263355255127,
      "learning_rate": 4.063131674051086e-07,
      "loss": 2.4148,
      "step": 144000
    },
    {
      "epoch": 2.9421973579435248,
      "grad_norm": 9.941821098327637,
      "learning_rate": 3.928374543152909e-07,
      "loss": 2.4994,
      "step": 144100
    },
    {
      "epoch": 2.944239132654103,
      "grad_norm": 11.726531982421875,
      "learning_rate": 3.7922562291143465e-07,
      "loss": 2.4702,
      "step": 144200
    },
    {
      "epoch": 2.9462809073646814,
      "grad_norm": 11.19430160522461,
      "learning_rate": 3.6561379150757845e-07,
      "loss": 2.4578,
      "step": 144300
    },
    {
      "epoch": 2.94832268207526,
      "grad_norm": 9.570396423339844,
      "learning_rate": 3.5200196010372214e-07,
      "loss": 2.5354,
      "step": 144400
    },
    {
      "epoch": 2.950364456785838,
      "grad_norm": 10.989849090576172,
      "learning_rate": 3.3839012869986594e-07,
      "loss": 2.5583,
      "step": 144500
    },
    {
      "epoch": 2.9524062314964166,
      "grad_norm": 12.197154998779297,
      "learning_rate": 3.2477829729600974e-07,
      "loss": 2.4941,
      "step": 144600
    },
    {
      "epoch": 2.954448006206995,
      "grad_norm": 10.222344398498535,
      "learning_rate": 3.111664658921535e-07,
      "loss": 2.4816,
      "step": 144700
    },
    {
      "epoch": 2.9564897809175736,
      "grad_norm": 9.963250160217285,
      "learning_rate": 2.975546344882973e-07,
      "loss": 2.4561,
      "step": 144800
    },
    {
      "epoch": 2.958531555628152,
      "grad_norm": 16.029863357543945,
      "learning_rate": 2.83942803084441e-07,
      "loss": 2.4748,
      "step": 144900
    },
    {
      "epoch": 2.9605733303387303,
      "grad_norm": 9.908947944641113,
      "learning_rate": 2.7033097168058477e-07,
      "loss": 2.4133,
      "step": 145000
    },
    {
      "epoch": 2.962615105049309,
      "grad_norm": 12.034757614135742,
      "learning_rate": 2.5671914027672856e-07,
      "loss": 2.4515,
      "step": 145100
    },
    {
      "epoch": 2.9646568797598873,
      "grad_norm": 11.539511680603027,
      "learning_rate": 2.431073088728723e-07,
      "loss": 2.4953,
      "step": 145200
    },
    {
      "epoch": 2.966698654470466,
      "grad_norm": 12.229674339294434,
      "learning_rate": 2.2949547746901608e-07,
      "loss": 2.5205,
      "step": 145300
    },
    {
      "epoch": 2.968740429181044,
      "grad_norm": 11.974417686462402,
      "learning_rate": 2.1588364606515982e-07,
      "loss": 2.5149,
      "step": 145400
    },
    {
      "epoch": 2.9707822038916225,
      "grad_norm": 10.373720169067383,
      "learning_rate": 2.0227181466130362e-07,
      "loss": 2.4948,
      "step": 145500
    },
    {
      "epoch": 2.972823978602201,
      "grad_norm": 12.940185546875,
      "learning_rate": 1.886599832574474e-07,
      "loss": 2.46,
      "step": 145600
    },
    {
      "epoch": 2.9748657533127796,
      "grad_norm": 10.567899703979492,
      "learning_rate": 1.7504815185359114e-07,
      "loss": 2.5109,
      "step": 145700
    },
    {
      "epoch": 2.976907528023358,
      "grad_norm": 9.376663208007812,
      "learning_rate": 1.6143632044973494e-07,
      "loss": 2.5295,
      "step": 145800
    },
    {
      "epoch": 2.978949302733936,
      "grad_norm": 10.497828483581543,
      "learning_rate": 1.478244890458787e-07,
      "loss": 2.4962,
      "step": 145900
    },
    {
      "epoch": 2.9809910774445147,
      "grad_norm": 13.55769157409668,
      "learning_rate": 1.3421265764202245e-07,
      "loss": 2.5335,
      "step": 146000
    }
  ],
  "logging_steps": 100,
  "max_steps": 146931,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 1000,
  "total_flos": 1.5565343065786886e+17,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
