{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 293862,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0010208873552892174,
      "grad_norm": 24.841882705688477,
      "learning_rate": 3.8000000000000005e-06,
      "loss": 4.2223,
      "step": 100
    },
    {
      "epoch": 0.0020417747105784347,
      "grad_norm": 23.14143943786621,
      "learning_rate": 7.800000000000002e-06,
      "loss": 4.0077,
      "step": 200
    },
    {
      "epoch": 0.0030626620658676523,
      "grad_norm": 19.71358299255371,
      "learning_rate": 1.18e-05,
      "loss": 3.9136,
      "step": 300
    },
    {
      "epoch": 0.0040835494211568695,
      "grad_norm": 22.62445640563965,
      "learning_rate": 1.58e-05,
      "loss": 3.8631,
      "step": 400
    },
    {
      "epoch": 0.005104436776446087,
      "grad_norm": 18.03577423095703,
      "learning_rate": 1.98e-05,
      "loss": 3.6524,
      "step": 500
    },
    {
      "epoch": 0.006125324131735305,
      "grad_norm": 28.122587203979492,
      "learning_rate": 1.9993523360217072e-05,
      "loss": 3.5918,
      "step": 600
    },
    {
      "epoch": 0.007146211487024522,
      "grad_norm": 18.482528686523438,
      "learning_rate": 1.998670584465609e-05,
      "loss": 3.4123,
      "step": 700
    },
    {
      "epoch": 0.008167098842313739,
      "grad_norm": 14.900408744812012,
      "learning_rate": 1.997988832909511e-05,
      "loss": 3.3661,
      "step": 800
    },
    {
      "epoch": 0.009187986197602957,
      "grad_norm": 26.731082916259766,
      "learning_rate": 1.9973070813534134e-05,
      "loss": 3.4076,
      "step": 900
    },
    {
      "epoch": 0.010208873552892173,
      "grad_norm": 28.310441970825195,
      "learning_rate": 1.9966253297973154e-05,
      "loss": 3.4117,
      "step": 1000
    },
    {
      "epoch": 0.011229760908181391,
      "grad_norm": 18.291467666625977,
      "learning_rate": 1.9959435782412177e-05,
      "loss": 3.3789,
      "step": 1100
    },
    {
      "epoch": 0.01225064826347061,
      "grad_norm": 15.798775672912598,
      "learning_rate": 1.9952618266851196e-05,
      "loss": 3.3175,
      "step": 1200
    },
    {
      "epoch": 0.013271535618759826,
      "grad_norm": 17.861421585083008,
      "learning_rate": 1.9945800751290216e-05,
      "loss": 3.3836,
      "step": 1300
    },
    {
      "epoch": 0.014292422974049044,
      "grad_norm": 19.552091598510742,
      "learning_rate": 1.9938983235729236e-05,
      "loss": 3.3702,
      "step": 1400
    },
    {
      "epoch": 0.015313310329338262,
      "grad_norm": 25.979093551635742,
      "learning_rate": 1.993216572016826e-05,
      "loss": 3.3168,
      "step": 1500
    },
    {
      "epoch": 0.016334197684627478,
      "grad_norm": 17.52277946472168,
      "learning_rate": 1.9925348204607278e-05,
      "loss": 3.3171,
      "step": 1600
    },
    {
      "epoch": 0.017355085039916694,
      "grad_norm": 19.898834228515625,
      "learning_rate": 1.99185306890463e-05,
      "loss": 3.366,
      "step": 1700
    },
    {
      "epoch": 0.018375972395205914,
      "grad_norm": 20.23374366760254,
      "learning_rate": 1.991171317348532e-05,
      "loss": 3.2408,
      "step": 1800
    },
    {
      "epoch": 0.01939685975049513,
      "grad_norm": 22.956592559814453,
      "learning_rate": 1.990489565792434e-05,
      "loss": 3.2506,
      "step": 1900
    },
    {
      "epoch": 0.020417747105784347,
      "grad_norm": 19.571277618408203,
      "learning_rate": 1.9898078142363363e-05,
      "loss": 3.423,
      "step": 2000
    },
    {
      "epoch": 0.021438634461073566,
      "grad_norm": 19.149085998535156,
      "learning_rate": 1.9891260626802383e-05,
      "loss": 3.3342,
      "step": 2100
    },
    {
      "epoch": 0.022459521816362783,
      "grad_norm": 19.273576736450195,
      "learning_rate": 1.9884443111241403e-05,
      "loss": 3.3234,
      "step": 2200
    },
    {
      "epoch": 0.023480409171652,
      "grad_norm": 19.92934799194336,
      "learning_rate": 1.9877625595680422e-05,
      "loss": 3.2753,
      "step": 2300
    },
    {
      "epoch": 0.02450129652694122,
      "grad_norm": 14.930896759033203,
      "learning_rate": 1.9870808080119445e-05,
      "loss": 3.2922,
      "step": 2400
    },
    {
      "epoch": 0.025522183882230435,
      "grad_norm": 21.040504455566406,
      "learning_rate": 1.9863990564558465e-05,
      "loss": 3.3308,
      "step": 2500
    },
    {
      "epoch": 0.02654307123751965,
      "grad_norm": 20.465604782104492,
      "learning_rate": 1.9857173048997488e-05,
      "loss": 3.1884,
      "step": 2600
    },
    {
      "epoch": 0.02756395859280887,
      "grad_norm": 21.749723434448242,
      "learning_rate": 1.9850355533436508e-05,
      "loss": 3.2854,
      "step": 2700
    },
    {
      "epoch": 0.028584845948098087,
      "grad_norm": 17.444459915161133,
      "learning_rate": 1.9843538017875527e-05,
      "loss": 3.2628,
      "step": 2800
    },
    {
      "epoch": 0.029605733303387304,
      "grad_norm": 15.505945205688477,
      "learning_rate": 1.9836720502314547e-05,
      "loss": 3.2697,
      "step": 2900
    },
    {
      "epoch": 0.030626620658676523,
      "grad_norm": 25.384159088134766,
      "learning_rate": 1.982990298675357e-05,
      "loss": 3.2207,
      "step": 3000
    },
    {
      "epoch": 0.031647508013965736,
      "grad_norm": 15.281416893005371,
      "learning_rate": 1.982308547119259e-05,
      "loss": 3.3064,
      "step": 3100
    },
    {
      "epoch": 0.032668395369254956,
      "grad_norm": 20.290468215942383,
      "learning_rate": 1.981626795563161e-05,
      "loss": 3.2622,
      "step": 3200
    },
    {
      "epoch": 0.033689282724544176,
      "grad_norm": 17.4942626953125,
      "learning_rate": 1.980945044007063e-05,
      "loss": 3.2472,
      "step": 3300
    },
    {
      "epoch": 0.03471017007983339,
      "grad_norm": 16.68461036682129,
      "learning_rate": 1.980263292450965e-05,
      "loss": 3.2729,
      "step": 3400
    },
    {
      "epoch": 0.03573105743512261,
      "grad_norm": 17.235265731811523,
      "learning_rate": 1.9795815408948675e-05,
      "loss": 3.2857,
      "step": 3500
    },
    {
      "epoch": 0.03675194479041183,
      "grad_norm": 19.872011184692383,
      "learning_rate": 1.9788997893387694e-05,
      "loss": 3.2121,
      "step": 3600
    },
    {
      "epoch": 0.03777283214570104,
      "grad_norm": 35.1069450378418,
      "learning_rate": 1.9782180377826714e-05,
      "loss": 3.2362,
      "step": 3700
    },
    {
      "epoch": 0.03879371950099026,
      "grad_norm": 18.761783599853516,
      "learning_rate": 1.9775362862265733e-05,
      "loss": 3.2066,
      "step": 3800
    },
    {
      "epoch": 0.03981460685627948,
      "grad_norm": 24.348430633544922,
      "learning_rate": 1.9768545346704756e-05,
      "loss": 3.246,
      "step": 3900
    },
    {
      "epoch": 0.04083549421156869,
      "grad_norm": 16.318553924560547,
      "learning_rate": 1.9761727831143776e-05,
      "loss": 3.2058,
      "step": 4000
    },
    {
      "epoch": 0.04185638156685791,
      "grad_norm": 17.457721710205078,
      "learning_rate": 1.9754910315582796e-05,
      "loss": 3.2577,
      "step": 4100
    },
    {
      "epoch": 0.04287726892214713,
      "grad_norm": 19.013221740722656,
      "learning_rate": 1.974809280002182e-05,
      "loss": 3.1869,
      "step": 4200
    },
    {
      "epoch": 0.043898156277436345,
      "grad_norm": 16.90885353088379,
      "learning_rate": 1.974127528446084e-05,
      "loss": 3.1757,
      "step": 4300
    },
    {
      "epoch": 0.044919043632725565,
      "grad_norm": 15.871418952941895,
      "learning_rate": 1.9734457768899858e-05,
      "loss": 3.2558,
      "step": 4400
    },
    {
      "epoch": 0.045939930988014785,
      "grad_norm": 17.085145950317383,
      "learning_rate": 1.972764025333888e-05,
      "loss": 3.3102,
      "step": 4500
    },
    {
      "epoch": 0.046960818343304,
      "grad_norm": 17.746604919433594,
      "learning_rate": 1.97208227377779e-05,
      "loss": 3.2364,
      "step": 4600
    },
    {
      "epoch": 0.04798170569859322,
      "grad_norm": 15.002768516540527,
      "learning_rate": 1.971400522221692e-05,
      "loss": 3.1773,
      "step": 4700
    },
    {
      "epoch": 0.04900259305388244,
      "grad_norm": 15.875140190124512,
      "learning_rate": 1.970718770665594e-05,
      "loss": 3.1425,
      "step": 4800
    },
    {
      "epoch": 0.05002348040917165,
      "grad_norm": 27.80532455444336,
      "learning_rate": 1.9700370191094963e-05,
      "loss": 3.1313,
      "step": 4900
    },
    {
      "epoch": 0.05104436776446087,
      "grad_norm": 18.03184700012207,
      "learning_rate": 1.9693552675533986e-05,
      "loss": 3.197,
      "step": 5000
    },
    {
      "epoch": 0.05206525511975009,
      "grad_norm": 19.918672561645508,
      "learning_rate": 1.9686735159973005e-05,
      "loss": 3.251,
      "step": 5100
    },
    {
      "epoch": 0.0530861424750393,
      "grad_norm": 21.679553985595703,
      "learning_rate": 1.9679917644412025e-05,
      "loss": 3.1676,
      "step": 5200
    },
    {
      "epoch": 0.05410702983032852,
      "grad_norm": 20.136335372924805,
      "learning_rate": 1.9673100128851045e-05,
      "loss": 3.2181,
      "step": 5300
    },
    {
      "epoch": 0.05512791718561774,
      "grad_norm": 20.142976760864258,
      "learning_rate": 1.9666282613290068e-05,
      "loss": 3.1424,
      "step": 5400
    },
    {
      "epoch": 0.056148804540906955,
      "grad_norm": 18.273956298828125,
      "learning_rate": 1.9659465097729087e-05,
      "loss": 3.2242,
      "step": 5500
    },
    {
      "epoch": 0.057169691896196174,
      "grad_norm": 21.959325790405273,
      "learning_rate": 1.9652647582168107e-05,
      "loss": 3.1998,
      "step": 5600
    },
    {
      "epoch": 0.058190579251485394,
      "grad_norm": 26.251712799072266,
      "learning_rate": 1.9645898241762738e-05,
      "loss": 3.2082,
      "step": 5700
    },
    {
      "epoch": 0.05921146660677461,
      "grad_norm": 16.196069717407227,
      "learning_rate": 1.963914890135737e-05,
      "loss": 3.3153,
      "step": 5800
    },
    {
      "epoch": 0.06023235396206383,
      "grad_norm": 20.94748878479004,
      "learning_rate": 1.9632331385796392e-05,
      "loss": 3.2247,
      "step": 5900
    },
    {
      "epoch": 0.06125324131735305,
      "grad_norm": 18.39457893371582,
      "learning_rate": 1.9625513870235412e-05,
      "loss": 3.2641,
      "step": 6000
    },
    {
      "epoch": 0.06227412867264226,
      "grad_norm": 21.29412269592285,
      "learning_rate": 1.961869635467443e-05,
      "loss": 3.1337,
      "step": 6100
    },
    {
      "epoch": 0.06329501602793147,
      "grad_norm": 16.29652976989746,
      "learning_rate": 1.961187883911345e-05,
      "loss": 3.1463,
      "step": 6200
    },
    {
      "epoch": 0.0643159033832207,
      "grad_norm": 15.26712703704834,
      "learning_rate": 1.960506132355247e-05,
      "loss": 3.1176,
      "step": 6300
    },
    {
      "epoch": 0.06533679073850991,
      "grad_norm": 20.644174575805664,
      "learning_rate": 1.9598243807991494e-05,
      "loss": 3.287,
      "step": 6400
    },
    {
      "epoch": 0.06635767809379912,
      "grad_norm": 20.406667709350586,
      "learning_rate": 1.9591426292430513e-05,
      "loss": 3.1436,
      "step": 6500
    },
    {
      "epoch": 0.06737856544908835,
      "grad_norm": 19.331411361694336,
      "learning_rate": 1.9584608776869536e-05,
      "loss": 3.1735,
      "step": 6600
    },
    {
      "epoch": 0.06839945280437756,
      "grad_norm": 16.944721221923828,
      "learning_rate": 1.9577791261308556e-05,
      "loss": 3.1825,
      "step": 6700
    },
    {
      "epoch": 0.06942034015966678,
      "grad_norm": 17.414587020874023,
      "learning_rate": 1.9570973745747575e-05,
      "loss": 3.2391,
      "step": 6800
    },
    {
      "epoch": 0.070441227514956,
      "grad_norm": 16.0064697265625,
      "learning_rate": 1.95641562301866e-05,
      "loss": 3.1006,
      "step": 6900
    },
    {
      "epoch": 0.07146211487024522,
      "grad_norm": 17.016300201416016,
      "learning_rate": 1.9557338714625618e-05,
      "loss": 3.2554,
      "step": 7000
    },
    {
      "epoch": 0.07248300222553443,
      "grad_norm": 16.429372787475586,
      "learning_rate": 1.9550521199064638e-05,
      "loss": 3.1406,
      "step": 7100
    },
    {
      "epoch": 0.07350388958082366,
      "grad_norm": 19.91683578491211,
      "learning_rate": 1.9543703683503657e-05,
      "loss": 3.183,
      "step": 7200
    },
    {
      "epoch": 0.07452477693611287,
      "grad_norm": 16.960439682006836,
      "learning_rate": 1.953688616794268e-05,
      "loss": 3.1605,
      "step": 7300
    },
    {
      "epoch": 0.07554566429140208,
      "grad_norm": 22.325265884399414,
      "learning_rate": 1.95300686523817e-05,
      "loss": 3.1485,
      "step": 7400
    },
    {
      "epoch": 0.07656655164669131,
      "grad_norm": 17.17003059387207,
      "learning_rate": 1.9523251136820723e-05,
      "loss": 3.1772,
      "step": 7500
    },
    {
      "epoch": 0.07758743900198052,
      "grad_norm": 18.39481544494629,
      "learning_rate": 1.9516433621259743e-05,
      "loss": 3.1571,
      "step": 7600
    },
    {
      "epoch": 0.07860832635726973,
      "grad_norm": 22.246004104614258,
      "learning_rate": 1.9509616105698762e-05,
      "loss": 3.1883,
      "step": 7700
    },
    {
      "epoch": 0.07962921371255896,
      "grad_norm": 14.290055274963379,
      "learning_rate": 1.9502798590137785e-05,
      "loss": 3.2193,
      "step": 7800
    },
    {
      "epoch": 0.08065010106784817,
      "grad_norm": 22.83107566833496,
      "learning_rate": 1.9495981074576805e-05,
      "loss": 3.0817,
      "step": 7900
    },
    {
      "epoch": 0.08167098842313739,
      "grad_norm": 18.670061111450195,
      "learning_rate": 1.9489163559015824e-05,
      "loss": 3.1655,
      "step": 8000
    },
    {
      "epoch": 0.08269187577842661,
      "grad_norm": 18.005508422851562,
      "learning_rate": 1.9482346043454844e-05,
      "loss": 3.1584,
      "step": 8100
    },
    {
      "epoch": 0.08371276313371583,
      "grad_norm": 17.345035552978516,
      "learning_rate": 1.9475528527893867e-05,
      "loss": 3.0696,
      "step": 8200
    },
    {
      "epoch": 0.08473365048900504,
      "grad_norm": 16.344457626342773,
      "learning_rate": 1.9468711012332887e-05,
      "loss": 3.0464,
      "step": 8300
    },
    {
      "epoch": 0.08575453784429427,
      "grad_norm": 21.821758270263672,
      "learning_rate": 1.946189349677191e-05,
      "loss": 3.138,
      "step": 8400
    },
    {
      "epoch": 0.08677542519958348,
      "grad_norm": 20.137598037719727,
      "learning_rate": 1.945507598121093e-05,
      "loss": 3.0861,
      "step": 8500
    },
    {
      "epoch": 0.08779631255487269,
      "grad_norm": 18.126787185668945,
      "learning_rate": 1.944825846564995e-05,
      "loss": 3.1039,
      "step": 8600
    },
    {
      "epoch": 0.08881719991016192,
      "grad_norm": 15.58698558807373,
      "learning_rate": 1.944144095008897e-05,
      "loss": 3.0522,
      "step": 8700
    },
    {
      "epoch": 0.08983808726545113,
      "grad_norm": 15.401494979858398,
      "learning_rate": 1.943462343452799e-05,
      "loss": 3.1062,
      "step": 8800
    },
    {
      "epoch": 0.09085897462074034,
      "grad_norm": 20.035945892333984,
      "learning_rate": 1.942780591896701e-05,
      "loss": 3.271,
      "step": 8900
    },
    {
      "epoch": 0.09187986197602957,
      "grad_norm": 18.845741271972656,
      "learning_rate": 1.942098840340603e-05,
      "loss": 3.1585,
      "step": 9000
    },
    {
      "epoch": 0.09290074933131878,
      "grad_norm": 14.927077293395996,
      "learning_rate": 1.9414170887845054e-05,
      "loss": 3.0455,
      "step": 9100
    },
    {
      "epoch": 0.093921636686608,
      "grad_norm": 14.114897727966309,
      "learning_rate": 1.9407353372284073e-05,
      "loss": 3.1454,
      "step": 9200
    },
    {
      "epoch": 0.09494252404189722,
      "grad_norm": 14.888904571533203,
      "learning_rate": 1.9400535856723096e-05,
      "loss": 3.2089,
      "step": 9300
    },
    {
      "epoch": 0.09596341139718643,
      "grad_norm": 16.637981414794922,
      "learning_rate": 1.9393718341162116e-05,
      "loss": 3.1407,
      "step": 9400
    },
    {
      "epoch": 0.09698429875247565,
      "grad_norm": 17.826770782470703,
      "learning_rate": 1.9386900825601136e-05,
      "loss": 3.0486,
      "step": 9500
    },
    {
      "epoch": 0.09800518610776487,
      "grad_norm": 18.37482452392578,
      "learning_rate": 1.9380083310040155e-05,
      "loss": 3.1564,
      "step": 9600
    },
    {
      "epoch": 0.09902607346305409,
      "grad_norm": 22.098665237426758,
      "learning_rate": 1.9373265794479178e-05,
      "loss": 3.0655,
      "step": 9700
    },
    {
      "epoch": 0.1000469608183433,
      "grad_norm": 17.584487915039062,
      "learning_rate": 1.9366448278918198e-05,
      "loss": 3.1117,
      "step": 9800
    },
    {
      "epoch": 0.10106784817363253,
      "grad_norm": 20.4021053314209,
      "learning_rate": 1.935963076335722e-05,
      "loss": 3.1782,
      "step": 9900
    },
    {
      "epoch": 0.10208873552892174,
      "grad_norm": 18.220762252807617,
      "learning_rate": 1.935281324779624e-05,
      "loss": 3.1349,
      "step": 10000
    },
    {
      "epoch": 0.10310962288421095,
      "grad_norm": 17.537107467651367,
      "learning_rate": 1.934599573223526e-05,
      "loss": 3.132,
      "step": 10100
    },
    {
      "epoch": 0.10413051023950018,
      "grad_norm": 17.582836151123047,
      "learning_rate": 1.933917821667428e-05,
      "loss": 3.0931,
      "step": 10200
    },
    {
      "epoch": 0.10515139759478939,
      "grad_norm": 16.102001190185547,
      "learning_rate": 1.9332360701113303e-05,
      "loss": 3.1852,
      "step": 10300
    },
    {
      "epoch": 0.1061722849500786,
      "grad_norm": 16.609540939331055,
      "learning_rate": 1.9325543185552322e-05,
      "loss": 3.1332,
      "step": 10400
    },
    {
      "epoch": 0.10719317230536783,
      "grad_norm": 19.80914878845215,
      "learning_rate": 1.9318725669991342e-05,
      "loss": 3.0919,
      "step": 10500
    },
    {
      "epoch": 0.10821405966065704,
      "grad_norm": 13.643003463745117,
      "learning_rate": 1.931190815443036e-05,
      "loss": 3.0214,
      "step": 10600
    },
    {
      "epoch": 0.10923494701594626,
      "grad_norm": 15.278874397277832,
      "learning_rate": 1.9305090638869385e-05,
      "loss": 3.1608,
      "step": 10700
    },
    {
      "epoch": 0.11025583437123548,
      "grad_norm": 21.15119743347168,
      "learning_rate": 1.9298273123308408e-05,
      "loss": 3.0752,
      "step": 10800
    },
    {
      "epoch": 0.1112767217265247,
      "grad_norm": 16.245107650756836,
      "learning_rate": 1.9291455607747427e-05,
      "loss": 3.1779,
      "step": 10900
    },
    {
      "epoch": 0.11229760908181391,
      "grad_norm": 22.533924102783203,
      "learning_rate": 1.9284638092186447e-05,
      "loss": 3.057,
      "step": 11000
    },
    {
      "epoch": 0.11331849643710314,
      "grad_norm": 18.454425811767578,
      "learning_rate": 1.9277820576625466e-05,
      "loss": 3.112,
      "step": 11100
    },
    {
      "epoch": 0.11433938379239235,
      "grad_norm": 17.18951988220215,
      "learning_rate": 1.927100306106449e-05,
      "loss": 3.2163,
      "step": 11200
    },
    {
      "epoch": 0.11536027114768156,
      "grad_norm": 15.82232666015625,
      "learning_rate": 1.926418554550351e-05,
      "loss": 3.1821,
      "step": 11300
    },
    {
      "epoch": 0.11638115850297079,
      "grad_norm": 27.26604652404785,
      "learning_rate": 1.925736802994253e-05,
      "loss": 3.0547,
      "step": 11400
    },
    {
      "epoch": 0.11740204585826,
      "grad_norm": 15.59089183807373,
      "learning_rate": 1.925055051438155e-05,
      "loss": 3.1172,
      "step": 11500
    },
    {
      "epoch": 0.11842293321354921,
      "grad_norm": 18.448986053466797,
      "learning_rate": 1.924373299882057e-05,
      "loss": 3.1611,
      "step": 11600
    },
    {
      "epoch": 0.11944382056883844,
      "grad_norm": 16.35788345336914,
      "learning_rate": 1.9236915483259594e-05,
      "loss": 3.0433,
      "step": 11700
    },
    {
      "epoch": 0.12046470792412765,
      "grad_norm": 18.8714656829834,
      "learning_rate": 1.9230097967698614e-05,
      "loss": 3.1047,
      "step": 11800
    },
    {
      "epoch": 0.12148559527941687,
      "grad_norm": 18.911434173583984,
      "learning_rate": 1.9223348627293245e-05,
      "loss": 3.0833,
      "step": 11900
    },
    {
      "epoch": 0.1225064826347061,
      "grad_norm": 16.424964904785156,
      "learning_rate": 1.9216531111732265e-05,
      "loss": 3.0853,
      "step": 12000
    },
    {
      "epoch": 0.1235273699899953,
      "grad_norm": 26.26258087158203,
      "learning_rate": 1.9209713596171284e-05,
      "loss": 3.0891,
      "step": 12100
    },
    {
      "epoch": 0.12454825734528452,
      "grad_norm": 16.563209533691406,
      "learning_rate": 1.9202896080610304e-05,
      "loss": 3.0946,
      "step": 12200
    },
    {
      "epoch": 0.12556914470057373,
      "grad_norm": 16.535858154296875,
      "learning_rate": 1.9196078565049327e-05,
      "loss": 3.0812,
      "step": 12300
    },
    {
      "epoch": 0.12659003205586294,
      "grad_norm": 15.555886268615723,
      "learning_rate": 1.9189261049488347e-05,
      "loss": 3.0906,
      "step": 12400
    },
    {
      "epoch": 0.12761091941115218,
      "grad_norm": 13.982954025268555,
      "learning_rate": 1.918244353392737e-05,
      "loss": 3.1354,
      "step": 12500
    },
    {
      "epoch": 0.1286318067664414,
      "grad_norm": 17.35477638244629,
      "learning_rate": 1.917562601836639e-05,
      "loss": 3.0082,
      "step": 12600
    },
    {
      "epoch": 0.1296526941217306,
      "grad_norm": 19.456707000732422,
      "learning_rate": 1.916880850280541e-05,
      "loss": 3.1215,
      "step": 12700
    },
    {
      "epoch": 0.13067358147701982,
      "grad_norm": 18.675302505493164,
      "learning_rate": 1.916199098724443e-05,
      "loss": 3.0543,
      "step": 12800
    },
    {
      "epoch": 0.13169446883230904,
      "grad_norm": 15.329839706420898,
      "learning_rate": 1.915517347168345e-05,
      "loss": 3.0847,
      "step": 12900
    },
    {
      "epoch": 0.13271535618759825,
      "grad_norm": 17.928220748901367,
      "learning_rate": 1.914835595612247e-05,
      "loss": 3.088,
      "step": 13000
    },
    {
      "epoch": 0.1337362435428875,
      "grad_norm": 18.768226623535156,
      "learning_rate": 1.914153844056149e-05,
      "loss": 3.1663,
      "step": 13100
    },
    {
      "epoch": 0.1347571308981767,
      "grad_norm": 13.849420547485352,
      "learning_rate": 1.9134720925000514e-05,
      "loss": 3.1109,
      "step": 13200
    },
    {
      "epoch": 0.13577801825346592,
      "grad_norm": 16.915847778320312,
      "learning_rate": 1.9127903409439533e-05,
      "loss": 3.0967,
      "step": 13300
    },
    {
      "epoch": 0.13679890560875513,
      "grad_norm": 19.937868118286133,
      "learning_rate": 1.9121085893878556e-05,
      "loss": 3.0822,
      "step": 13400
    },
    {
      "epoch": 0.13781979296404434,
      "grad_norm": 16.599512100219727,
      "learning_rate": 1.9114268378317576e-05,
      "loss": 3.0887,
      "step": 13500
    },
    {
      "epoch": 0.13884068031933355,
      "grad_norm": 12.571476936340332,
      "learning_rate": 1.9107450862756596e-05,
      "loss": 3.0686,
      "step": 13600
    },
    {
      "epoch": 0.1398615676746228,
      "grad_norm": 17.199520111083984,
      "learning_rate": 1.9100633347195615e-05,
      "loss": 3.1513,
      "step": 13700
    },
    {
      "epoch": 0.140882455029912,
      "grad_norm": 16.93634796142578,
      "learning_rate": 1.9093815831634638e-05,
      "loss": 3.0154,
      "step": 13800
    },
    {
      "epoch": 0.14190334238520122,
      "grad_norm": 17.69172477722168,
      "learning_rate": 1.9086998316073658e-05,
      "loss": 3.0441,
      "step": 13900
    },
    {
      "epoch": 0.14292422974049043,
      "grad_norm": 18.598230361938477,
      "learning_rate": 1.9080180800512677e-05,
      "loss": 3.1362,
      "step": 14000
    },
    {
      "epoch": 0.14394511709577965,
      "grad_norm": 14.292088508605957,
      "learning_rate": 1.90733632849517e-05,
      "loss": 3.1391,
      "step": 14100
    },
    {
      "epoch": 0.14496600445106886,
      "grad_norm": 22.218467712402344,
      "learning_rate": 1.906661394454633e-05,
      "loss": 3.0559,
      "step": 14200
    },
    {
      "epoch": 0.1459868918063581,
      "grad_norm": 15.93030071258545,
      "learning_rate": 1.905979642898535e-05,
      "loss": 3.1705,
      "step": 14300
    },
    {
      "epoch": 0.1470077791616473,
      "grad_norm": 15.129323959350586,
      "learning_rate": 1.905297891342437e-05,
      "loss": 3.0795,
      "step": 14400
    },
    {
      "epoch": 0.14802866651693652,
      "grad_norm": 17.74407958984375,
      "learning_rate": 1.904616139786339e-05,
      "loss": 3.0448,
      "step": 14500
    },
    {
      "epoch": 0.14904955387222574,
      "grad_norm": 19.65030288696289,
      "learning_rate": 1.9039343882302413e-05,
      "loss": 3.0568,
      "step": 14600
    },
    {
      "epoch": 0.15007044122751495,
      "grad_norm": 14.105206489562988,
      "learning_rate": 1.9032526366741433e-05,
      "loss": 3.0666,
      "step": 14700
    },
    {
      "epoch": 0.15109132858280416,
      "grad_norm": 20.982851028442383,
      "learning_rate": 1.9025708851180453e-05,
      "loss": 3.1132,
      "step": 14800
    },
    {
      "epoch": 0.1521122159380934,
      "grad_norm": 15.500088691711426,
      "learning_rate": 1.9018891335619476e-05,
      "loss": 3.0817,
      "step": 14900
    },
    {
      "epoch": 0.15313310329338262,
      "grad_norm": 18.31558609008789,
      "learning_rate": 1.9012073820058495e-05,
      "loss": 2.9991,
      "step": 15000
    },
    {
      "epoch": 0.15415399064867183,
      "grad_norm": 18.739788055419922,
      "learning_rate": 1.9005256304497518e-05,
      "loss": 3.1275,
      "step": 15100
    },
    {
      "epoch": 0.15517487800396104,
      "grad_norm": 16.954580307006836,
      "learning_rate": 1.8998438788936538e-05,
      "loss": 3.0976,
      "step": 15200
    },
    {
      "epoch": 0.15619576535925025,
      "grad_norm": 18.19746971130371,
      "learning_rate": 1.8991621273375557e-05,
      "loss": 3.1143,
      "step": 15300
    },
    {
      "epoch": 0.15721665271453947,
      "grad_norm": 15.086838722229004,
      "learning_rate": 1.8984803757814577e-05,
      "loss": 3.0052,
      "step": 15400
    },
    {
      "epoch": 0.1582375400698287,
      "grad_norm": 15.476760864257812,
      "learning_rate": 1.89779862422536e-05,
      "loss": 3.0764,
      "step": 15500
    },
    {
      "epoch": 0.15925842742511792,
      "grad_norm": 17.931182861328125,
      "learning_rate": 1.897116872669262e-05,
      "loss": 3.0301,
      "step": 15600
    },
    {
      "epoch": 0.16027931478040713,
      "grad_norm": 17.67885971069336,
      "learning_rate": 1.8964351211131643e-05,
      "loss": 3.0348,
      "step": 15700
    },
    {
      "epoch": 0.16130020213569635,
      "grad_norm": 15.209677696228027,
      "learning_rate": 1.8957601870726274e-05,
      "loss": 3.0211,
      "step": 15800
    },
    {
      "epoch": 0.16232108949098556,
      "grad_norm": 17.056642532348633,
      "learning_rate": 1.8950784355165293e-05,
      "loss": 3.0639,
      "step": 15900
    },
    {
      "epoch": 0.16334197684627477,
      "grad_norm": 17.098215103149414,
      "learning_rate": 1.8943966839604313e-05,
      "loss": 3.0619,
      "step": 16000
    },
    {
      "epoch": 0.164362864201564,
      "grad_norm": 16.316749572753906,
      "learning_rate": 1.8937149324043333e-05,
      "loss": 3.112,
      "step": 16100
    },
    {
      "epoch": 0.16538375155685323,
      "grad_norm": 17.98507308959961,
      "learning_rate": 1.8930331808482356e-05,
      "loss": 2.9874,
      "step": 16200
    },
    {
      "epoch": 0.16640463891214244,
      "grad_norm": 24.110698699951172,
      "learning_rate": 1.8923514292921375e-05,
      "loss": 3.0873,
      "step": 16300
    },
    {
      "epoch": 0.16742552626743165,
      "grad_norm": 20.623502731323242,
      "learning_rate": 1.8916696777360395e-05,
      "loss": 2.9531,
      "step": 16400
    },
    {
      "epoch": 0.16844641362272086,
      "grad_norm": 19.19286346435547,
      "learning_rate": 1.8909879261799418e-05,
      "loss": 3.1245,
      "step": 16500
    },
    {
      "epoch": 0.16946730097801008,
      "grad_norm": 22.014699935913086,
      "learning_rate": 1.8903061746238437e-05,
      "loss": 3.0646,
      "step": 16600
    },
    {
      "epoch": 0.17048818833329932,
      "grad_norm": 20.281566619873047,
      "learning_rate": 1.8896244230677457e-05,
      "loss": 3.0804,
      "step": 16700
    },
    {
      "epoch": 0.17150907568858853,
      "grad_norm": 17.307432174682617,
      "learning_rate": 1.888942671511648e-05,
      "loss": 3.0722,
      "step": 16800
    },
    {
      "epoch": 0.17252996304387774,
      "grad_norm": 18.47530746459961,
      "learning_rate": 1.88826091995555e-05,
      "loss": 3.05,
      "step": 16900
    },
    {
      "epoch": 0.17355085039916696,
      "grad_norm": 19.196556091308594,
      "learning_rate": 1.887579168399452e-05,
      "loss": 3.1206,
      "step": 17000
    },
    {
      "epoch": 0.17457173775445617,
      "grad_norm": 16.85285186767578,
      "learning_rate": 1.886897416843354e-05,
      "loss": 3.0539,
      "step": 17100
    },
    {
      "epoch": 0.17559262510974538,
      "grad_norm": 16.319904327392578,
      "learning_rate": 1.8862156652872562e-05,
      "loss": 2.9806,
      "step": 17200
    },
    {
      "epoch": 0.1766135124650346,
      "grad_norm": 17.900962829589844,
      "learning_rate": 1.8855339137311585e-05,
      "loss": 3.0022,
      "step": 17300
    },
    {
      "epoch": 0.17763439982032384,
      "grad_norm": 16.826671600341797,
      "learning_rate": 1.8848521621750605e-05,
      "loss": 3.1707,
      "step": 17400
    },
    {
      "epoch": 0.17865528717561305,
      "grad_norm": 19.85601234436035,
      "learning_rate": 1.8841704106189624e-05,
      "loss": 2.9615,
      "step": 17500
    },
    {
      "epoch": 0.17967617453090226,
      "grad_norm": 21.323955535888672,
      "learning_rate": 1.8834886590628644e-05,
      "loss": 3.0393,
      "step": 17600
    },
    {
      "epoch": 0.18069706188619147,
      "grad_norm": 18.965791702270508,
      "learning_rate": 1.8828069075067667e-05,
      "loss": 3.0409,
      "step": 17700
    },
    {
      "epoch": 0.1817179492414807,
      "grad_norm": 14.302221298217773,
      "learning_rate": 1.8821251559506686e-05,
      "loss": 2.9522,
      "step": 17800
    },
    {
      "epoch": 0.1827388365967699,
      "grad_norm": 20.813365936279297,
      "learning_rate": 1.8814434043945706e-05,
      "loss": 2.9954,
      "step": 17900
    },
    {
      "epoch": 0.18375972395205914,
      "grad_norm": 14.607134819030762,
      "learning_rate": 1.8807616528384726e-05,
      "loss": 3.0344,
      "step": 18000
    },
    {
      "epoch": 0.18478061130734835,
      "grad_norm": 15.990105628967285,
      "learning_rate": 1.880079901282375e-05,
      "loss": 3.1307,
      "step": 18100
    },
    {
      "epoch": 0.18580149866263757,
      "grad_norm": 18.424461364746094,
      "learning_rate": 1.879398149726277e-05,
      "loss": 3.0673,
      "step": 18200
    },
    {
      "epoch": 0.18682238601792678,
      "grad_norm": 17.750295639038086,
      "learning_rate": 1.878716398170179e-05,
      "loss": 2.9959,
      "step": 18300
    },
    {
      "epoch": 0.187843273373216,
      "grad_norm": 18.706491470336914,
      "learning_rate": 1.878034646614081e-05,
      "loss": 2.9954,
      "step": 18400
    },
    {
      "epoch": 0.1888641607285052,
      "grad_norm": 17.646059036254883,
      "learning_rate": 1.877352895057983e-05,
      "loss": 2.9962,
      "step": 18500
    },
    {
      "epoch": 0.18988504808379444,
      "grad_norm": 23.66650390625,
      "learning_rate": 1.876671143501885e-05,
      "loss": 3.0043,
      "step": 18600
    },
    {
      "epoch": 0.19090593543908366,
      "grad_norm": 18.994260787963867,
      "learning_rate": 1.8759893919457873e-05,
      "loss": 2.9576,
      "step": 18700
    },
    {
      "epoch": 0.19192682279437287,
      "grad_norm": 15.988335609436035,
      "learning_rate": 1.8753076403896893e-05,
      "loss": 2.9979,
      "step": 18800
    },
    {
      "epoch": 0.19294771014966208,
      "grad_norm": 15.045005798339844,
      "learning_rate": 1.8746258888335912e-05,
      "loss": 2.9999,
      "step": 18900
    },
    {
      "epoch": 0.1939685975049513,
      "grad_norm": 16.286304473876953,
      "learning_rate": 1.8739441372774935e-05,
      "loss": 2.9512,
      "step": 19000
    },
    {
      "epoch": 0.1949894848602405,
      "grad_norm": 15.366350173950195,
      "learning_rate": 1.8732623857213955e-05,
      "loss": 3.0262,
      "step": 19100
    },
    {
      "epoch": 0.19601037221552975,
      "grad_norm": 16.133901596069336,
      "learning_rate": 1.8725806341652978e-05,
      "loss": 2.929,
      "step": 19200
    },
    {
      "epoch": 0.19703125957081896,
      "grad_norm": 15.439696311950684,
      "learning_rate": 1.8718988826091998e-05,
      "loss": 3.0368,
      "step": 19300
    },
    {
      "epoch": 0.19805214692610817,
      "grad_norm": 17.186433792114258,
      "learning_rate": 1.8712171310531017e-05,
      "loss": 3.0655,
      "step": 19400
    },
    {
      "epoch": 0.1990730342813974,
      "grad_norm": 16.161766052246094,
      "learning_rate": 1.8705353794970037e-05,
      "loss": 3.0519,
      "step": 19500
    },
    {
      "epoch": 0.2000939216366866,
      "grad_norm": 27.294355392456055,
      "learning_rate": 1.869853627940906e-05,
      "loss": 3.0362,
      "step": 19600
    },
    {
      "epoch": 0.2011148089919758,
      "grad_norm": 16.476587295532227,
      "learning_rate": 1.869171876384808e-05,
      "loss": 3.0499,
      "step": 19700
    },
    {
      "epoch": 0.20213569634726505,
      "grad_norm": 16.246706008911133,
      "learning_rate": 1.868496942344271e-05,
      "loss": 2.9918,
      "step": 19800
    },
    {
      "epoch": 0.20315658370255427,
      "grad_norm": 18.623653411865234,
      "learning_rate": 1.867815190788173e-05,
      "loss": 3.1038,
      "step": 19900
    },
    {
      "epoch": 0.20417747105784348,
      "grad_norm": 22.20587921142578,
      "learning_rate": 1.8671334392320753e-05,
      "loss": 3.0331,
      "step": 20000
    },
    {
      "epoch": 0.2051983584131327,
      "grad_norm": 16.35923957824707,
      "learning_rate": 1.8664516876759773e-05,
      "loss": 2.9015,
      "step": 20100
    },
    {
      "epoch": 0.2062192457684219,
      "grad_norm": 17.23679542541504,
      "learning_rate": 1.8657699361198793e-05,
      "loss": 3.0755,
      "step": 20200
    },
    {
      "epoch": 0.20724013312371112,
      "grad_norm": 17.421131134033203,
      "learning_rate": 1.8650881845637812e-05,
      "loss": 2.987,
      "step": 20300
    },
    {
      "epoch": 0.20826102047900036,
      "grad_norm": 18.855236053466797,
      "learning_rate": 1.8644064330076835e-05,
      "loss": 2.9357,
      "step": 20400
    },
    {
      "epoch": 0.20928190783428957,
      "grad_norm": 14.540058135986328,
      "learning_rate": 1.8637246814515855e-05,
      "loss": 3.0835,
      "step": 20500
    },
    {
      "epoch": 0.21030279518957878,
      "grad_norm": 14.123910903930664,
      "learning_rate": 1.8630429298954878e-05,
      "loss": 2.9843,
      "step": 20600
    },
    {
      "epoch": 0.211323682544868,
      "grad_norm": 18.542285919189453,
      "learning_rate": 1.8623611783393897e-05,
      "loss": 2.9781,
      "step": 20700
    },
    {
      "epoch": 0.2123445699001572,
      "grad_norm": 15.831439971923828,
      "learning_rate": 1.8616794267832917e-05,
      "loss": 2.9855,
      "step": 20800
    },
    {
      "epoch": 0.21336545725544642,
      "grad_norm": 15.901843070983887,
      "learning_rate": 1.860997675227194e-05,
      "loss": 2.9244,
      "step": 20900
    },
    {
      "epoch": 0.21438634461073566,
      "grad_norm": 11.12134075164795,
      "learning_rate": 1.860315923671096e-05,
      "loss": 3.0137,
      "step": 21000
    },
    {
      "epoch": 0.21540723196602488,
      "grad_norm": 13.862783432006836,
      "learning_rate": 1.859634172114998e-05,
      "loss": 2.9888,
      "step": 21100
    },
    {
      "epoch": 0.2164281193213141,
      "grad_norm": 16.617145538330078,
      "learning_rate": 1.8589524205589e-05,
      "loss": 3.0222,
      "step": 21200
    },
    {
      "epoch": 0.2174490066766033,
      "grad_norm": 16.225515365600586,
      "learning_rate": 1.8582706690028022e-05,
      "loss": 3.0102,
      "step": 21300
    },
    {
      "epoch": 0.21846989403189251,
      "grad_norm": 15.013635635375977,
      "learning_rate": 1.8575889174467045e-05,
      "loss": 3.1231,
      "step": 21400
    },
    {
      "epoch": 0.21949078138718173,
      "grad_norm": 17.7394962310791,
      "learning_rate": 1.8569071658906064e-05,
      "loss": 3.0397,
      "step": 21500
    },
    {
      "epoch": 0.22051166874247097,
      "grad_norm": 13.476709365844727,
      "learning_rate": 1.8562254143345084e-05,
      "loss": 3.0892,
      "step": 21600
    },
    {
      "epoch": 0.22153255609776018,
      "grad_norm": 15.238077163696289,
      "learning_rate": 1.8555436627784104e-05,
      "loss": 3.04,
      "step": 21700
    },
    {
      "epoch": 0.2225534434530494,
      "grad_norm": 16.204240798950195,
      "learning_rate": 1.8548619112223127e-05,
      "loss": 2.9116,
      "step": 21800
    },
    {
      "epoch": 0.2235743308083386,
      "grad_norm": 20.75495147705078,
      "learning_rate": 1.8541801596662146e-05,
      "loss": 3.0202,
      "step": 21900
    },
    {
      "epoch": 0.22459521816362782,
      "grad_norm": 22.718891143798828,
      "learning_rate": 1.8534984081101166e-05,
      "loss": 3.0163,
      "step": 22000
    },
    {
      "epoch": 0.22561610551891703,
      "grad_norm": 13.973361015319824,
      "learning_rate": 1.8528166565540186e-05,
      "loss": 3.0945,
      "step": 22100
    },
    {
      "epoch": 0.22663699287420627,
      "grad_norm": 13.73924446105957,
      "learning_rate": 1.852134904997921e-05,
      "loss": 3.1187,
      "step": 22200
    },
    {
      "epoch": 0.22765788022949549,
      "grad_norm": 14.924769401550293,
      "learning_rate": 1.8514531534418228e-05,
      "loss": 3.0426,
      "step": 22300
    },
    {
      "epoch": 0.2286787675847847,
      "grad_norm": 16.296419143676758,
      "learning_rate": 1.850771401885725e-05,
      "loss": 2.9824,
      "step": 22400
    },
    {
      "epoch": 0.2296996549400739,
      "grad_norm": 14.412198066711426,
      "learning_rate": 1.850089650329627e-05,
      "loss": 3.0447,
      "step": 22500
    },
    {
      "epoch": 0.23072054229536312,
      "grad_norm": 18.697933197021484,
      "learning_rate": 1.849407898773529e-05,
      "loss": 3.014,
      "step": 22600
    },
    {
      "epoch": 0.23174142965065234,
      "grad_norm": 18.91050910949707,
      "learning_rate": 1.848732964732992e-05,
      "loss": 3.0329,
      "step": 22700
    },
    {
      "epoch": 0.23276231700594158,
      "grad_norm": 18.99909210205078,
      "learning_rate": 1.8480580306924553e-05,
      "loss": 3.049,
      "step": 22800
    },
    {
      "epoch": 0.2337832043612308,
      "grad_norm": 15.58850383758545,
      "learning_rate": 1.8473762791363572e-05,
      "loss": 3.0084,
      "step": 22900
    },
    {
      "epoch": 0.23480409171652,
      "grad_norm": 16.06903076171875,
      "learning_rate": 1.8466945275802592e-05,
      "loss": 3.0478,
      "step": 23000
    },
    {
      "epoch": 0.23582497907180922,
      "grad_norm": 16.32893180847168,
      "learning_rate": 1.8460127760241615e-05,
      "loss": 3.0094,
      "step": 23100
    },
    {
      "epoch": 0.23684586642709843,
      "grad_norm": 16.820388793945312,
      "learning_rate": 1.8453310244680634e-05,
      "loss": 3.0135,
      "step": 23200
    },
    {
      "epoch": 0.23786675378238764,
      "grad_norm": 20.7681941986084,
      "learning_rate": 1.8446492729119657e-05,
      "loss": 2.9659,
      "step": 23300
    },
    {
      "epoch": 0.23888764113767688,
      "grad_norm": 15.092550277709961,
      "learning_rate": 1.8439675213558677e-05,
      "loss": 2.9565,
      "step": 23400
    },
    {
      "epoch": 0.2399085284929661,
      "grad_norm": 17.81173324584961,
      "learning_rate": 1.8432857697997697e-05,
      "loss": 3.0164,
      "step": 23500
    },
    {
      "epoch": 0.2409294158482553,
      "grad_norm": 19.506135940551758,
      "learning_rate": 1.8426040182436716e-05,
      "loss": 3.0245,
      "step": 23600
    },
    {
      "epoch": 0.24195030320354452,
      "grad_norm": 18.15677833557129,
      "learning_rate": 1.841922266687574e-05,
      "loss": 2.9619,
      "step": 23700
    },
    {
      "epoch": 0.24297119055883373,
      "grad_norm": 24.48411750793457,
      "learning_rate": 1.841240515131476e-05,
      "loss": 3.0304,
      "step": 23800
    },
    {
      "epoch": 0.24399207791412295,
      "grad_norm": 20.522525787353516,
      "learning_rate": 1.8405587635753782e-05,
      "loss": 3.0709,
      "step": 23900
    },
    {
      "epoch": 0.2450129652694122,
      "grad_norm": 19.610376358032227,
      "learning_rate": 1.83987701201928e-05,
      "loss": 3.0325,
      "step": 24000
    },
    {
      "epoch": 0.2460338526247014,
      "grad_norm": 14.773591995239258,
      "learning_rate": 1.839195260463182e-05,
      "loss": 2.9529,
      "step": 24100
    },
    {
      "epoch": 0.2470547399799906,
      "grad_norm": 19.547758102416992,
      "learning_rate": 1.838513508907084e-05,
      "loss": 2.9911,
      "step": 24200
    },
    {
      "epoch": 0.24807562733527982,
      "grad_norm": 16.0813045501709,
      "learning_rate": 1.8378317573509864e-05,
      "loss": 2.9056,
      "step": 24300
    },
    {
      "epoch": 0.24909651469056904,
      "grad_norm": 21.736787796020508,
      "learning_rate": 1.8371500057948883e-05,
      "loss": 3.0319,
      "step": 24400
    },
    {
      "epoch": 0.25011740204585825,
      "grad_norm": 16.229236602783203,
      "learning_rate": 1.8364682542387903e-05,
      "loss": 2.935,
      "step": 24500
    },
    {
      "epoch": 0.25113828940114746,
      "grad_norm": 18.08658218383789,
      "learning_rate": 1.8357865026826923e-05,
      "loss": 2.976,
      "step": 24600
    },
    {
      "epoch": 0.2521591767564367,
      "grad_norm": 16.74418067932129,
      "learning_rate": 1.8351047511265946e-05,
      "loss": 2.9915,
      "step": 24700
    },
    {
      "epoch": 0.2531800641117259,
      "grad_norm": 15.528841018676758,
      "learning_rate": 1.834422999570497e-05,
      "loss": 3.0218,
      "step": 24800
    },
    {
      "epoch": 0.25420095146701516,
      "grad_norm": 15.503874778747559,
      "learning_rate": 1.833741248014399e-05,
      "loss": 3.0221,
      "step": 24900
    },
    {
      "epoch": 0.25522183882230437,
      "grad_norm": 17.550077438354492,
      "learning_rate": 1.8330594964583008e-05,
      "loss": 2.918,
      "step": 25000
    },
    {
      "epoch": 0.2562427261775936,
      "grad_norm": 19.333438873291016,
      "learning_rate": 1.8323777449022028e-05,
      "loss": 2.9237,
      "step": 25100
    },
    {
      "epoch": 0.2572636135328828,
      "grad_norm": 16.929903030395508,
      "learning_rate": 1.831695993346105e-05,
      "loss": 2.9865,
      "step": 25200
    },
    {
      "epoch": 0.258284500888172,
      "grad_norm": 18.198497772216797,
      "learning_rate": 1.831014241790007e-05,
      "loss": 2.9795,
      "step": 25300
    },
    {
      "epoch": 0.2593053882434612,
      "grad_norm": 22.543052673339844,
      "learning_rate": 1.830332490233909e-05,
      "loss": 3.0477,
      "step": 25400
    },
    {
      "epoch": 0.26032627559875043,
      "grad_norm": 19.50048828125,
      "learning_rate": 1.8296507386778113e-05,
      "loss": 2.9217,
      "step": 25500
    },
    {
      "epoch": 0.26134716295403965,
      "grad_norm": 20.728946685791016,
      "learning_rate": 1.8289689871217132e-05,
      "loss": 2.9632,
      "step": 25600
    },
    {
      "epoch": 0.26236805030932886,
      "grad_norm": 15.966204643249512,
      "learning_rate": 1.8282872355656155e-05,
      "loss": 2.8775,
      "step": 25700
    },
    {
      "epoch": 0.2633889376646181,
      "grad_norm": 19.16714859008789,
      "learning_rate": 1.8276054840095175e-05,
      "loss": 3.002,
      "step": 25800
    },
    {
      "epoch": 0.2644098250199073,
      "grad_norm": 13.22396469116211,
      "learning_rate": 1.8269237324534195e-05,
      "loss": 3.0092,
      "step": 25900
    },
    {
      "epoch": 0.2654307123751965,
      "grad_norm": 20.159038543701172,
      "learning_rate": 1.8262419808973214e-05,
      "loss": 2.9213,
      "step": 26000
    },
    {
      "epoch": 0.2664515997304857,
      "grad_norm": 14.91690731048584,
      "learning_rate": 1.8255602293412237e-05,
      "loss": 3.0444,
      "step": 26100
    },
    {
      "epoch": 0.267472487085775,
      "grad_norm": 22.927818298339844,
      "learning_rate": 1.8248784777851257e-05,
      "loss": 3.0155,
      "step": 26200
    },
    {
      "epoch": 0.2684933744410642,
      "grad_norm": 16.39606475830078,
      "learning_rate": 1.8241967262290277e-05,
      "loss": 2.8382,
      "step": 26300
    },
    {
      "epoch": 0.2695142617963534,
      "grad_norm": 17.669700622558594,
      "learning_rate": 1.82351497467293e-05,
      "loss": 3.0761,
      "step": 26400
    },
    {
      "epoch": 0.2705351491516426,
      "grad_norm": 15.20538330078125,
      "learning_rate": 1.822833223116832e-05,
      "loss": 2.9557,
      "step": 26500
    },
    {
      "epoch": 0.27155603650693183,
      "grad_norm": 22.678049087524414,
      "learning_rate": 1.822151471560734e-05,
      "loss": 2.9451,
      "step": 26600
    },
    {
      "epoch": 0.27257692386222104,
      "grad_norm": 17.281770706176758,
      "learning_rate": 1.8214697200046362e-05,
      "loss": 3.0623,
      "step": 26700
    },
    {
      "epoch": 0.27359781121751026,
      "grad_norm": 17.49955177307129,
      "learning_rate": 1.820787968448538e-05,
      "loss": 2.9333,
      "step": 26800
    },
    {
      "epoch": 0.27461869857279947,
      "grad_norm": 18.846839904785156,
      "learning_rate": 1.82010621689244e-05,
      "loss": 2.965,
      "step": 26900
    },
    {
      "epoch": 0.2756395859280887,
      "grad_norm": 17.429452896118164,
      "learning_rate": 1.819424465336342e-05,
      "loss": 2.8887,
      "step": 27000
    },
    {
      "epoch": 0.2766604732833779,
      "grad_norm": 29.333820343017578,
      "learning_rate": 1.8187427137802444e-05,
      "loss": 2.8876,
      "step": 27100
    },
    {
      "epoch": 0.2776813606386671,
      "grad_norm": 18.41864776611328,
      "learning_rate": 1.8180609622241467e-05,
      "loss": 3.0192,
      "step": 27200
    },
    {
      "epoch": 0.2787022479939563,
      "grad_norm": 18.8918514251709,
      "learning_rate": 1.8173792106680486e-05,
      "loss": 2.9185,
      "step": 27300
    },
    {
      "epoch": 0.2797231353492456,
      "grad_norm": 12.998002052307129,
      "learning_rate": 1.8166974591119506e-05,
      "loss": 2.8715,
      "step": 27400
    },
    {
      "epoch": 0.2807440227045348,
      "grad_norm": 18.671222686767578,
      "learning_rate": 1.8160157075558526e-05,
      "loss": 2.9823,
      "step": 27500
    },
    {
      "epoch": 0.281764910059824,
      "grad_norm": 24.242094039916992,
      "learning_rate": 1.815333955999755e-05,
      "loss": 3.0565,
      "step": 27600
    },
    {
      "epoch": 0.2827857974151132,
      "grad_norm": 17.85924530029297,
      "learning_rate": 1.8146522044436568e-05,
      "loss": 2.9761,
      "step": 27700
    },
    {
      "epoch": 0.28380668477040244,
      "grad_norm": 17.531578063964844,
      "learning_rate": 1.8139704528875588e-05,
      "loss": 2.9961,
      "step": 27800
    },
    {
      "epoch": 0.28482757212569165,
      "grad_norm": 19.07610321044922,
      "learning_rate": 1.8132887013314607e-05,
      "loss": 3.0179,
      "step": 27900
    },
    {
      "epoch": 0.28584845948098087,
      "grad_norm": 29.138883590698242,
      "learning_rate": 1.812606949775363e-05,
      "loss": 2.9815,
      "step": 28000
    },
    {
      "epoch": 0.2868693468362701,
      "grad_norm": 15.694374084472656,
      "learning_rate": 1.811925198219265e-05,
      "loss": 2.8407,
      "step": 28100
    },
    {
      "epoch": 0.2878902341915593,
      "grad_norm": 20.948402404785156,
      "learning_rate": 1.8112434466631673e-05,
      "loss": 2.9257,
      "step": 28200
    },
    {
      "epoch": 0.2889111215468485,
      "grad_norm": 17.835264205932617,
      "learning_rate": 1.8105616951070693e-05,
      "loss": 2.9323,
      "step": 28300
    },
    {
      "epoch": 0.2899320089021377,
      "grad_norm": 15.399815559387207,
      "learning_rate": 1.8098799435509712e-05,
      "loss": 2.8821,
      "step": 28400
    },
    {
      "epoch": 0.29095289625742693,
      "grad_norm": 13.115978240966797,
      "learning_rate": 1.8091981919948732e-05,
      "loss": 2.8695,
      "step": 28500
    },
    {
      "epoch": 0.2919737836127162,
      "grad_norm": 17.230815887451172,
      "learning_rate": 1.8085232579543363e-05,
      "loss": 2.9926,
      "step": 28600
    },
    {
      "epoch": 0.2929946709680054,
      "grad_norm": 20.77741241455078,
      "learning_rate": 1.8078483239137994e-05,
      "loss": 2.8679,
      "step": 28700
    },
    {
      "epoch": 0.2940155583232946,
      "grad_norm": 15.277021408081055,
      "learning_rate": 1.8071665723577017e-05,
      "loss": 3.001,
      "step": 28800
    },
    {
      "epoch": 0.29503644567858384,
      "grad_norm": 13.998103141784668,
      "learning_rate": 1.8064848208016037e-05,
      "loss": 2.9473,
      "step": 28900
    },
    {
      "epoch": 0.29605733303387305,
      "grad_norm": 15.191620826721191,
      "learning_rate": 1.8058030692455056e-05,
      "loss": 2.8759,
      "step": 29000
    },
    {
      "epoch": 0.29707822038916226,
      "grad_norm": 18.238262176513672,
      "learning_rate": 1.805121317689408e-05,
      "loss": 2.8962,
      "step": 29100
    },
    {
      "epoch": 0.2980991077444515,
      "grad_norm": 16.420536041259766,
      "learning_rate": 1.80443956613331e-05,
      "loss": 3.016,
      "step": 29200
    },
    {
      "epoch": 0.2991199950997407,
      "grad_norm": 16.65898323059082,
      "learning_rate": 1.803757814577212e-05,
      "loss": 2.9685,
      "step": 29300
    },
    {
      "epoch": 0.3001408824550299,
      "grad_norm": 14.010841369628906,
      "learning_rate": 1.8030760630211138e-05,
      "loss": 3.0174,
      "step": 29400
    },
    {
      "epoch": 0.3011617698103191,
      "grad_norm": 17.370006561279297,
      "learning_rate": 1.802394311465016e-05,
      "loss": 2.9647,
      "step": 29500
    },
    {
      "epoch": 0.3021826571656083,
      "grad_norm": 17.70941925048828,
      "learning_rate": 1.8017125599089184e-05,
      "loss": 2.995,
      "step": 29600
    },
    {
      "epoch": 0.30320354452089754,
      "grad_norm": 18.466955184936523,
      "learning_rate": 1.8010308083528204e-05,
      "loss": 2.9577,
      "step": 29700
    },
    {
      "epoch": 0.3042244318761868,
      "grad_norm": 15.085166931152344,
      "learning_rate": 1.8003490567967223e-05,
      "loss": 2.929,
      "step": 29800
    },
    {
      "epoch": 0.305245319231476,
      "grad_norm": 14.273608207702637,
      "learning_rate": 1.7996741227561854e-05,
      "loss": 3.0293,
      "step": 29900
    },
    {
      "epoch": 0.30626620658676523,
      "grad_norm": 19.873687744140625,
      "learning_rate": 1.7989923712000874e-05,
      "loss": 2.9601,
      "step": 30000
    },
    {
      "epoch": 0.30728709394205445,
      "grad_norm": 36.88760757446289,
      "learning_rate": 1.7983106196439894e-05,
      "loss": 2.9195,
      "step": 30100
    },
    {
      "epoch": 0.30830798129734366,
      "grad_norm": 16.25327491760254,
      "learning_rate": 1.7976288680878913e-05,
      "loss": 2.9485,
      "step": 30200
    },
    {
      "epoch": 0.30932886865263287,
      "grad_norm": 12.840707778930664,
      "learning_rate": 1.7969471165317936e-05,
      "loss": 2.9009,
      "step": 30300
    },
    {
      "epoch": 0.3103497560079221,
      "grad_norm": 18.161046981811523,
      "learning_rate": 1.796265364975696e-05,
      "loss": 2.9875,
      "step": 30400
    },
    {
      "epoch": 0.3113706433632113,
      "grad_norm": 25.412179946899414,
      "learning_rate": 1.795583613419598e-05,
      "loss": 2.962,
      "step": 30500
    },
    {
      "epoch": 0.3123915307185005,
      "grad_norm": 14.862384796142578,
      "learning_rate": 1.7949018618635e-05,
      "loss": 2.9881,
      "step": 30600
    },
    {
      "epoch": 0.3134124180737897,
      "grad_norm": 14.668319702148438,
      "learning_rate": 1.7942201103074018e-05,
      "loss": 2.988,
      "step": 30700
    },
    {
      "epoch": 0.31443330542907894,
      "grad_norm": 17.97743797302246,
      "learning_rate": 1.793538358751304e-05,
      "loss": 2.9182,
      "step": 30800
    },
    {
      "epoch": 0.31545419278436815,
      "grad_norm": 17.513044357299805,
      "learning_rate": 1.792856607195206e-05,
      "loss": 2.9652,
      "step": 30900
    },
    {
      "epoch": 0.3164750801396574,
      "grad_norm": 18.131074905395508,
      "learning_rate": 1.792174855639108e-05,
      "loss": 2.9615,
      "step": 31000
    },
    {
      "epoch": 0.31749596749494663,
      "grad_norm": 17.715728759765625,
      "learning_rate": 1.79149310408301e-05,
      "loss": 2.9829,
      "step": 31100
    },
    {
      "epoch": 0.31851685485023584,
      "grad_norm": 16.263315200805664,
      "learning_rate": 1.7908113525269123e-05,
      "loss": 2.9297,
      "step": 31200
    },
    {
      "epoch": 0.31953774220552505,
      "grad_norm": 19.40182113647461,
      "learning_rate": 1.7901296009708146e-05,
      "loss": 2.883,
      "step": 31300
    },
    {
      "epoch": 0.32055862956081427,
      "grad_norm": 15.191802978515625,
      "learning_rate": 1.7894478494147166e-05,
      "loss": 2.9008,
      "step": 31400
    },
    {
      "epoch": 0.3215795169161035,
      "grad_norm": 18.968780517578125,
      "learning_rate": 1.7887660978586185e-05,
      "loss": 2.9239,
      "step": 31500
    },
    {
      "epoch": 0.3226004042713927,
      "grad_norm": 16.143672943115234,
      "learning_rate": 1.7880843463025205e-05,
      "loss": 2.8949,
      "step": 31600
    },
    {
      "epoch": 0.3236212916266819,
      "grad_norm": 19.500019073486328,
      "learning_rate": 1.7874025947464228e-05,
      "loss": 2.8625,
      "step": 31700
    },
    {
      "epoch": 0.3246421789819711,
      "grad_norm": 19.672626495361328,
      "learning_rate": 1.7867208431903248e-05,
      "loss": 2.9986,
      "step": 31800
    },
    {
      "epoch": 0.32566306633726033,
      "grad_norm": 18.246049880981445,
      "learning_rate": 1.7860390916342267e-05,
      "loss": 2.8869,
      "step": 31900
    },
    {
      "epoch": 0.32668395369254954,
      "grad_norm": 16.5230712890625,
      "learning_rate": 1.7853573400781287e-05,
      "loss": 2.9461,
      "step": 32000
    },
    {
      "epoch": 0.32770484104783876,
      "grad_norm": 13.759838104248047,
      "learning_rate": 1.784675588522031e-05,
      "loss": 2.9095,
      "step": 32100
    },
    {
      "epoch": 0.328725728403128,
      "grad_norm": 20.883127212524414,
      "learning_rate": 1.783993836965933e-05,
      "loss": 2.8838,
      "step": 32200
    },
    {
      "epoch": 0.32974661575841724,
      "grad_norm": 15.702425956726074,
      "learning_rate": 1.7833120854098352e-05,
      "loss": 2.9701,
      "step": 32300
    },
    {
      "epoch": 0.33076750311370645,
      "grad_norm": 21.145009994506836,
      "learning_rate": 1.7826303338537372e-05,
      "loss": 2.9046,
      "step": 32400
    },
    {
      "epoch": 0.33178839046899566,
      "grad_norm": 18.080915451049805,
      "learning_rate": 1.781948582297639e-05,
      "loss": 3.0578,
      "step": 32500
    },
    {
      "epoch": 0.3328092778242849,
      "grad_norm": 15.779363632202148,
      "learning_rate": 1.781266830741541e-05,
      "loss": 2.9675,
      "step": 32600
    },
    {
      "epoch": 0.3338301651795741,
      "grad_norm": 17.63916015625,
      "learning_rate": 1.7805850791854434e-05,
      "loss": 2.9146,
      "step": 32700
    },
    {
      "epoch": 0.3348510525348633,
      "grad_norm": 14.58967113494873,
      "learning_rate": 1.7799033276293454e-05,
      "loss": 2.9524,
      "step": 32800
    },
    {
      "epoch": 0.3358719398901525,
      "grad_norm": 19.73111915588379,
      "learning_rate": 1.7792215760732477e-05,
      "loss": 2.9136,
      "step": 32900
    },
    {
      "epoch": 0.33689282724544173,
      "grad_norm": 18.857986450195312,
      "learning_rate": 1.7785398245171497e-05,
      "loss": 2.9343,
      "step": 33000
    },
    {
      "epoch": 0.33791371460073094,
      "grad_norm": 14.44428825378418,
      "learning_rate": 1.7778580729610516e-05,
      "loss": 2.9696,
      "step": 33100
    },
    {
      "epoch": 0.33893460195602015,
      "grad_norm": 13.700292587280273,
      "learning_rate": 1.777176321404954e-05,
      "loss": 2.8973,
      "step": 33200
    },
    {
      "epoch": 0.33995548931130937,
      "grad_norm": 16.119747161865234,
      "learning_rate": 1.776494569848856e-05,
      "loss": 2.9171,
      "step": 33300
    },
    {
      "epoch": 0.34097637666659864,
      "grad_norm": 18.387596130371094,
      "learning_rate": 1.775812818292758e-05,
      "loss": 2.9204,
      "step": 33400
    },
    {
      "epoch": 0.34199726402188785,
      "grad_norm": 22.769914627075195,
      "learning_rate": 1.7751310667366598e-05,
      "loss": 2.8905,
      "step": 33500
    },
    {
      "epoch": 0.34301815137717706,
      "grad_norm": 17.427385330200195,
      "learning_rate": 1.774449315180562e-05,
      "loss": 2.8154,
      "step": 33600
    },
    {
      "epoch": 0.3440390387324663,
      "grad_norm": 15.917625427246094,
      "learning_rate": 1.773767563624464e-05,
      "loss": 2.9101,
      "step": 33700
    },
    {
      "epoch": 0.3450599260877555,
      "grad_norm": 17.39680290222168,
      "learning_rate": 1.7730858120683664e-05,
      "loss": 2.8469,
      "step": 33800
    },
    {
      "epoch": 0.3460808134430447,
      "grad_norm": 19.349822998046875,
      "learning_rate": 1.7724040605122683e-05,
      "loss": 2.9597,
      "step": 33900
    },
    {
      "epoch": 0.3471017007983339,
      "grad_norm": 26.059383392333984,
      "learning_rate": 1.7717223089561703e-05,
      "loss": 2.961,
      "step": 34000
    },
    {
      "epoch": 0.3481225881536231,
      "grad_norm": 13.962657928466797,
      "learning_rate": 1.7710405574000722e-05,
      "loss": 2.9402,
      "step": 34100
    },
    {
      "epoch": 0.34914347550891234,
      "grad_norm": 14.35244083404541,
      "learning_rate": 1.7703588058439746e-05,
      "loss": 2.8376,
      "step": 34200
    },
    {
      "epoch": 0.35016436286420155,
      "grad_norm": 14.876498222351074,
      "learning_rate": 1.7696770542878765e-05,
      "loss": 2.9166,
      "step": 34300
    },
    {
      "epoch": 0.35118525021949076,
      "grad_norm": 17.99911880493164,
      "learning_rate": 1.7689953027317785e-05,
      "loss": 3.0206,
      "step": 34400
    },
    {
      "epoch": 0.35220613757478,
      "grad_norm": 19.31058692932129,
      "learning_rate": 1.7683135511756804e-05,
      "loss": 2.9042,
      "step": 34500
    },
    {
      "epoch": 0.3532270249300692,
      "grad_norm": 17.08619499206543,
      "learning_rate": 1.7676317996195827e-05,
      "loss": 2.8602,
      "step": 34600
    },
    {
      "epoch": 0.35424791228535846,
      "grad_norm": 15.521244049072266,
      "learning_rate": 1.766950048063485e-05,
      "loss": 2.8685,
      "step": 34700
    },
    {
      "epoch": 0.35526879964064767,
      "grad_norm": 21.12894058227539,
      "learning_rate": 1.766268296507387e-05,
      "loss": 3.009,
      "step": 34800
    },
    {
      "epoch": 0.3562896869959369,
      "grad_norm": 16.830671310424805,
      "learning_rate": 1.765586544951289e-05,
      "loss": 2.9179,
      "step": 34900
    },
    {
      "epoch": 0.3573105743512261,
      "grad_norm": 22.38498878479004,
      "learning_rate": 1.764904793395191e-05,
      "loss": 2.9582,
      "step": 35000
    },
    {
      "epoch": 0.3583314617065153,
      "grad_norm": 18.61494255065918,
      "learning_rate": 1.7642230418390932e-05,
      "loss": 2.8936,
      "step": 35100
    },
    {
      "epoch": 0.3593523490618045,
      "grad_norm": 16.915878295898438,
      "learning_rate": 1.7635412902829952e-05,
      "loss": 2.94,
      "step": 35200
    },
    {
      "epoch": 0.36037323641709373,
      "grad_norm": 16.062746047973633,
      "learning_rate": 1.762859538726897e-05,
      "loss": 2.9736,
      "step": 35300
    },
    {
      "epoch": 0.36139412377238295,
      "grad_norm": 19.218732833862305,
      "learning_rate": 1.7621777871707994e-05,
      "loss": 2.8986,
      "step": 35400
    },
    {
      "epoch": 0.36241501112767216,
      "grad_norm": 15.045774459838867,
      "learning_rate": 1.7614960356147014e-05,
      "loss": 2.9487,
      "step": 35500
    },
    {
      "epoch": 0.3634358984829614,
      "grad_norm": 18.572282791137695,
      "learning_rate": 1.7608211015741645e-05,
      "loss": 2.9163,
      "step": 35600
    },
    {
      "epoch": 0.3644567858382506,
      "grad_norm": 16.123497009277344,
      "learning_rate": 1.7601393500180665e-05,
      "loss": 2.9237,
      "step": 35700
    },
    {
      "epoch": 0.3654776731935398,
      "grad_norm": 15.381569862365723,
      "learning_rate": 1.7594575984619688e-05,
      "loss": 2.9317,
      "step": 35800
    },
    {
      "epoch": 0.36649856054882907,
      "grad_norm": 14.438336372375488,
      "learning_rate": 1.7587758469058707e-05,
      "loss": 2.9914,
      "step": 35900
    },
    {
      "epoch": 0.3675194479041183,
      "grad_norm": 15.171579360961914,
      "learning_rate": 1.7580940953497727e-05,
      "loss": 2.9139,
      "step": 36000
    },
    {
      "epoch": 0.3685403352594075,
      "grad_norm": 19.218717575073242,
      "learning_rate": 1.7574123437936747e-05,
      "loss": 2.9485,
      "step": 36100
    },
    {
      "epoch": 0.3695612226146967,
      "grad_norm": 20.581876754760742,
      "learning_rate": 1.756730592237577e-05,
      "loss": 2.9277,
      "step": 36200
    },
    {
      "epoch": 0.3705821099699859,
      "grad_norm": 18.46770477294922,
      "learning_rate": 1.756048840681479e-05,
      "loss": 2.9839,
      "step": 36300
    },
    {
      "epoch": 0.37160299732527513,
      "grad_norm": 14.292405128479004,
      "learning_rate": 1.7553670891253812e-05,
      "loss": 2.8168,
      "step": 36400
    },
    {
      "epoch": 0.37262388468056434,
      "grad_norm": 15.292518615722656,
      "learning_rate": 1.7546853375692832e-05,
      "loss": 2.9221,
      "step": 36500
    },
    {
      "epoch": 0.37364477203585356,
      "grad_norm": 15.944416999816895,
      "learning_rate": 1.7540104035287463e-05,
      "loss": 2.9037,
      "step": 36600
    },
    {
      "epoch": 0.37466565939114277,
      "grad_norm": 19.98464584350586,
      "learning_rate": 1.7533286519726483e-05,
      "loss": 3.0082,
      "step": 36700
    },
    {
      "epoch": 0.375686546746432,
      "grad_norm": 15.651169776916504,
      "learning_rate": 1.7526469004165502e-05,
      "loss": 2.8678,
      "step": 36800
    },
    {
      "epoch": 0.3767074341017212,
      "grad_norm": 16.577150344848633,
      "learning_rate": 1.7519651488604522e-05,
      "loss": 2.8733,
      "step": 36900
    },
    {
      "epoch": 0.3777283214570104,
      "grad_norm": 15.774858474731445,
      "learning_rate": 1.7512833973043545e-05,
      "loss": 2.9394,
      "step": 37000
    },
    {
      "epoch": 0.3787492088122997,
      "grad_norm": 18.35483169555664,
      "learning_rate": 1.7506016457482568e-05,
      "loss": 2.8941,
      "step": 37100
    },
    {
      "epoch": 0.3797700961675889,
      "grad_norm": 17.35249137878418,
      "learning_rate": 1.7499198941921587e-05,
      "loss": 2.8351,
      "step": 37200
    },
    {
      "epoch": 0.3807909835228781,
      "grad_norm": 15.966257095336914,
      "learning_rate": 1.7492381426360607e-05,
      "loss": 2.9399,
      "step": 37300
    },
    {
      "epoch": 0.3818118708781673,
      "grad_norm": 13.645195007324219,
      "learning_rate": 1.7485563910799627e-05,
      "loss": 2.9069,
      "step": 37400
    },
    {
      "epoch": 0.3828327582334565,
      "grad_norm": 22.872419357299805,
      "learning_rate": 1.747874639523865e-05,
      "loss": 2.8772,
      "step": 37500
    },
    {
      "epoch": 0.38385364558874574,
      "grad_norm": 14.987547874450684,
      "learning_rate": 1.747192887967767e-05,
      "loss": 2.8895,
      "step": 37600
    },
    {
      "epoch": 0.38487453294403495,
      "grad_norm": 12.039593696594238,
      "learning_rate": 1.746511136411669e-05,
      "loss": 2.936,
      "step": 37700
    },
    {
      "epoch": 0.38589542029932417,
      "grad_norm": 17.475067138671875,
      "learning_rate": 1.7458293848555712e-05,
      "loss": 2.9383,
      "step": 37800
    },
    {
      "epoch": 0.3869163076546134,
      "grad_norm": 14.25981330871582,
      "learning_rate": 1.745147633299473e-05,
      "loss": 2.8827,
      "step": 37900
    },
    {
      "epoch": 0.3879371950099026,
      "grad_norm": 15.838835716247559,
      "learning_rate": 1.744465881743375e-05,
      "loss": 2.8998,
      "step": 38000
    },
    {
      "epoch": 0.3889580823651918,
      "grad_norm": 17.523677825927734,
      "learning_rate": 1.7437841301872774e-05,
      "loss": 2.9337,
      "step": 38100
    },
    {
      "epoch": 0.389978969720481,
      "grad_norm": 19.89725685119629,
      "learning_rate": 1.7431023786311794e-05,
      "loss": 2.8665,
      "step": 38200
    },
    {
      "epoch": 0.3909998570757703,
      "grad_norm": 14.788044929504395,
      "learning_rate": 1.7424206270750813e-05,
      "loss": 2.9041,
      "step": 38300
    },
    {
      "epoch": 0.3920207444310595,
      "grad_norm": 14.734932899475098,
      "learning_rate": 1.7417388755189833e-05,
      "loss": 2.8833,
      "step": 38400
    },
    {
      "epoch": 0.3930416317863487,
      "grad_norm": 14.784002304077148,
      "learning_rate": 1.7410571239628856e-05,
      "loss": 2.9145,
      "step": 38500
    },
    {
      "epoch": 0.3940625191416379,
      "grad_norm": 13.946574211120605,
      "learning_rate": 1.7403753724067876e-05,
      "loss": 2.8714,
      "step": 38600
    },
    {
      "epoch": 0.39508340649692714,
      "grad_norm": 17.04364013671875,
      "learning_rate": 1.73969362085069e-05,
      "loss": 2.8196,
      "step": 38700
    },
    {
      "epoch": 0.39610429385221635,
      "grad_norm": 16.74332046508789,
      "learning_rate": 1.739011869294592e-05,
      "loss": 2.9823,
      "step": 38800
    },
    {
      "epoch": 0.39712518120750556,
      "grad_norm": 20.046276092529297,
      "learning_rate": 1.7383301177384938e-05,
      "loss": 2.9022,
      "step": 38900
    },
    {
      "epoch": 0.3981460685627948,
      "grad_norm": 15.018179893493652,
      "learning_rate": 1.737648366182396e-05,
      "loss": 2.8692,
      "step": 39000
    },
    {
      "epoch": 0.399166955918084,
      "grad_norm": 18.878562927246094,
      "learning_rate": 1.736966614626298e-05,
      "loss": 2.932,
      "step": 39100
    },
    {
      "epoch": 0.4001878432733732,
      "grad_norm": 14.514636993408203,
      "learning_rate": 1.7362848630702e-05,
      "loss": 2.9652,
      "step": 39200
    },
    {
      "epoch": 0.4012087306286624,
      "grad_norm": 16.8803653717041,
      "learning_rate": 1.735603111514102e-05,
      "loss": 2.852,
      "step": 39300
    },
    {
      "epoch": 0.4022296179839516,
      "grad_norm": 20.415508270263672,
      "learning_rate": 1.7349213599580043e-05,
      "loss": 2.855,
      "step": 39400
    },
    {
      "epoch": 0.4032505053392409,
      "grad_norm": 14.00813102722168,
      "learning_rate": 1.7342396084019066e-05,
      "loss": 2.8724,
      "step": 39500
    },
    {
      "epoch": 0.4042713926945301,
      "grad_norm": 13.450998306274414,
      "learning_rate": 1.7335578568458085e-05,
      "loss": 2.9418,
      "step": 39600
    },
    {
      "epoch": 0.4052922800498193,
      "grad_norm": 13.528029441833496,
      "learning_rate": 1.7328761052897105e-05,
      "loss": 2.8996,
      "step": 39700
    },
    {
      "epoch": 0.40631316740510853,
      "grad_norm": 13.86734390258789,
      "learning_rate": 1.7321943537336125e-05,
      "loss": 2.9402,
      "step": 39800
    },
    {
      "epoch": 0.40733405476039775,
      "grad_norm": 25.10052490234375,
      "learning_rate": 1.7315126021775148e-05,
      "loss": 2.8914,
      "step": 39900
    },
    {
      "epoch": 0.40835494211568696,
      "grad_norm": 19.16450309753418,
      "learning_rate": 1.7308308506214167e-05,
      "loss": 2.94,
      "step": 40000
    },
    {
      "epoch": 0.40937582947097617,
      "grad_norm": 20.597232818603516,
      "learning_rate": 1.7301490990653187e-05,
      "loss": 2.8881,
      "step": 40100
    },
    {
      "epoch": 0.4103967168262654,
      "grad_norm": 16.92115592956543,
      "learning_rate": 1.7294673475092207e-05,
      "loss": 2.8847,
      "step": 40200
    },
    {
      "epoch": 0.4114176041815546,
      "grad_norm": 18.05141830444336,
      "learning_rate": 1.728785595953123e-05,
      "loss": 2.904,
      "step": 40300
    },
    {
      "epoch": 0.4124384915368438,
      "grad_norm": 17.51869010925293,
      "learning_rate": 1.728103844397025e-05,
      "loss": 2.8687,
      "step": 40400
    },
    {
      "epoch": 0.413459378892133,
      "grad_norm": 17.7916316986084,
      "learning_rate": 1.7274220928409272e-05,
      "loss": 2.9595,
      "step": 40500
    },
    {
      "epoch": 0.41448026624742224,
      "grad_norm": 20.608530044555664,
      "learning_rate": 1.7267403412848292e-05,
      "loss": 2.8789,
      "step": 40600
    },
    {
      "epoch": 0.4155011536027115,
      "grad_norm": 13.773937225341797,
      "learning_rate": 1.726058589728731e-05,
      "loss": 2.9678,
      "step": 40700
    },
    {
      "epoch": 0.4165220409580007,
      "grad_norm": 16.39723014831543,
      "learning_rate": 1.725376838172633e-05,
      "loss": 2.865,
      "step": 40800
    },
    {
      "epoch": 0.41754292831328993,
      "grad_norm": 16.759765625,
      "learning_rate": 1.7246950866165354e-05,
      "loss": 2.8027,
      "step": 40900
    },
    {
      "epoch": 0.41856381566857914,
      "grad_norm": 19.927215576171875,
      "learning_rate": 1.7240133350604374e-05,
      "loss": 2.8879,
      "step": 41000
    },
    {
      "epoch": 0.41958470302386836,
      "grad_norm": 17.29587745666504,
      "learning_rate": 1.7233315835043397e-05,
      "loss": 2.8652,
      "step": 41100
    },
    {
      "epoch": 0.42060559037915757,
      "grad_norm": 20.40043830871582,
      "learning_rate": 1.7226498319482416e-05,
      "loss": 2.9098,
      "step": 41200
    },
    {
      "epoch": 0.4216264777344468,
      "grad_norm": 18.373807907104492,
      "learning_rate": 1.7219680803921436e-05,
      "loss": 2.9079,
      "step": 41300
    },
    {
      "epoch": 0.422647365089736,
      "grad_norm": 19.62664222717285,
      "learning_rate": 1.721286328836046e-05,
      "loss": 2.8832,
      "step": 41400
    },
    {
      "epoch": 0.4236682524450252,
      "grad_norm": 15.654008865356445,
      "learning_rate": 1.720604577279948e-05,
      "loss": 2.9125,
      "step": 41500
    },
    {
      "epoch": 0.4246891398003144,
      "grad_norm": 15.10980224609375,
      "learning_rate": 1.7199228257238498e-05,
      "loss": 2.8242,
      "step": 41600
    },
    {
      "epoch": 0.42571002715560363,
      "grad_norm": 15.807576179504395,
      "learning_rate": 1.7192410741677518e-05,
      "loss": 2.9489,
      "step": 41700
    },
    {
      "epoch": 0.42673091451089284,
      "grad_norm": 14.138705253601074,
      "learning_rate": 1.718559322611654e-05,
      "loss": 2.9306,
      "step": 41800
    },
    {
      "epoch": 0.4277518018661821,
      "grad_norm": 19.825733184814453,
      "learning_rate": 1.717877571055556e-05,
      "loss": 2.9177,
      "step": 41900
    },
    {
      "epoch": 0.4287726892214713,
      "grad_norm": 15.957036018371582,
      "learning_rate": 1.7171958194994583e-05,
      "loss": 2.9146,
      "step": 42000
    },
    {
      "epoch": 0.42979357657676054,
      "grad_norm": 17.84798812866211,
      "learning_rate": 1.7165140679433603e-05,
      "loss": 2.8341,
      "step": 42100
    },
    {
      "epoch": 0.43081446393204975,
      "grad_norm": 15.486225128173828,
      "learning_rate": 1.7158323163872623e-05,
      "loss": 2.908,
      "step": 42200
    },
    {
      "epoch": 0.43183535128733896,
      "grad_norm": 17.243227005004883,
      "learning_rate": 1.7151505648311642e-05,
      "loss": 2.8907,
      "step": 42300
    },
    {
      "epoch": 0.4328562386426282,
      "grad_norm": 18.749900817871094,
      "learning_rate": 1.7144688132750665e-05,
      "loss": 2.8875,
      "step": 42400
    },
    {
      "epoch": 0.4338771259979174,
      "grad_norm": 22.974966049194336,
      "learning_rate": 1.7137870617189685e-05,
      "loss": 2.8094,
      "step": 42500
    },
    {
      "epoch": 0.4348980133532066,
      "grad_norm": 18.854333877563477,
      "learning_rate": 1.7131053101628704e-05,
      "loss": 2.9166,
      "step": 42600
    },
    {
      "epoch": 0.4359189007084958,
      "grad_norm": 10.473959922790527,
      "learning_rate": 1.7124235586067724e-05,
      "loss": 2.9233,
      "step": 42700
    },
    {
      "epoch": 0.43693978806378503,
      "grad_norm": 15.164804458618164,
      "learning_rate": 1.7117418070506747e-05,
      "loss": 2.8522,
      "step": 42800
    },
    {
      "epoch": 0.43796067541907424,
      "grad_norm": 16.08922576904297,
      "learning_rate": 1.711060055494577e-05,
      "loss": 2.9326,
      "step": 42900
    },
    {
      "epoch": 0.43898156277436345,
      "grad_norm": 19.1790771484375,
      "learning_rate": 1.710378303938479e-05,
      "loss": 2.9192,
      "step": 43000
    },
    {
      "epoch": 0.44000245012965267,
      "grad_norm": 14.579111099243164,
      "learning_rate": 1.709696552382381e-05,
      "loss": 2.8514,
      "step": 43100
    },
    {
      "epoch": 0.44102333748494194,
      "grad_norm": 15.352651596069336,
      "learning_rate": 1.709014800826283e-05,
      "loss": 2.912,
      "step": 43200
    },
    {
      "epoch": 0.44204422484023115,
      "grad_norm": 19.86180877685547,
      "learning_rate": 1.7083330492701852e-05,
      "loss": 2.893,
      "step": 43300
    },
    {
      "epoch": 0.44306511219552036,
      "grad_norm": 12.239179611206055,
      "learning_rate": 1.707651297714087e-05,
      "loss": 2.8397,
      "step": 43400
    },
    {
      "epoch": 0.4440859995508096,
      "grad_norm": 13.07601261138916,
      "learning_rate": 1.706969546157989e-05,
      "loss": 2.8861,
      "step": 43500
    },
    {
      "epoch": 0.4451068869060988,
      "grad_norm": 14.713488578796387,
      "learning_rate": 1.7062877946018914e-05,
      "loss": 2.8889,
      "step": 43600
    },
    {
      "epoch": 0.446127774261388,
      "grad_norm": 15.138575553894043,
      "learning_rate": 1.7056060430457934e-05,
      "loss": 2.904,
      "step": 43700
    },
    {
      "epoch": 0.4471486616166772,
      "grad_norm": 19.68803596496582,
      "learning_rate": 1.7049242914896957e-05,
      "loss": 2.9148,
      "step": 43800
    },
    {
      "epoch": 0.4481695489719664,
      "grad_norm": 16.05628204345703,
      "learning_rate": 1.7042425399335976e-05,
      "loss": 2.8714,
      "step": 43900
    },
    {
      "epoch": 0.44919043632725564,
      "grad_norm": 17.25923728942871,
      "learning_rate": 1.7035676058930604e-05,
      "loss": 2.8246,
      "step": 44000
    },
    {
      "epoch": 0.45021132368254485,
      "grad_norm": 15.676199913024902,
      "learning_rate": 1.7028858543369627e-05,
      "loss": 2.8497,
      "step": 44100
    },
    {
      "epoch": 0.45123221103783406,
      "grad_norm": 13.748002052307129,
      "learning_rate": 1.7022041027808647e-05,
      "loss": 2.8241,
      "step": 44200
    },
    {
      "epoch": 0.4522530983931233,
      "grad_norm": 18.35150909423828,
      "learning_rate": 1.7015223512247666e-05,
      "loss": 2.9139,
      "step": 44300
    },
    {
      "epoch": 0.45327398574841254,
      "grad_norm": 17.714191436767578,
      "learning_rate": 1.700840599668669e-05,
      "loss": 2.848,
      "step": 44400
    },
    {
      "epoch": 0.45429487310370176,
      "grad_norm": 16.174331665039062,
      "learning_rate": 1.700158848112571e-05,
      "loss": 2.7854,
      "step": 44500
    },
    {
      "epoch": 0.45531576045899097,
      "grad_norm": 15.371034622192383,
      "learning_rate": 1.6994770965564732e-05,
      "loss": 2.9105,
      "step": 44600
    },
    {
      "epoch": 0.4563366478142802,
      "grad_norm": 15.754937171936035,
      "learning_rate": 1.698795345000375e-05,
      "loss": 2.8758,
      "step": 44700
    },
    {
      "epoch": 0.4573575351695694,
      "grad_norm": 20.11613655090332,
      "learning_rate": 1.698113593444277e-05,
      "loss": 2.8489,
      "step": 44800
    },
    {
      "epoch": 0.4583784225248586,
      "grad_norm": 16.130102157592773,
      "learning_rate": 1.697431841888179e-05,
      "loss": 2.8101,
      "step": 44900
    },
    {
      "epoch": 0.4593993098801478,
      "grad_norm": 17.949581146240234,
      "learning_rate": 1.6967500903320814e-05,
      "loss": 2.8734,
      "step": 45000
    },
    {
      "epoch": 0.46042019723543703,
      "grad_norm": 17.890687942504883,
      "learning_rate": 1.6960683387759834e-05,
      "loss": 2.8884,
      "step": 45100
    },
    {
      "epoch": 0.46144108459072625,
      "grad_norm": 14.827680587768555,
      "learning_rate": 1.6953865872198853e-05,
      "loss": 2.9115,
      "step": 45200
    },
    {
      "epoch": 0.46246197194601546,
      "grad_norm": 16.19016456604004,
      "learning_rate": 1.6947048356637876e-05,
      "loss": 2.97,
      "step": 45300
    },
    {
      "epoch": 0.4634828593013047,
      "grad_norm": 19.190093994140625,
      "learning_rate": 1.6940230841076896e-05,
      "loss": 2.9338,
      "step": 45400
    },
    {
      "epoch": 0.4645037466565939,
      "grad_norm": 16.655487060546875,
      "learning_rate": 1.693341332551592e-05,
      "loss": 2.8742,
      "step": 45500
    },
    {
      "epoch": 0.46552463401188315,
      "grad_norm": 13.11747932434082,
      "learning_rate": 1.692659580995494e-05,
      "loss": 2.8156,
      "step": 45600
    },
    {
      "epoch": 0.46654552136717237,
      "grad_norm": 15.441630363464355,
      "learning_rate": 1.6919778294393958e-05,
      "loss": 2.8194,
      "step": 45700
    },
    {
      "epoch": 0.4675664087224616,
      "grad_norm": 20.35051727294922,
      "learning_rate": 1.6912960778832978e-05,
      "loss": 2.8745,
      "step": 45800
    },
    {
      "epoch": 0.4685872960777508,
      "grad_norm": 18.821020126342773,
      "learning_rate": 1.6906143263272e-05,
      "loss": 2.8304,
      "step": 45900
    },
    {
      "epoch": 0.46960818343304,
      "grad_norm": 15.43952751159668,
      "learning_rate": 1.689932574771102e-05,
      "loss": 2.7889,
      "step": 46000
    },
    {
      "epoch": 0.4706290707883292,
      "grad_norm": 18.962989807128906,
      "learning_rate": 1.6892508232150043e-05,
      "loss": 2.8064,
      "step": 46100
    },
    {
      "epoch": 0.47164995814361843,
      "grad_norm": 15.169068336486816,
      "learning_rate": 1.6885690716589063e-05,
      "loss": 3.0135,
      "step": 46200
    },
    {
      "epoch": 0.47267084549890764,
      "grad_norm": 15.517409324645996,
      "learning_rate": 1.6878873201028082e-05,
      "loss": 2.8663,
      "step": 46300
    },
    {
      "epoch": 0.47369173285419686,
      "grad_norm": 14.419342041015625,
      "learning_rate": 1.6872055685467102e-05,
      "loss": 2.8982,
      "step": 46400
    },
    {
      "epoch": 0.47471262020948607,
      "grad_norm": 14.350824356079102,
      "learning_rate": 1.6865238169906125e-05,
      "loss": 2.9036,
      "step": 46500
    },
    {
      "epoch": 0.4757335075647753,
      "grad_norm": 15.413952827453613,
      "learning_rate": 1.6858420654345145e-05,
      "loss": 2.9619,
      "step": 46600
    },
    {
      "epoch": 0.4767543949200645,
      "grad_norm": 20.913028717041016,
      "learning_rate": 1.6851603138784164e-05,
      "loss": 2.9254,
      "step": 46700
    },
    {
      "epoch": 0.47777528227535376,
      "grad_norm": 14.993290901184082,
      "learning_rate": 1.6844785623223184e-05,
      "loss": 2.7913,
      "step": 46800
    },
    {
      "epoch": 0.478796169630643,
      "grad_norm": 19.864383697509766,
      "learning_rate": 1.6837968107662207e-05,
      "loss": 2.8672,
      "step": 46900
    },
    {
      "epoch": 0.4798170569859322,
      "grad_norm": 14.030213356018066,
      "learning_rate": 1.683115059210123e-05,
      "loss": 2.8903,
      "step": 47000
    },
    {
      "epoch": 0.4808379443412214,
      "grad_norm": 16.286224365234375,
      "learning_rate": 1.682433307654025e-05,
      "loss": 2.7281,
      "step": 47100
    },
    {
      "epoch": 0.4818588316965106,
      "grad_norm": 12.298444747924805,
      "learning_rate": 1.681758373613488e-05,
      "loss": 2.8119,
      "step": 47200
    },
    {
      "epoch": 0.4828797190517998,
      "grad_norm": 17.927120208740234,
      "learning_rate": 1.68107662205739e-05,
      "loss": 2.8394,
      "step": 47300
    },
    {
      "epoch": 0.48390060640708904,
      "grad_norm": 15.509214401245117,
      "learning_rate": 1.680394870501292e-05,
      "loss": 2.824,
      "step": 47400
    },
    {
      "epoch": 0.48492149376237825,
      "grad_norm": 15.786747932434082,
      "learning_rate": 1.679713118945194e-05,
      "loss": 2.886,
      "step": 47500
    },
    {
      "epoch": 0.48594238111766747,
      "grad_norm": 12.411942481994629,
      "learning_rate": 1.6790313673890963e-05,
      "loss": 2.9185,
      "step": 47600
    },
    {
      "epoch": 0.4869632684729567,
      "grad_norm": 18.78854751586914,
      "learning_rate": 1.6783496158329982e-05,
      "loss": 2.8586,
      "step": 47700
    },
    {
      "epoch": 0.4879841558282459,
      "grad_norm": 26.70046615600586,
      "learning_rate": 1.6776678642769005e-05,
      "loss": 2.8077,
      "step": 47800
    },
    {
      "epoch": 0.4890050431835351,
      "grad_norm": 14.259395599365234,
      "learning_rate": 1.6769861127208025e-05,
      "loss": 2.7931,
      "step": 47900
    },
    {
      "epoch": 0.4900259305388244,
      "grad_norm": 17.409923553466797,
      "learning_rate": 1.6763043611647044e-05,
      "loss": 2.8624,
      "step": 48000
    },
    {
      "epoch": 0.4910468178941136,
      "grad_norm": 12.816615104675293,
      "learning_rate": 1.6756226096086064e-05,
      "loss": 2.8367,
      "step": 48100
    },
    {
      "epoch": 0.4920677052494028,
      "grad_norm": 14.577905654907227,
      "learning_rate": 1.6749408580525087e-05,
      "loss": 2.8621,
      "step": 48200
    },
    {
      "epoch": 0.493088592604692,
      "grad_norm": 18.182086944580078,
      "learning_rate": 1.6742591064964107e-05,
      "loss": 2.8734,
      "step": 48300
    },
    {
      "epoch": 0.4941094799599812,
      "grad_norm": 17.03827476501465,
      "learning_rate": 1.6735773549403126e-05,
      "loss": 2.8469,
      "step": 48400
    },
    {
      "epoch": 0.49513036731527044,
      "grad_norm": 18.05947494506836,
      "learning_rate": 1.672895603384215e-05,
      "loss": 2.9468,
      "step": 48500
    },
    {
      "epoch": 0.49615125467055965,
      "grad_norm": 16.618453979492188,
      "learning_rate": 1.672213851828117e-05,
      "loss": 2.8203,
      "step": 48600
    },
    {
      "epoch": 0.49717214202584886,
      "grad_norm": 13.269350051879883,
      "learning_rate": 1.6715321002720192e-05,
      "loss": 2.8809,
      "step": 48700
    },
    {
      "epoch": 0.4981930293811381,
      "grad_norm": 19.006052017211914,
      "learning_rate": 1.670850348715921e-05,
      "loss": 2.9104,
      "step": 48800
    },
    {
      "epoch": 0.4992139167364273,
      "grad_norm": 19.142831802368164,
      "learning_rate": 1.670168597159823e-05,
      "loss": 2.8096,
      "step": 48900
    },
    {
      "epoch": 0.5002348040917165,
      "grad_norm": 15.508299827575684,
      "learning_rate": 1.669486845603725e-05,
      "loss": 2.7936,
      "step": 49000
    },
    {
      "epoch": 0.5012556914470058,
      "grad_norm": 19.475372314453125,
      "learning_rate": 1.6688050940476274e-05,
      "loss": 2.8894,
      "step": 49100
    },
    {
      "epoch": 0.5022765788022949,
      "grad_norm": 17.141847610473633,
      "learning_rate": 1.6681233424915293e-05,
      "loss": 2.9493,
      "step": 49200
    },
    {
      "epoch": 0.5032974661575842,
      "grad_norm": 20.905773162841797,
      "learning_rate": 1.6674415909354313e-05,
      "loss": 2.8899,
      "step": 49300
    },
    {
      "epoch": 0.5043183535128734,
      "grad_norm": 15.383978843688965,
      "learning_rate": 1.6667598393793336e-05,
      "loss": 2.8617,
      "step": 49400
    },
    {
      "epoch": 0.5053392408681626,
      "grad_norm": 14.473283767700195,
      "learning_rate": 1.6660849053387967e-05,
      "loss": 2.8553,
      "step": 49500
    },
    {
      "epoch": 0.5063601282234518,
      "grad_norm": 17.928279876708984,
      "learning_rate": 1.6654031537826987e-05,
      "loss": 2.796,
      "step": 49600
    },
    {
      "epoch": 0.507381015578741,
      "grad_norm": 16.993179321289062,
      "learning_rate": 1.6647214022266006e-05,
      "loss": 2.8961,
      "step": 49700
    },
    {
      "epoch": 0.5084019029340303,
      "grad_norm": 20.37748908996582,
      "learning_rate": 1.664039650670503e-05,
      "loss": 2.8257,
      "step": 49800
    },
    {
      "epoch": 0.5094227902893195,
      "grad_norm": 16.551958084106445,
      "learning_rate": 1.663357899114405e-05,
      "loss": 2.873,
      "step": 49900
    },
    {
      "epoch": 0.5104436776446087,
      "grad_norm": 13.874126434326172,
      "learning_rate": 1.662676147558307e-05,
      "loss": 2.8763,
      "step": 50000
    },
    {
      "epoch": 0.5114645649998979,
      "grad_norm": 20.315439224243164,
      "learning_rate": 1.6619943960022088e-05,
      "loss": 2.8733,
      "step": 50100
    },
    {
      "epoch": 0.5124854523551872,
      "grad_norm": 15.05618953704834,
      "learning_rate": 1.661312644446111e-05,
      "loss": 2.8761,
      "step": 50200
    },
    {
      "epoch": 0.5135063397104763,
      "grad_norm": 13.958582878112793,
      "learning_rate": 1.660630892890013e-05,
      "loss": 2.8101,
      "step": 50300
    },
    {
      "epoch": 0.5145272270657656,
      "grad_norm": 17.60342025756836,
      "learning_rate": 1.6599491413339154e-05,
      "loss": 2.9273,
      "step": 50400
    },
    {
      "epoch": 0.5155481144210547,
      "grad_norm": 18.282920837402344,
      "learning_rate": 1.6592673897778173e-05,
      "loss": 2.8219,
      "step": 50500
    },
    {
      "epoch": 0.516569001776344,
      "grad_norm": 15.552337646484375,
      "learning_rate": 1.6585856382217193e-05,
      "loss": 2.8419,
      "step": 50600
    },
    {
      "epoch": 0.5175898891316332,
      "grad_norm": 12.53076457977295,
      "learning_rate": 1.6579038866656213e-05,
      "loss": 2.896,
      "step": 50700
    },
    {
      "epoch": 0.5186107764869224,
      "grad_norm": 15.423718452453613,
      "learning_rate": 1.6572221351095236e-05,
      "loss": 2.8646,
      "step": 50800
    },
    {
      "epoch": 0.5196316638422116,
      "grad_norm": 15.637558937072754,
      "learning_rate": 1.6565403835534255e-05,
      "loss": 2.7895,
      "step": 50900
    },
    {
      "epoch": 0.5206525511975009,
      "grad_norm": 15.072954177856445,
      "learning_rate": 1.655858631997328e-05,
      "loss": 2.8784,
      "step": 51000
    },
    {
      "epoch": 0.5216734385527901,
      "grad_norm": 19.522489547729492,
      "learning_rate": 1.6551768804412298e-05,
      "loss": 2.9507,
      "step": 51100
    },
    {
      "epoch": 0.5226943259080793,
      "grad_norm": 16.26158905029297,
      "learning_rate": 1.6544951288851318e-05,
      "loss": 2.8169,
      "step": 51200
    },
    {
      "epoch": 0.5237152132633686,
      "grad_norm": 17.075279235839844,
      "learning_rate": 1.653813377329034e-05,
      "loss": 2.8441,
      "step": 51300
    },
    {
      "epoch": 0.5247361006186577,
      "grad_norm": 15.36861515045166,
      "learning_rate": 1.653131625772936e-05,
      "loss": 2.9473,
      "step": 51400
    },
    {
      "epoch": 0.525756987973947,
      "grad_norm": 18.222625732421875,
      "learning_rate": 1.652449874216838e-05,
      "loss": 2.88,
      "step": 51500
    },
    {
      "epoch": 0.5267778753292361,
      "grad_norm": 16.489404678344727,
      "learning_rate": 1.65176812266074e-05,
      "loss": 2.7934,
      "step": 51600
    },
    {
      "epoch": 0.5277987626845254,
      "grad_norm": 17.466493606567383,
      "learning_rate": 1.6510863711046422e-05,
      "loss": 2.8974,
      "step": 51700
    },
    {
      "epoch": 0.5288196500398146,
      "grad_norm": 16.734798431396484,
      "learning_rate": 1.6504046195485442e-05,
      "loss": 2.8745,
      "step": 51800
    },
    {
      "epoch": 0.5298405373951038,
      "grad_norm": 15.517805099487305,
      "learning_rate": 1.6497228679924465e-05,
      "loss": 2.8652,
      "step": 51900
    },
    {
      "epoch": 0.530861424750393,
      "grad_norm": 14.223600387573242,
      "learning_rate": 1.6490411164363485e-05,
      "loss": 2.8445,
      "step": 52000
    },
    {
      "epoch": 0.5318823121056823,
      "grad_norm": 20.32221794128418,
      "learning_rate": 1.6483593648802504e-05,
      "loss": 2.8272,
      "step": 52100
    },
    {
      "epoch": 0.5329031994609714,
      "grad_norm": 13.517822265625,
      "learning_rate": 1.6476844308397135e-05,
      "loss": 2.8184,
      "step": 52200
    },
    {
      "epoch": 0.5339240868162607,
      "grad_norm": 16.33116912841797,
      "learning_rate": 1.6470026792836155e-05,
      "loss": 2.8362,
      "step": 52300
    },
    {
      "epoch": 0.53494497417155,
      "grad_norm": 17.8441162109375,
      "learning_rate": 1.6463209277275175e-05,
      "loss": 2.8227,
      "step": 52400
    },
    {
      "epoch": 0.5359658615268391,
      "grad_norm": 19.841419219970703,
      "learning_rate": 1.6456391761714198e-05,
      "loss": 2.8706,
      "step": 52500
    },
    {
      "epoch": 0.5369867488821284,
      "grad_norm": 18.651697158813477,
      "learning_rate": 1.644957424615322e-05,
      "loss": 2.8362,
      "step": 52600
    },
    {
      "epoch": 0.5380076362374175,
      "grad_norm": 18.37090301513672,
      "learning_rate": 1.644275673059224e-05,
      "loss": 2.8549,
      "step": 52700
    },
    {
      "epoch": 0.5390285235927068,
      "grad_norm": 18.9202938079834,
      "learning_rate": 1.643593921503126e-05,
      "loss": 2.7692,
      "step": 52800
    },
    {
      "epoch": 0.540049410947996,
      "grad_norm": 14.20426082611084,
      "learning_rate": 1.642912169947028e-05,
      "loss": 2.9305,
      "step": 52900
    },
    {
      "epoch": 0.5410702983032852,
      "grad_norm": 16.906097412109375,
      "learning_rate": 1.6422304183909302e-05,
      "loss": 2.7767,
      "step": 53000
    },
    {
      "epoch": 0.5420911856585744,
      "grad_norm": 16.51059913635254,
      "learning_rate": 1.6415486668348322e-05,
      "loss": 2.8307,
      "step": 53100
    },
    {
      "epoch": 0.5431120730138637,
      "grad_norm": 12.543400764465332,
      "learning_rate": 1.6408669152787342e-05,
      "loss": 2.8717,
      "step": 53200
    },
    {
      "epoch": 0.5441329603691528,
      "grad_norm": 16.00010108947754,
      "learning_rate": 1.640185163722636e-05,
      "loss": 2.8513,
      "step": 53300
    },
    {
      "epoch": 0.5451538477244421,
      "grad_norm": 19.054311752319336,
      "learning_rate": 1.6395034121665384e-05,
      "loss": 2.8004,
      "step": 53400
    },
    {
      "epoch": 0.5461747350797314,
      "grad_norm": 18.921329498291016,
      "learning_rate": 1.6388216606104407e-05,
      "loss": 2.8608,
      "step": 53500
    },
    {
      "epoch": 0.5471956224350205,
      "grad_norm": 21.528928756713867,
      "learning_rate": 1.6381399090543427e-05,
      "loss": 2.8268,
      "step": 53600
    },
    {
      "epoch": 0.5482165097903098,
      "grad_norm": 17.034015655517578,
      "learning_rate": 1.6374581574982447e-05,
      "loss": 2.8889,
      "step": 53700
    },
    {
      "epoch": 0.5492373971455989,
      "grad_norm": 17.458253860473633,
      "learning_rate": 1.6367764059421466e-05,
      "loss": 2.879,
      "step": 53800
    },
    {
      "epoch": 0.5502582845008882,
      "grad_norm": 16.72980308532715,
      "learning_rate": 1.636094654386049e-05,
      "loss": 2.8373,
      "step": 53900
    },
    {
      "epoch": 0.5512791718561774,
      "grad_norm": 17.617443084716797,
      "learning_rate": 1.635412902829951e-05,
      "loss": 2.828,
      "step": 54000
    },
    {
      "epoch": 0.5523000592114666,
      "grad_norm": 17.028223037719727,
      "learning_rate": 1.634731151273853e-05,
      "loss": 2.8705,
      "step": 54100
    },
    {
      "epoch": 0.5533209465667558,
      "grad_norm": 13.147368431091309,
      "learning_rate": 1.6340493997177548e-05,
      "loss": 2.8381,
      "step": 54200
    },
    {
      "epoch": 0.5543418339220451,
      "grad_norm": 14.69131851196289,
      "learning_rate": 1.633367648161657e-05,
      "loss": 2.8067,
      "step": 54300
    },
    {
      "epoch": 0.5553627212773342,
      "grad_norm": 14.52851390838623,
      "learning_rate": 1.632685896605559e-05,
      "loss": 2.8537,
      "step": 54400
    },
    {
      "epoch": 0.5563836086326235,
      "grad_norm": 13.270715713500977,
      "learning_rate": 1.6320041450494614e-05,
      "loss": 2.9071,
      "step": 54500
    },
    {
      "epoch": 0.5574044959879126,
      "grad_norm": 14.314715385437012,
      "learning_rate": 1.6313223934933633e-05,
      "loss": 2.8206,
      "step": 54600
    },
    {
      "epoch": 0.5584253833432019,
      "grad_norm": 13.18354320526123,
      "learning_rate": 1.6306406419372653e-05,
      "loss": 2.7994,
      "step": 54700
    },
    {
      "epoch": 0.5594462706984912,
      "grad_norm": 14.937588691711426,
      "learning_rate": 1.6299588903811673e-05,
      "loss": 2.9002,
      "step": 54800
    },
    {
      "epoch": 0.5604671580537803,
      "grad_norm": 14.258023262023926,
      "learning_rate": 1.6292771388250696e-05,
      "loss": 2.9898,
      "step": 54900
    },
    {
      "epoch": 0.5614880454090696,
      "grad_norm": 16.20225715637207,
      "learning_rate": 1.6285953872689715e-05,
      "loss": 2.8324,
      "step": 55000
    },
    {
      "epoch": 0.5625089327643588,
      "grad_norm": 18.244815826416016,
      "learning_rate": 1.6279136357128738e-05,
      "loss": 2.7417,
      "step": 55100
    },
    {
      "epoch": 0.563529820119648,
      "grad_norm": 17.409255981445312,
      "learning_rate": 1.6272318841567758e-05,
      "loss": 2.8929,
      "step": 55200
    },
    {
      "epoch": 0.5645507074749372,
      "grad_norm": 15.240727424621582,
      "learning_rate": 1.6265501326006777e-05,
      "loss": 2.7815,
      "step": 55300
    },
    {
      "epoch": 0.5655715948302265,
      "grad_norm": 16.21141242980957,
      "learning_rate": 1.62586838104458e-05,
      "loss": 2.8258,
      "step": 55400
    },
    {
      "epoch": 0.5665924821855156,
      "grad_norm": 20.58622169494629,
      "learning_rate": 1.625186629488482e-05,
      "loss": 2.8288,
      "step": 55500
    },
    {
      "epoch": 0.5676133695408049,
      "grad_norm": 11.969764709472656,
      "learning_rate": 1.624504877932384e-05,
      "loss": 2.7537,
      "step": 55600
    },
    {
      "epoch": 0.568634256896094,
      "grad_norm": 16.89133071899414,
      "learning_rate": 1.623823126376286e-05,
      "loss": 2.9347,
      "step": 55700
    },
    {
      "epoch": 0.5696551442513833,
      "grad_norm": 16.7518253326416,
      "learning_rate": 1.6231413748201882e-05,
      "loss": 2.8425,
      "step": 55800
    },
    {
      "epoch": 0.5706760316066726,
      "grad_norm": 16.595102310180664,
      "learning_rate": 1.6224596232640902e-05,
      "loss": 2.8525,
      "step": 55900
    },
    {
      "epoch": 0.5716969189619617,
      "grad_norm": 14.64725399017334,
      "learning_rate": 1.6217778717079925e-05,
      "loss": 2.7813,
      "step": 56000
    },
    {
      "epoch": 0.572717806317251,
      "grad_norm": 21.061248779296875,
      "learning_rate": 1.6210961201518945e-05,
      "loss": 2.8461,
      "step": 56100
    },
    {
      "epoch": 0.5737386936725402,
      "grad_norm": 17.862957000732422,
      "learning_rate": 1.6204211861113576e-05,
      "loss": 2.8284,
      "step": 56200
    },
    {
      "epoch": 0.5747595810278294,
      "grad_norm": 20.92426300048828,
      "learning_rate": 1.6197394345552595e-05,
      "loss": 2.8985,
      "step": 56300
    },
    {
      "epoch": 0.5757804683831186,
      "grad_norm": 16.46223258972168,
      "learning_rate": 1.6190576829991615e-05,
      "loss": 2.8061,
      "step": 56400
    },
    {
      "epoch": 0.5768013557384079,
      "grad_norm": 18.80734634399414,
      "learning_rate": 1.6183759314430634e-05,
      "loss": 2.9094,
      "step": 56500
    },
    {
      "epoch": 0.577822243093697,
      "grad_norm": 19.6870059967041,
      "learning_rate": 1.6176941798869657e-05,
      "loss": 2.8425,
      "step": 56600
    },
    {
      "epoch": 0.5788431304489863,
      "grad_norm": 12.508328437805176,
      "learning_rate": 1.6170124283308677e-05,
      "loss": 2.7482,
      "step": 56700
    },
    {
      "epoch": 0.5798640178042754,
      "grad_norm": 18.455642700195312,
      "learning_rate": 1.61633067677477e-05,
      "loss": 2.844,
      "step": 56800
    },
    {
      "epoch": 0.5808849051595647,
      "grad_norm": 20.32497787475586,
      "learning_rate": 1.615648925218672e-05,
      "loss": 2.9029,
      "step": 56900
    },
    {
      "epoch": 0.5819057925148539,
      "grad_norm": 15.091113090515137,
      "learning_rate": 1.614967173662574e-05,
      "loss": 2.8457,
      "step": 57000
    },
    {
      "epoch": 0.5829266798701431,
      "grad_norm": 18.187084197998047,
      "learning_rate": 1.6142854221064762e-05,
      "loss": 2.7715,
      "step": 57100
    },
    {
      "epoch": 0.5839475672254324,
      "grad_norm": 14.324213981628418,
      "learning_rate": 1.6136036705503782e-05,
      "loss": 2.8419,
      "step": 57200
    },
    {
      "epoch": 0.5849684545807216,
      "grad_norm": 16.893404006958008,
      "learning_rate": 1.61292191899428e-05,
      "loss": 2.8574,
      "step": 57300
    },
    {
      "epoch": 0.5859893419360108,
      "grad_norm": 18.683544158935547,
      "learning_rate": 1.612240167438182e-05,
      "loss": 2.8341,
      "step": 57400
    },
    {
      "epoch": 0.5870102292913,
      "grad_norm": 15.469090461730957,
      "learning_rate": 1.6115584158820844e-05,
      "loss": 2.8365,
      "step": 57500
    },
    {
      "epoch": 0.5880311166465892,
      "grad_norm": 18.175813674926758,
      "learning_rate": 1.6108766643259864e-05,
      "loss": 2.8197,
      "step": 57600
    },
    {
      "epoch": 0.5890520040018784,
      "grad_norm": 16.817445755004883,
      "learning_rate": 1.6101949127698887e-05,
      "loss": 2.768,
      "step": 57700
    },
    {
      "epoch": 0.5900728913571677,
      "grad_norm": 15.818990707397461,
      "learning_rate": 1.6095131612137906e-05,
      "loss": 2.7706,
      "step": 57800
    },
    {
      "epoch": 0.5910937787124568,
      "grad_norm": 19.577587127685547,
      "learning_rate": 1.6088314096576926e-05,
      "loss": 2.7666,
      "step": 57900
    },
    {
      "epoch": 0.5921146660677461,
      "grad_norm": 19.78089141845703,
      "learning_rate": 1.6081564756171557e-05,
      "loss": 2.7614,
      "step": 58000
    },
    {
      "epoch": 0.5931355534230353,
      "grad_norm": 15.852368354797363,
      "learning_rate": 1.6074747240610577e-05,
      "loss": 2.7964,
      "step": 58100
    },
    {
      "epoch": 0.5941564407783245,
      "grad_norm": 17.627809524536133,
      "learning_rate": 1.6067929725049596e-05,
      "loss": 2.8119,
      "step": 58200
    },
    {
      "epoch": 0.5951773281336138,
      "grad_norm": 15.213300704956055,
      "learning_rate": 1.606111220948862e-05,
      "loss": 2.9221,
      "step": 58300
    },
    {
      "epoch": 0.596198215488903,
      "grad_norm": 13.732583045959473,
      "learning_rate": 1.6054294693927642e-05,
      "loss": 2.8166,
      "step": 58400
    },
    {
      "epoch": 0.5972191028441922,
      "grad_norm": 15.70589542388916,
      "learning_rate": 1.604754535352227e-05,
      "loss": 2.8287,
      "step": 58500
    },
    {
      "epoch": 0.5982399901994814,
      "grad_norm": 15.370576858520508,
      "learning_rate": 1.6040727837961293e-05,
      "loss": 2.7325,
      "step": 58600
    },
    {
      "epoch": 0.5992608775547706,
      "grad_norm": 17.03598403930664,
      "learning_rate": 1.6033910322400313e-05,
      "loss": 2.8553,
      "step": 58700
    },
    {
      "epoch": 0.6002817649100598,
      "grad_norm": 15.21975040435791,
      "learning_rate": 1.6027092806839332e-05,
      "loss": 2.7645,
      "step": 58800
    },
    {
      "epoch": 0.6013026522653491,
      "grad_norm": 15.824531555175781,
      "learning_rate": 1.6020275291278352e-05,
      "loss": 2.842,
      "step": 58900
    },
    {
      "epoch": 0.6023235396206382,
      "grad_norm": 14.595128059387207,
      "learning_rate": 1.6013457775717375e-05,
      "loss": 2.8499,
      "step": 59000
    },
    {
      "epoch": 0.6033444269759275,
      "grad_norm": 21.274436950683594,
      "learning_rate": 1.6006640260156395e-05,
      "loss": 2.8615,
      "step": 59100
    },
    {
      "epoch": 0.6043653143312167,
      "grad_norm": 19.097219467163086,
      "learning_rate": 1.5999822744595418e-05,
      "loss": 2.7924,
      "step": 59200
    },
    {
      "epoch": 0.6053862016865059,
      "grad_norm": 18.009292602539062,
      "learning_rate": 1.5993005229034437e-05,
      "loss": 2.868,
      "step": 59300
    },
    {
      "epoch": 0.6064070890417951,
      "grad_norm": 13.965327262878418,
      "learning_rate": 1.5986187713473457e-05,
      "loss": 2.8768,
      "step": 59400
    },
    {
      "epoch": 0.6074279763970843,
      "grad_norm": 17.976102828979492,
      "learning_rate": 1.597937019791248e-05,
      "loss": 2.8346,
      "step": 59500
    },
    {
      "epoch": 0.6084488637523736,
      "grad_norm": 21.188596725463867,
      "learning_rate": 1.59725526823515e-05,
      "loss": 2.8264,
      "step": 59600
    },
    {
      "epoch": 0.6094697511076628,
      "grad_norm": 14.973579406738281,
      "learning_rate": 1.596573516679052e-05,
      "loss": 2.8723,
      "step": 59700
    },
    {
      "epoch": 0.610490638462952,
      "grad_norm": 21.539295196533203,
      "learning_rate": 1.595891765122954e-05,
      "loss": 2.8562,
      "step": 59800
    },
    {
      "epoch": 0.6115115258182412,
      "grad_norm": 17.056291580200195,
      "learning_rate": 1.5952100135668562e-05,
      "loss": 2.7294,
      "step": 59900
    },
    {
      "epoch": 0.6125324131735305,
      "grad_norm": 14.561790466308594,
      "learning_rate": 1.594528262010758e-05,
      "loss": 2.8132,
      "step": 60000
    },
    {
      "epoch": 0.6135533005288196,
      "grad_norm": 17.256450653076172,
      "learning_rate": 1.5938465104546604e-05,
      "loss": 2.9185,
      "step": 60100
    },
    {
      "epoch": 0.6145741878841089,
      "grad_norm": 14.18523120880127,
      "learning_rate": 1.5931647588985624e-05,
      "loss": 2.7657,
      "step": 60200
    },
    {
      "epoch": 0.615595075239398,
      "grad_norm": 17.131315231323242,
      "learning_rate": 1.5924830073424644e-05,
      "loss": 2.7858,
      "step": 60300
    },
    {
      "epoch": 0.6166159625946873,
      "grad_norm": 18.072826385498047,
      "learning_rate": 1.5918012557863663e-05,
      "loss": 2.7212,
      "step": 60400
    },
    {
      "epoch": 0.6176368499499765,
      "grad_norm": 17.717100143432617,
      "learning_rate": 1.5911195042302686e-05,
      "loss": 2.7925,
      "step": 60500
    },
    {
      "epoch": 0.6186577373052657,
      "grad_norm": 15.375275611877441,
      "learning_rate": 1.5904377526741706e-05,
      "loss": 2.8995,
      "step": 60600
    },
    {
      "epoch": 0.6196786246605549,
      "grad_norm": 17.661571502685547,
      "learning_rate": 1.5897560011180725e-05,
      "loss": 2.735,
      "step": 60700
    },
    {
      "epoch": 0.6206995120158442,
      "grad_norm": 15.187737464904785,
      "learning_rate": 1.5890742495619745e-05,
      "loss": 2.77,
      "step": 60800
    },
    {
      "epoch": 0.6217203993711334,
      "grad_norm": 14.883070945739746,
      "learning_rate": 1.5883924980058768e-05,
      "loss": 2.757,
      "step": 60900
    },
    {
      "epoch": 0.6227412867264226,
      "grad_norm": 15.530497550964355,
      "learning_rate": 1.58771756396534e-05,
      "loss": 2.7488,
      "step": 61000
    },
    {
      "epoch": 0.6237621740817119,
      "grad_norm": 16.48181915283203,
      "learning_rate": 1.587035812409242e-05,
      "loss": 2.8204,
      "step": 61100
    },
    {
      "epoch": 0.624783061437001,
      "grad_norm": 12.201286315917969,
      "learning_rate": 1.5863540608531442e-05,
      "loss": 2.8095,
      "step": 61200
    },
    {
      "epoch": 0.6258039487922903,
      "grad_norm": 13.49111557006836,
      "learning_rate": 1.585672309297046e-05,
      "loss": 2.7891,
      "step": 61300
    },
    {
      "epoch": 0.6268248361475794,
      "grad_norm": 15.377950668334961,
      "learning_rate": 1.584990557740948e-05,
      "loss": 2.8183,
      "step": 61400
    },
    {
      "epoch": 0.6278457235028687,
      "grad_norm": 13.275126457214355,
      "learning_rate": 1.58430880618485e-05,
      "loss": 2.8307,
      "step": 61500
    },
    {
      "epoch": 0.6288666108581579,
      "grad_norm": 17.41512107849121,
      "learning_rate": 1.5836270546287524e-05,
      "loss": 2.7235,
      "step": 61600
    },
    {
      "epoch": 0.6298874982134471,
      "grad_norm": 16.745046615600586,
      "learning_rate": 1.5829453030726543e-05,
      "loss": 2.8065,
      "step": 61700
    },
    {
      "epoch": 0.6309083855687363,
      "grad_norm": 13.933331489562988,
      "learning_rate": 1.5822635515165566e-05,
      "loss": 2.8074,
      "step": 61800
    },
    {
      "epoch": 0.6319292729240256,
      "grad_norm": 15.259765625,
      "learning_rate": 1.5815817999604586e-05,
      "loss": 2.8828,
      "step": 61900
    },
    {
      "epoch": 0.6329501602793148,
      "grad_norm": 15.589700698852539,
      "learning_rate": 1.5809000484043605e-05,
      "loss": 2.7906,
      "step": 62000
    },
    {
      "epoch": 0.633971047634604,
      "grad_norm": 15.734766960144043,
      "learning_rate": 1.5802182968482625e-05,
      "loss": 2.8183,
      "step": 62100
    },
    {
      "epoch": 0.6349919349898933,
      "grad_norm": 18.986730575561523,
      "learning_rate": 1.5795365452921648e-05,
      "loss": 2.8247,
      "step": 62200
    },
    {
      "epoch": 0.6360128223451824,
      "grad_norm": 17.21930503845215,
      "learning_rate": 1.5788547937360668e-05,
      "loss": 2.8239,
      "step": 62300
    },
    {
      "epoch": 0.6370337097004717,
      "grad_norm": 12.517038345336914,
      "learning_rate": 1.5781730421799687e-05,
      "loss": 2.8205,
      "step": 62400
    },
    {
      "epoch": 0.6380545970557608,
      "grad_norm": 24.346973419189453,
      "learning_rate": 1.577491290623871e-05,
      "loss": 2.8211,
      "step": 62500
    },
    {
      "epoch": 0.6390754844110501,
      "grad_norm": 13.492459297180176,
      "learning_rate": 1.576809539067773e-05,
      "loss": 2.8532,
      "step": 62600
    },
    {
      "epoch": 0.6400963717663393,
      "grad_norm": 19.761388778686523,
      "learning_rate": 1.5761277875116753e-05,
      "loss": 2.8448,
      "step": 62700
    },
    {
      "epoch": 0.6411172591216285,
      "grad_norm": 17.254785537719727,
      "learning_rate": 1.5754460359555773e-05,
      "loss": 2.7924,
      "step": 62800
    },
    {
      "epoch": 0.6421381464769177,
      "grad_norm": 16.81206703186035,
      "learning_rate": 1.5747642843994792e-05,
      "loss": 2.7896,
      "step": 62900
    },
    {
      "epoch": 0.643159033832207,
      "grad_norm": 19.993896484375,
      "learning_rate": 1.5740825328433812e-05,
      "loss": 2.745,
      "step": 63000
    },
    {
      "epoch": 0.6441799211874961,
      "grad_norm": 14.293617248535156,
      "learning_rate": 1.5734007812872835e-05,
      "loss": 2.7855,
      "step": 63100
    },
    {
      "epoch": 0.6452008085427854,
      "grad_norm": 14.357647895812988,
      "learning_rate": 1.5727190297311854e-05,
      "loss": 2.8282,
      "step": 63200
    },
    {
      "epoch": 0.6462216958980747,
      "grad_norm": 14.293203353881836,
      "learning_rate": 1.5720372781750877e-05,
      "loss": 2.8112,
      "step": 63300
    },
    {
      "epoch": 0.6472425832533638,
      "grad_norm": 13.33574104309082,
      "learning_rate": 1.5713555266189897e-05,
      "loss": 2.8335,
      "step": 63400
    },
    {
      "epoch": 0.6482634706086531,
      "grad_norm": 18.574127197265625,
      "learning_rate": 1.5706737750628917e-05,
      "loss": 2.7985,
      "step": 63500
    },
    {
      "epoch": 0.6492843579639422,
      "grad_norm": 18.511184692382812,
      "learning_rate": 1.569992023506794e-05,
      "loss": 2.83,
      "step": 63600
    },
    {
      "epoch": 0.6503052453192315,
      "grad_norm": 18.63844108581543,
      "learning_rate": 1.569310271950696e-05,
      "loss": 2.7964,
      "step": 63700
    },
    {
      "epoch": 0.6513261326745207,
      "grad_norm": 16.804271697998047,
      "learning_rate": 1.568628520394598e-05,
      "loss": 2.8222,
      "step": 63800
    },
    {
      "epoch": 0.6523470200298099,
      "grad_norm": 18.160747528076172,
      "learning_rate": 1.5679467688385e-05,
      "loss": 2.7918,
      "step": 63900
    },
    {
      "epoch": 0.6533679073850991,
      "grad_norm": 20.033111572265625,
      "learning_rate": 1.567265017282402e-05,
      "loss": 2.8421,
      "step": 64000
    },
    {
      "epoch": 0.6543887947403884,
      "grad_norm": 12.901726722717285,
      "learning_rate": 1.566583265726304e-05,
      "loss": 2.8046,
      "step": 64100
    },
    {
      "epoch": 0.6554096820956775,
      "grad_norm": 15.724846839904785,
      "learning_rate": 1.5659015141702064e-05,
      "loss": 2.8663,
      "step": 64200
    },
    {
      "epoch": 0.6564305694509668,
      "grad_norm": 18.315353393554688,
      "learning_rate": 1.5652197626141084e-05,
      "loss": 2.7946,
      "step": 64300
    },
    {
      "epoch": 0.657451456806256,
      "grad_norm": 17.46661949157715,
      "learning_rate": 1.5645380110580103e-05,
      "loss": 2.8054,
      "step": 64400
    },
    {
      "epoch": 0.6584723441615452,
      "grad_norm": 12.727389335632324,
      "learning_rate": 1.5638562595019123e-05,
      "loss": 2.8199,
      "step": 64500
    },
    {
      "epoch": 0.6594932315168345,
      "grad_norm": 14.266151428222656,
      "learning_rate": 1.5631745079458146e-05,
      "loss": 2.7375,
      "step": 64600
    },
    {
      "epoch": 0.6605141188721236,
      "grad_norm": 13.498929977416992,
      "learning_rate": 1.5624927563897166e-05,
      "loss": 2.7228,
      "step": 64700
    },
    {
      "epoch": 0.6615350062274129,
      "grad_norm": 18.071014404296875,
      "learning_rate": 1.5618110048336185e-05,
      "loss": 2.7494,
      "step": 64800
    },
    {
      "epoch": 0.6625558935827021,
      "grad_norm": 15.365900993347168,
      "learning_rate": 1.5611292532775205e-05,
      "loss": 2.864,
      "step": 64900
    },
    {
      "epoch": 0.6635767809379913,
      "grad_norm": 15.57388687133789,
      "learning_rate": 1.5604475017214228e-05,
      "loss": 2.7704,
      "step": 65000
    },
    {
      "epoch": 0.6645976682932805,
      "grad_norm": 16.61906623840332,
      "learning_rate": 1.559765750165325e-05,
      "loss": 2.8193,
      "step": 65100
    },
    {
      "epoch": 0.6656185556485698,
      "grad_norm": 13.099658012390137,
      "learning_rate": 1.559083998609227e-05,
      "loss": 2.7988,
      "step": 65200
    },
    {
      "epoch": 0.6666394430038589,
      "grad_norm": 16.65217399597168,
      "learning_rate": 1.558402247053129e-05,
      "loss": 2.8631,
      "step": 65300
    },
    {
      "epoch": 0.6676603303591482,
      "grad_norm": 22.355167388916016,
      "learning_rate": 1.557720495497031e-05,
      "loss": 2.7924,
      "step": 65400
    },
    {
      "epoch": 0.6686812177144373,
      "grad_norm": 16.141124725341797,
      "learning_rate": 1.5570387439409333e-05,
      "loss": 2.8356,
      "step": 65500
    },
    {
      "epoch": 0.6697021050697266,
      "grad_norm": 19.70626449584961,
      "learning_rate": 1.5563569923848352e-05,
      "loss": 2.7969,
      "step": 65600
    },
    {
      "epoch": 0.6707229924250159,
      "grad_norm": 17.536643981933594,
      "learning_rate": 1.5556752408287372e-05,
      "loss": 2.878,
      "step": 65700
    },
    {
      "epoch": 0.671743879780305,
      "grad_norm": 23.77068519592285,
      "learning_rate": 1.5549934892726395e-05,
      "loss": 2.8909,
      "step": 65800
    },
    {
      "epoch": 0.6727647671355943,
      "grad_norm": 15.283015251159668,
      "learning_rate": 1.5543117377165415e-05,
      "loss": 2.8651,
      "step": 65900
    },
    {
      "epoch": 0.6737856544908835,
      "grad_norm": 16.344587326049805,
      "learning_rate": 1.5536299861604434e-05,
      "loss": 2.8287,
      "step": 66000
    },
    {
      "epoch": 0.6748065418461727,
      "grad_norm": 19.479780197143555,
      "learning_rate": 1.5529482346043457e-05,
      "loss": 2.7764,
      "step": 66100
    },
    {
      "epoch": 0.6758274292014619,
      "grad_norm": 14.98304557800293,
      "learning_rate": 1.5522664830482477e-05,
      "loss": 2.8337,
      "step": 66200
    },
    {
      "epoch": 0.6768483165567512,
      "grad_norm": 16.993261337280273,
      "learning_rate": 1.5515847314921497e-05,
      "loss": 2.7365,
      "step": 66300
    },
    {
      "epoch": 0.6778692039120403,
      "grad_norm": 14.38416576385498,
      "learning_rate": 1.5509029799360516e-05,
      "loss": 2.7675,
      "step": 66400
    },
    {
      "epoch": 0.6788900912673296,
      "grad_norm": 18.563684463500977,
      "learning_rate": 1.550221228379954e-05,
      "loss": 2.7975,
      "step": 66500
    },
    {
      "epoch": 0.6799109786226187,
      "grad_norm": 17.004127502441406,
      "learning_rate": 1.5495394768238562e-05,
      "loss": 2.8349,
      "step": 66600
    },
    {
      "epoch": 0.680931865977908,
      "grad_norm": 15.062936782836914,
      "learning_rate": 1.5488577252677582e-05,
      "loss": 2.7669,
      "step": 66700
    },
    {
      "epoch": 0.6819527533331973,
      "grad_norm": 13.187400817871094,
      "learning_rate": 1.54817597371166e-05,
      "loss": 2.7072,
      "step": 66800
    },
    {
      "epoch": 0.6829736406884864,
      "grad_norm": 16.14765739440918,
      "learning_rate": 1.547494222155562e-05,
      "loss": 2.8309,
      "step": 66900
    },
    {
      "epoch": 0.6839945280437757,
      "grad_norm": 13.778032302856445,
      "learning_rate": 1.5468124705994644e-05,
      "loss": 2.7498,
      "step": 67000
    },
    {
      "epoch": 0.6850154153990649,
      "grad_norm": 21.008224487304688,
      "learning_rate": 1.5461307190433664e-05,
      "loss": 2.8001,
      "step": 67100
    },
    {
      "epoch": 0.6860363027543541,
      "grad_norm": 16.778310775756836,
      "learning_rate": 1.5454557850028295e-05,
      "loss": 2.7506,
      "step": 67200
    },
    {
      "epoch": 0.6870571901096433,
      "grad_norm": 22.31749725341797,
      "learning_rate": 1.5447740334467314e-05,
      "loss": 2.7524,
      "step": 67300
    },
    {
      "epoch": 0.6880780774649325,
      "grad_norm": 16.72358512878418,
      "learning_rate": 1.5440922818906337e-05,
      "loss": 2.7897,
      "step": 67400
    },
    {
      "epoch": 0.6890989648202217,
      "grad_norm": 20.519025802612305,
      "learning_rate": 1.5434105303345357e-05,
      "loss": 2.718,
      "step": 67500
    },
    {
      "epoch": 0.690119852175511,
      "grad_norm": 15.034658432006836,
      "learning_rate": 1.5427287787784377e-05,
      "loss": 2.7419,
      "step": 67600
    },
    {
      "epoch": 0.6911407395308001,
      "grad_norm": 21.500253677368164,
      "learning_rate": 1.5420470272223396e-05,
      "loss": 2.7913,
      "step": 67700
    },
    {
      "epoch": 0.6921616268860894,
      "grad_norm": 16.986207962036133,
      "learning_rate": 1.541365275666242e-05,
      "loss": 2.8274,
      "step": 67800
    },
    {
      "epoch": 0.6931825142413786,
      "grad_norm": 14.422626495361328,
      "learning_rate": 1.540683524110144e-05,
      "loss": 2.8043,
      "step": 67900
    },
    {
      "epoch": 0.6942034015966678,
      "grad_norm": 26.599895477294922,
      "learning_rate": 1.540008590069607e-05,
      "loss": 2.7645,
      "step": 68000
    },
    {
      "epoch": 0.6952242889519571,
      "grad_norm": 17.57256507873535,
      "learning_rate": 1.539326838513509e-05,
      "loss": 2.778,
      "step": 68100
    },
    {
      "epoch": 0.6962451763072462,
      "grad_norm": 16.32808494567871,
      "learning_rate": 1.5386450869574113e-05,
      "loss": 2.7842,
      "step": 68200
    },
    {
      "epoch": 0.6972660636625355,
      "grad_norm": 13.312591552734375,
      "learning_rate": 1.5379633354013132e-05,
      "loss": 2.8337,
      "step": 68300
    },
    {
      "epoch": 0.6982869510178247,
      "grad_norm": 16.83256721496582,
      "learning_rate": 1.5372815838452152e-05,
      "loss": 2.7911,
      "step": 68400
    },
    {
      "epoch": 0.6993078383731139,
      "grad_norm": 13.574993133544922,
      "learning_rate": 1.5365998322891175e-05,
      "loss": 2.7914,
      "step": 68500
    },
    {
      "epoch": 0.7003287257284031,
      "grad_norm": 16.18210220336914,
      "learning_rate": 1.5359180807330194e-05,
      "loss": 2.8043,
      "step": 68600
    },
    {
      "epoch": 0.7013496130836924,
      "grad_norm": 18.719356536865234,
      "learning_rate": 1.5352363291769214e-05,
      "loss": 2.8084,
      "step": 68700
    },
    {
      "epoch": 0.7023705004389815,
      "grad_norm": 22.900318145751953,
      "learning_rate": 1.5345545776208234e-05,
      "loss": 2.7983,
      "step": 68800
    },
    {
      "epoch": 0.7033913877942708,
      "grad_norm": 19.200883865356445,
      "learning_rate": 1.5338728260647257e-05,
      "loss": 2.8087,
      "step": 68900
    },
    {
      "epoch": 0.70441227514956,
      "grad_norm": 16.925912857055664,
      "learning_rate": 1.5331910745086276e-05,
      "loss": 2.8302,
      "step": 69000
    },
    {
      "epoch": 0.7054331625048492,
      "grad_norm": 13.724746704101562,
      "learning_rate": 1.53250932295253e-05,
      "loss": 2.8364,
      "step": 69100
    },
    {
      "epoch": 0.7064540498601384,
      "grad_norm": 19.604572296142578,
      "learning_rate": 1.531827571396432e-05,
      "loss": 2.7464,
      "step": 69200
    },
    {
      "epoch": 0.7074749372154276,
      "grad_norm": 17.336244583129883,
      "learning_rate": 1.531145819840334e-05,
      "loss": 2.7214,
      "step": 69300
    },
    {
      "epoch": 0.7084958245707169,
      "grad_norm": 13.081050872802734,
      "learning_rate": 1.530464068284236e-05,
      "loss": 2.6876,
      "step": 69400
    },
    {
      "epoch": 0.7095167119260061,
      "grad_norm": 13.989113807678223,
      "learning_rate": 1.529782316728138e-05,
      "loss": 2.7904,
      "step": 69500
    },
    {
      "epoch": 0.7105375992812953,
      "grad_norm": 15.415413856506348,
      "learning_rate": 1.52910056517204e-05,
      "loss": 2.7883,
      "step": 69600
    },
    {
      "epoch": 0.7115584866365845,
      "grad_norm": 15.253498077392578,
      "learning_rate": 1.528418813615942e-05,
      "loss": 2.7495,
      "step": 69700
    },
    {
      "epoch": 0.7125793739918738,
      "grad_norm": 13.19413948059082,
      "learning_rate": 1.5277370620598443e-05,
      "loss": 2.7505,
      "step": 69800
    },
    {
      "epoch": 0.7136002613471629,
      "grad_norm": 14.3743257522583,
      "learning_rate": 1.5270553105037463e-05,
      "loss": 2.8263,
      "step": 69900
    },
    {
      "epoch": 0.7146211487024522,
      "grad_norm": 15.551652908325195,
      "learning_rate": 1.5263735589476486e-05,
      "loss": 2.7054,
      "step": 70000
    },
    {
      "epoch": 0.7156420360577413,
      "grad_norm": 15.031867027282715,
      "learning_rate": 1.5256918073915506e-05,
      "loss": 2.8656,
      "step": 70100
    },
    {
      "epoch": 0.7166629234130306,
      "grad_norm": 28.040014266967773,
      "learning_rate": 1.5250100558354525e-05,
      "loss": 2.7288,
      "step": 70200
    },
    {
      "epoch": 0.7176838107683198,
      "grad_norm": 15.365151405334473,
      "learning_rate": 1.5243283042793547e-05,
      "loss": 2.8058,
      "step": 70300
    },
    {
      "epoch": 0.718704698123609,
      "grad_norm": 13.580364227294922,
      "learning_rate": 1.5236465527232566e-05,
      "loss": 2.8412,
      "step": 70400
    },
    {
      "epoch": 0.7197255854788983,
      "grad_norm": 11.850945472717285,
      "learning_rate": 1.5229648011671587e-05,
      "loss": 2.7411,
      "step": 70500
    },
    {
      "epoch": 0.7207464728341875,
      "grad_norm": 13.617212295532227,
      "learning_rate": 1.5222830496110607e-05,
      "loss": 2.757,
      "step": 70600
    },
    {
      "epoch": 0.7217673601894767,
      "grad_norm": 24.36994743347168,
      "learning_rate": 1.521601298054963e-05,
      "loss": 2.8203,
      "step": 70700
    },
    {
      "epoch": 0.7227882475447659,
      "grad_norm": 15.562823295593262,
      "learning_rate": 1.5209195464988651e-05,
      "loss": 2.8754,
      "step": 70800
    },
    {
      "epoch": 0.7238091349000552,
      "grad_norm": 23.197416305541992,
      "learning_rate": 1.5202377949427671e-05,
      "loss": 2.8287,
      "step": 70900
    },
    {
      "epoch": 0.7248300222553443,
      "grad_norm": 13.975162506103516,
      "learning_rate": 1.5195560433866692e-05,
      "loss": 2.7483,
      "step": 71000
    },
    {
      "epoch": 0.7258509096106336,
      "grad_norm": 21.984561920166016,
      "learning_rate": 1.5188742918305712e-05,
      "loss": 2.7604,
      "step": 71100
    },
    {
      "epoch": 0.7268717969659227,
      "grad_norm": 12.685203552246094,
      "learning_rate": 1.5181925402744733e-05,
      "loss": 2.8166,
      "step": 71200
    },
    {
      "epoch": 0.727892684321212,
      "grad_norm": 16.625898361206055,
      "learning_rate": 1.5175107887183753e-05,
      "loss": 2.8258,
      "step": 71300
    },
    {
      "epoch": 0.7289135716765012,
      "grad_norm": 14.598060607910156,
      "learning_rate": 1.5168290371622774e-05,
      "loss": 2.8738,
      "step": 71400
    },
    {
      "epoch": 0.7299344590317904,
      "grad_norm": 14.467137336730957,
      "learning_rate": 1.5161472856061796e-05,
      "loss": 2.7946,
      "step": 71500
    },
    {
      "epoch": 0.7309553463870796,
      "grad_norm": 20.45441246032715,
      "learning_rate": 1.5154655340500817e-05,
      "loss": 2.8014,
      "step": 71600
    },
    {
      "epoch": 0.7319762337423689,
      "grad_norm": 26.352746963500977,
      "learning_rate": 1.5147837824939836e-05,
      "loss": 2.797,
      "step": 71700
    },
    {
      "epoch": 0.7329971210976581,
      "grad_norm": 22.3912410736084,
      "learning_rate": 1.5141020309378858e-05,
      "loss": 2.7948,
      "step": 71800
    },
    {
      "epoch": 0.7340180084529473,
      "grad_norm": 19.069759368896484,
      "learning_rate": 1.5134202793817877e-05,
      "loss": 2.8476,
      "step": 71900
    },
    {
      "epoch": 0.7350388958082366,
      "grad_norm": 16.599300384521484,
      "learning_rate": 1.5127385278256899e-05,
      "loss": 2.7866,
      "step": 72000
    },
    {
      "epoch": 0.7360597831635257,
      "grad_norm": 14.092479705810547,
      "learning_rate": 1.5120567762695918e-05,
      "loss": 2.8055,
      "step": 72100
    },
    {
      "epoch": 0.737080670518815,
      "grad_norm": 11.893414497375488,
      "learning_rate": 1.511381842229055e-05,
      "loss": 2.7828,
      "step": 72200
    },
    {
      "epoch": 0.7381015578741041,
      "grad_norm": 17.00248146057129,
      "learning_rate": 1.5107000906729569e-05,
      "loss": 2.7954,
      "step": 72300
    },
    {
      "epoch": 0.7391224452293934,
      "grad_norm": 25.161046981811523,
      "learning_rate": 1.5100183391168592e-05,
      "loss": 2.8051,
      "step": 72400
    },
    {
      "epoch": 0.7401433325846826,
      "grad_norm": 16.545270919799805,
      "learning_rate": 1.5093365875607613e-05,
      "loss": 2.7006,
      "step": 72500
    },
    {
      "epoch": 0.7411642199399718,
      "grad_norm": 29.180763244628906,
      "learning_rate": 1.5086548360046633e-05,
      "loss": 2.7831,
      "step": 72600
    },
    {
      "epoch": 0.742185107295261,
      "grad_norm": 19.020898818969727,
      "learning_rate": 1.5079730844485654e-05,
      "loss": 2.7903,
      "step": 72700
    },
    {
      "epoch": 0.7432059946505503,
      "grad_norm": 17.25710105895996,
      "learning_rate": 1.5072913328924674e-05,
      "loss": 2.745,
      "step": 72800
    },
    {
      "epoch": 0.7442268820058395,
      "grad_norm": 19.495990753173828,
      "learning_rate": 1.5066095813363695e-05,
      "loss": 2.7134,
      "step": 72900
    },
    {
      "epoch": 0.7452477693611287,
      "grad_norm": 18.877880096435547,
      "learning_rate": 1.5059278297802715e-05,
      "loss": 2.7834,
      "step": 73000
    },
    {
      "epoch": 0.746268656716418,
      "grad_norm": 16.088558197021484,
      "learning_rate": 1.5052460782241736e-05,
      "loss": 2.7753,
      "step": 73100
    },
    {
      "epoch": 0.7472895440717071,
      "grad_norm": 13.975948333740234,
      "learning_rate": 1.5045643266680759e-05,
      "loss": 2.759,
      "step": 73200
    },
    {
      "epoch": 0.7483104314269964,
      "grad_norm": 21.83111572265625,
      "learning_rate": 1.5038825751119779e-05,
      "loss": 2.7068,
      "step": 73300
    },
    {
      "epoch": 0.7493313187822855,
      "grad_norm": 18.28260040283203,
      "learning_rate": 1.50320082355588e-05,
      "loss": 2.8639,
      "step": 73400
    },
    {
      "epoch": 0.7503522061375748,
      "grad_norm": 17.947689056396484,
      "learning_rate": 1.502519071999782e-05,
      "loss": 2.8241,
      "step": 73500
    },
    {
      "epoch": 0.751373093492864,
      "grad_norm": 17.87350845336914,
      "learning_rate": 1.501844137959245e-05,
      "loss": 2.711,
      "step": 73600
    },
    {
      "epoch": 0.7523939808481532,
      "grad_norm": 15.528623580932617,
      "learning_rate": 1.501162386403147e-05,
      "loss": 2.764,
      "step": 73700
    },
    {
      "epoch": 0.7534148682034424,
      "grad_norm": 13.1683988571167,
      "learning_rate": 1.5004806348470492e-05,
      "loss": 2.9179,
      "step": 73800
    },
    {
      "epoch": 0.7544357555587317,
      "grad_norm": 18.336116790771484,
      "learning_rate": 1.4997988832909511e-05,
      "loss": 2.7872,
      "step": 73900
    },
    {
      "epoch": 0.7554566429140208,
      "grad_norm": 17.345853805541992,
      "learning_rate": 1.4991171317348534e-05,
      "loss": 2.7531,
      "step": 74000
    },
    {
      "epoch": 0.7564775302693101,
      "grad_norm": 23.27149200439453,
      "learning_rate": 1.4984353801787554e-05,
      "loss": 2.8037,
      "step": 74100
    },
    {
      "epoch": 0.7574984176245994,
      "grad_norm": 15.59054183959961,
      "learning_rate": 1.4977536286226575e-05,
      "loss": 2.7882,
      "step": 74200
    },
    {
      "epoch": 0.7585193049798885,
      "grad_norm": 17.117307662963867,
      "learning_rate": 1.4970718770665595e-05,
      "loss": 2.7841,
      "step": 74300
    },
    {
      "epoch": 0.7595401923351778,
      "grad_norm": 16.702617645263672,
      "learning_rate": 1.4963901255104616e-05,
      "loss": 2.83,
      "step": 74400
    },
    {
      "epoch": 0.7605610796904669,
      "grad_norm": 14.03164291381836,
      "learning_rate": 1.4957083739543636e-05,
      "loss": 2.689,
      "step": 74500
    },
    {
      "epoch": 0.7615819670457562,
      "grad_norm": 18.377681732177734,
      "learning_rate": 1.4950266223982657e-05,
      "loss": 2.777,
      "step": 74600
    },
    {
      "epoch": 0.7626028544010454,
      "grad_norm": 15.692795753479004,
      "learning_rate": 1.4943448708421677e-05,
      "loss": 2.7427,
      "step": 74700
    },
    {
      "epoch": 0.7636237417563346,
      "grad_norm": 15.0400972366333,
      "learning_rate": 1.49366311928607e-05,
      "loss": 2.7795,
      "step": 74800
    },
    {
      "epoch": 0.7646446291116238,
      "grad_norm": 17.87969207763672,
      "learning_rate": 1.4929813677299721e-05,
      "loss": 2.6672,
      "step": 74900
    },
    {
      "epoch": 0.765665516466913,
      "grad_norm": 14.793878555297852,
      "learning_rate": 1.492299616173874e-05,
      "loss": 2.807,
      "step": 75000
    },
    {
      "epoch": 0.7666864038222022,
      "grad_norm": 16.18178939819336,
      "learning_rate": 1.4916178646177762e-05,
      "loss": 2.7585,
      "step": 75100
    },
    {
      "epoch": 0.7677072911774915,
      "grad_norm": 15.921005249023438,
      "learning_rate": 1.4909361130616782e-05,
      "loss": 2.7304,
      "step": 75200
    },
    {
      "epoch": 0.7687281785327807,
      "grad_norm": 10.503887176513672,
      "learning_rate": 1.4902543615055803e-05,
      "loss": 2.7761,
      "step": 75300
    },
    {
      "epoch": 0.7697490658880699,
      "grad_norm": 16.459976196289062,
      "learning_rate": 1.4895726099494823e-05,
      "loss": 2.9012,
      "step": 75400
    },
    {
      "epoch": 0.7707699532433592,
      "grad_norm": 19.79732894897461,
      "learning_rate": 1.4888908583933844e-05,
      "loss": 2.7255,
      "step": 75500
    },
    {
      "epoch": 0.7717908405986483,
      "grad_norm": 16.343000411987305,
      "learning_rate": 1.4882091068372865e-05,
      "loss": 2.7963,
      "step": 75600
    },
    {
      "epoch": 0.7728117279539376,
      "grad_norm": 17.00201416015625,
      "learning_rate": 1.4875273552811886e-05,
      "loss": 2.7601,
      "step": 75700
    },
    {
      "epoch": 0.7738326153092268,
      "grad_norm": 15.881599426269531,
      "learning_rate": 1.4868456037250906e-05,
      "loss": 2.8262,
      "step": 75800
    },
    {
      "epoch": 0.774853502664516,
      "grad_norm": 24.265594482421875,
      "learning_rate": 1.4861638521689927e-05,
      "loss": 2.7074,
      "step": 75900
    },
    {
      "epoch": 0.7758743900198052,
      "grad_norm": 13.591802597045898,
      "learning_rate": 1.4854821006128947e-05,
      "loss": 2.6872,
      "step": 76000
    },
    {
      "epoch": 0.7768952773750945,
      "grad_norm": 21.211729049682617,
      "learning_rate": 1.4848003490567968e-05,
      "loss": 2.7694,
      "step": 76100
    },
    {
      "epoch": 0.7779161647303836,
      "grad_norm": 15.124744415283203,
      "learning_rate": 1.4841185975006988e-05,
      "loss": 2.748,
      "step": 76200
    },
    {
      "epoch": 0.7789370520856729,
      "grad_norm": 18.482677459716797,
      "learning_rate": 1.483436845944601e-05,
      "loss": 2.7461,
      "step": 76300
    },
    {
      "epoch": 0.779957939440962,
      "grad_norm": 14.672356605529785,
      "learning_rate": 1.4827550943885029e-05,
      "loss": 2.7766,
      "step": 76400
    },
    {
      "epoch": 0.7809788267962513,
      "grad_norm": 15.628443717956543,
      "learning_rate": 1.4820733428324052e-05,
      "loss": 2.7928,
      "step": 76500
    },
    {
      "epoch": 0.7819997141515406,
      "grad_norm": 15.03225326538086,
      "learning_rate": 1.4813915912763073e-05,
      "loss": 2.7683,
      "step": 76600
    },
    {
      "epoch": 0.7830206015068297,
      "grad_norm": 17.07921028137207,
      "learning_rate": 1.4807098397202093e-05,
      "loss": 2.6953,
      "step": 76700
    },
    {
      "epoch": 0.784041488862119,
      "grad_norm": 16.298612594604492,
      "learning_rate": 1.4800280881641114e-05,
      "loss": 2.7752,
      "step": 76800
    },
    {
      "epoch": 0.7850623762174082,
      "grad_norm": 15.263204574584961,
      "learning_rate": 1.4793463366080134e-05,
      "loss": 2.8251,
      "step": 76900
    },
    {
      "epoch": 0.7860832635726974,
      "grad_norm": 13.723700523376465,
      "learning_rate": 1.4786645850519155e-05,
      "loss": 2.7791,
      "step": 77000
    },
    {
      "epoch": 0.7871041509279866,
      "grad_norm": 15.924833297729492,
      "learning_rate": 1.4779828334958175e-05,
      "loss": 2.7023,
      "step": 77100
    },
    {
      "epoch": 0.7881250382832758,
      "grad_norm": 19.486053466796875,
      "learning_rate": 1.4773010819397196e-05,
      "loss": 2.8096,
      "step": 77200
    },
    {
      "epoch": 0.789145925638565,
      "grad_norm": 15.223259925842285,
      "learning_rate": 1.4766193303836219e-05,
      "loss": 2.776,
      "step": 77300
    },
    {
      "epoch": 0.7901668129938543,
      "grad_norm": 16.452898025512695,
      "learning_rate": 1.4759375788275239e-05,
      "loss": 2.7486,
      "step": 77400
    },
    {
      "epoch": 0.7911877003491434,
      "grad_norm": 15.981109619140625,
      "learning_rate": 1.4752626447869868e-05,
      "loss": 2.816,
      "step": 77500
    },
    {
      "epoch": 0.7922085877044327,
      "grad_norm": 14.521820068359375,
      "learning_rate": 1.474580893230889e-05,
      "loss": 2.8119,
      "step": 77600
    },
    {
      "epoch": 0.7932294750597219,
      "grad_norm": 19.861387252807617,
      "learning_rate": 1.4738991416747909e-05,
      "loss": 2.7604,
      "step": 77700
    },
    {
      "epoch": 0.7942503624150111,
      "grad_norm": 18.177358627319336,
      "learning_rate": 1.473217390118693e-05,
      "loss": 2.8252,
      "step": 77800
    },
    {
      "epoch": 0.7952712497703004,
      "grad_norm": 15.675226211547852,
      "learning_rate": 1.472535638562595e-05,
      "loss": 2.7312,
      "step": 77900
    },
    {
      "epoch": 0.7962921371255895,
      "grad_norm": 19.014156341552734,
      "learning_rate": 1.4718538870064971e-05,
      "loss": 2.7091,
      "step": 78000
    },
    {
      "epoch": 0.7973130244808788,
      "grad_norm": 18.836824417114258,
      "learning_rate": 1.4711721354503994e-05,
      "loss": 2.8271,
      "step": 78100
    },
    {
      "epoch": 0.798333911836168,
      "grad_norm": 11.866348266601562,
      "learning_rate": 1.4704903838943014e-05,
      "loss": 2.671,
      "step": 78200
    },
    {
      "epoch": 0.7993547991914572,
      "grad_norm": 11.922706604003906,
      "learning_rate": 1.4698086323382035e-05,
      "loss": 2.7454,
      "step": 78300
    },
    {
      "epoch": 0.8003756865467464,
      "grad_norm": 17.40085792541504,
      "learning_rate": 1.4691268807821055e-05,
      "loss": 2.8613,
      "step": 78400
    },
    {
      "epoch": 0.8013965739020357,
      "grad_norm": 16.715734481811523,
      "learning_rate": 1.4684451292260076e-05,
      "loss": 2.7459,
      "step": 78500
    },
    {
      "epoch": 0.8024174612573248,
      "grad_norm": 11.749914169311523,
      "learning_rate": 1.4677633776699096e-05,
      "loss": 2.8418,
      "step": 78600
    },
    {
      "epoch": 0.8034383486126141,
      "grad_norm": 15.232486724853516,
      "learning_rate": 1.4670816261138117e-05,
      "loss": 2.8289,
      "step": 78700
    },
    {
      "epoch": 0.8044592359679033,
      "grad_norm": 20.232908248901367,
      "learning_rate": 1.4663998745577137e-05,
      "loss": 2.8327,
      "step": 78800
    },
    {
      "epoch": 0.8054801233231925,
      "grad_norm": 17.575042724609375,
      "learning_rate": 1.465718123001616e-05,
      "loss": 2.8241,
      "step": 78900
    },
    {
      "epoch": 0.8065010106784818,
      "grad_norm": 21.885683059692383,
      "learning_rate": 1.4650363714455181e-05,
      "loss": 2.7116,
      "step": 79000
    },
    {
      "epoch": 0.807521898033771,
      "grad_norm": 16.260759353637695,
      "learning_rate": 1.46435461988942e-05,
      "loss": 2.741,
      "step": 79100
    },
    {
      "epoch": 0.8085427853890602,
      "grad_norm": 14.39148998260498,
      "learning_rate": 1.4636728683333222e-05,
      "loss": 2.727,
      "step": 79200
    },
    {
      "epoch": 0.8095636727443494,
      "grad_norm": 16.7940673828125,
      "learning_rate": 1.4629911167772241e-05,
      "loss": 2.7182,
      "step": 79300
    },
    {
      "epoch": 0.8105845600996386,
      "grad_norm": 16.182537078857422,
      "learning_rate": 1.4623093652211263e-05,
      "loss": 2.7635,
      "step": 79400
    },
    {
      "epoch": 0.8116054474549278,
      "grad_norm": 16.43488883972168,
      "learning_rate": 1.4616276136650282e-05,
      "loss": 2.7369,
      "step": 79500
    },
    {
      "epoch": 0.8126263348102171,
      "grad_norm": 20.659692764282227,
      "learning_rate": 1.4609458621089304e-05,
      "loss": 2.7199,
      "step": 79600
    },
    {
      "epoch": 0.8136472221655062,
      "grad_norm": 15.559951782226562,
      "learning_rate": 1.4602709280683935e-05,
      "loss": 2.7597,
      "step": 79700
    },
    {
      "epoch": 0.8146681095207955,
      "grad_norm": 12.773983001708984,
      "learning_rate": 1.4595891765122956e-05,
      "loss": 2.7341,
      "step": 79800
    },
    {
      "epoch": 0.8156889968760846,
      "grad_norm": 17.489322662353516,
      "learning_rate": 1.4589074249561976e-05,
      "loss": 2.75,
      "step": 79900
    },
    {
      "epoch": 0.8167098842313739,
      "grad_norm": 17.421815872192383,
      "learning_rate": 1.4582256734000997e-05,
      "loss": 2.6872,
      "step": 80000
    },
    {
      "epoch": 0.8177307715866631,
      "grad_norm": 21.37164306640625,
      "learning_rate": 1.4575439218440017e-05,
      "loss": 2.7677,
      "step": 80100
    },
    {
      "epoch": 0.8187516589419523,
      "grad_norm": 17.913089752197266,
      "learning_rate": 1.4568621702879038e-05,
      "loss": 2.7551,
      "step": 80200
    },
    {
      "epoch": 0.8197725462972416,
      "grad_norm": 16.3402042388916,
      "learning_rate": 1.4561804187318058e-05,
      "loss": 2.7533,
      "step": 80300
    },
    {
      "epoch": 0.8207934336525308,
      "grad_norm": 13.495491981506348,
      "learning_rate": 1.4554986671757079e-05,
      "loss": 2.6766,
      "step": 80400
    },
    {
      "epoch": 0.82181432100782,
      "grad_norm": 15.03876781463623,
      "learning_rate": 1.4548169156196099e-05,
      "loss": 2.7663,
      "step": 80500
    },
    {
      "epoch": 0.8228352083631092,
      "grad_norm": 15.541868209838867,
      "learning_rate": 1.4541351640635122e-05,
      "loss": 2.72,
      "step": 80600
    },
    {
      "epoch": 0.8238560957183985,
      "grad_norm": 15.032567024230957,
      "learning_rate": 1.4534534125074143e-05,
      "loss": 2.7701,
      "step": 80700
    },
    {
      "epoch": 0.8248769830736876,
      "grad_norm": 10.63978385925293,
      "learning_rate": 1.4527716609513162e-05,
      "loss": 2.7157,
      "step": 80800
    },
    {
      "epoch": 0.8258978704289769,
      "grad_norm": 14.476566314697266,
      "learning_rate": 1.4520899093952184e-05,
      "loss": 2.7186,
      "step": 80900
    },
    {
      "epoch": 0.826918757784266,
      "grad_norm": 14.40715503692627,
      "learning_rate": 1.4514081578391203e-05,
      "loss": 2.8597,
      "step": 81000
    },
    {
      "epoch": 0.8279396451395553,
      "grad_norm": 14.351330757141113,
      "learning_rate": 1.4507264062830225e-05,
      "loss": 2.8219,
      "step": 81100
    },
    {
      "epoch": 0.8289605324948445,
      "grad_norm": 12.114312171936035,
      "learning_rate": 1.4500446547269244e-05,
      "loss": 2.711,
      "step": 81200
    },
    {
      "epoch": 0.8299814198501337,
      "grad_norm": 12.927080154418945,
      "learning_rate": 1.4493629031708266e-05,
      "loss": 2.7434,
      "step": 81300
    },
    {
      "epoch": 0.831002307205423,
      "grad_norm": 17.928152084350586,
      "learning_rate": 1.4486811516147287e-05,
      "loss": 2.7832,
      "step": 81400
    },
    {
      "epoch": 0.8320231945607122,
      "grad_norm": 15.448394775390625,
      "learning_rate": 1.4479994000586308e-05,
      "loss": 2.6771,
      "step": 81500
    },
    {
      "epoch": 0.8330440819160014,
      "grad_norm": 15.6212797164917,
      "learning_rate": 1.4473176485025328e-05,
      "loss": 2.7099,
      "step": 81600
    },
    {
      "epoch": 0.8340649692712906,
      "grad_norm": 15.91671085357666,
      "learning_rate": 1.446635896946435e-05,
      "loss": 2.7795,
      "step": 81700
    },
    {
      "epoch": 0.8350858566265799,
      "grad_norm": 15.399449348449707,
      "learning_rate": 1.4459541453903369e-05,
      "loss": 2.796,
      "step": 81800
    },
    {
      "epoch": 0.836106743981869,
      "grad_norm": 14.066574096679688,
      "learning_rate": 1.445272393834239e-05,
      "loss": 2.7949,
      "step": 81900
    },
    {
      "epoch": 0.8371276313371583,
      "grad_norm": 14.96524429321289,
      "learning_rate": 1.444590642278141e-05,
      "loss": 2.719,
      "step": 82000
    },
    {
      "epoch": 0.8381485186924474,
      "grad_norm": 16.943965911865234,
      "learning_rate": 1.4439088907220431e-05,
      "loss": 2.7382,
      "step": 82100
    },
    {
      "epoch": 0.8391694060477367,
      "grad_norm": 17.195053100585938,
      "learning_rate": 1.4432271391659454e-05,
      "loss": 2.7863,
      "step": 82200
    },
    {
      "epoch": 0.8401902934030259,
      "grad_norm": 17.6351375579834,
      "learning_rate": 1.4425453876098474e-05,
      "loss": 2.7791,
      "step": 82300
    },
    {
      "epoch": 0.8412111807583151,
      "grad_norm": 16.271127700805664,
      "learning_rate": 1.4418636360537495e-05,
      "loss": 2.7792,
      "step": 82400
    },
    {
      "epoch": 0.8422320681136043,
      "grad_norm": 12.787484169006348,
      "learning_rate": 1.4411818844976515e-05,
      "loss": 2.6857,
      "step": 82500
    },
    {
      "epoch": 0.8432529554688936,
      "grad_norm": 16.766237258911133,
      "learning_rate": 1.4405001329415536e-05,
      "loss": 2.7231,
      "step": 82600
    },
    {
      "epoch": 0.8442738428241828,
      "grad_norm": 16.01413917541504,
      "learning_rate": 1.4398183813854556e-05,
      "loss": 2.6979,
      "step": 82700
    },
    {
      "epoch": 0.845294730179472,
      "grad_norm": 15.811673164367676,
      "learning_rate": 1.4391366298293577e-05,
      "loss": 2.796,
      "step": 82800
    },
    {
      "epoch": 0.8463156175347613,
      "grad_norm": 15.02389144897461,
      "learning_rate": 1.4384548782732596e-05,
      "loss": 2.741,
      "step": 82900
    },
    {
      "epoch": 0.8473365048900504,
      "grad_norm": 19.763357162475586,
      "learning_rate": 1.437773126717162e-05,
      "loss": 2.6866,
      "step": 83000
    },
    {
      "epoch": 0.8483573922453397,
      "grad_norm": 17.283010482788086,
      "learning_rate": 1.437091375161064e-05,
      "loss": 2.7628,
      "step": 83100
    },
    {
      "epoch": 0.8493782796006288,
      "grad_norm": 17.150346755981445,
      "learning_rate": 1.436416441120527e-05,
      "loss": 2.778,
      "step": 83200
    },
    {
      "epoch": 0.8503991669559181,
      "grad_norm": 12.367528915405273,
      "learning_rate": 1.4357346895644292e-05,
      "loss": 2.7975,
      "step": 83300
    },
    {
      "epoch": 0.8514200543112073,
      "grad_norm": 20.4873104095459,
      "learning_rate": 1.4350529380083311e-05,
      "loss": 2.7365,
      "step": 83400
    },
    {
      "epoch": 0.8524409416664965,
      "grad_norm": 12.180034637451172,
      "learning_rate": 1.4343711864522332e-05,
      "loss": 2.8101,
      "step": 83500
    },
    {
      "epoch": 0.8534618290217857,
      "grad_norm": 17.08711051940918,
      "learning_rate": 1.4336894348961352e-05,
      "loss": 2.7819,
      "step": 83600
    },
    {
      "epoch": 0.854482716377075,
      "grad_norm": 18.300899505615234,
      "learning_rate": 1.4330076833400373e-05,
      "loss": 2.7174,
      "step": 83700
    },
    {
      "epoch": 0.8555036037323642,
      "grad_norm": 15.76967716217041,
      "learning_rate": 1.4323259317839393e-05,
      "loss": 2.7626,
      "step": 83800
    },
    {
      "epoch": 0.8565244910876534,
      "grad_norm": 17.756223678588867,
      "learning_rate": 1.4316441802278416e-05,
      "loss": 2.7321,
      "step": 83900
    },
    {
      "epoch": 0.8575453784429427,
      "grad_norm": 23.284988403320312,
      "learning_rate": 1.4309624286717436e-05,
      "loss": 2.7893,
      "step": 84000
    },
    {
      "epoch": 0.8585662657982318,
      "grad_norm": 16.11842918395996,
      "learning_rate": 1.4302806771156457e-05,
      "loss": 2.7945,
      "step": 84100
    },
    {
      "epoch": 0.8595871531535211,
      "grad_norm": 17.969083786010742,
      "learning_rate": 1.4295989255595477e-05,
      "loss": 2.7973,
      "step": 84200
    },
    {
      "epoch": 0.8606080405088102,
      "grad_norm": 12.553478240966797,
      "learning_rate": 1.4289171740034498e-05,
      "loss": 2.7744,
      "step": 84300
    },
    {
      "epoch": 0.8616289278640995,
      "grad_norm": 16.992652893066406,
      "learning_rate": 1.4282354224473517e-05,
      "loss": 2.6935,
      "step": 84400
    },
    {
      "epoch": 0.8626498152193887,
      "grad_norm": 16.356163024902344,
      "learning_rate": 1.4275536708912539e-05,
      "loss": 2.8314,
      "step": 84500
    },
    {
      "epoch": 0.8636707025746779,
      "grad_norm": 19.619321823120117,
      "learning_rate": 1.4268719193351558e-05,
      "loss": 2.6687,
      "step": 84600
    },
    {
      "epoch": 0.8646915899299671,
      "grad_norm": 15.616500854492188,
      "learning_rate": 1.4261901677790581e-05,
      "loss": 2.6722,
      "step": 84700
    },
    {
      "epoch": 0.8657124772852564,
      "grad_norm": 18.35799789428711,
      "learning_rate": 1.4255084162229603e-05,
      "loss": 2.7345,
      "step": 84800
    },
    {
      "epoch": 0.8667333646405455,
      "grad_norm": 16.30411148071289,
      "learning_rate": 1.4248266646668622e-05,
      "loss": 2.7156,
      "step": 84900
    },
    {
      "epoch": 0.8677542519958348,
      "grad_norm": 19.5701961517334,
      "learning_rate": 1.4241449131107644e-05,
      "loss": 2.728,
      "step": 85000
    },
    {
      "epoch": 0.868775139351124,
      "grad_norm": 17.385202407836914,
      "learning_rate": 1.4234631615546663e-05,
      "loss": 2.6917,
      "step": 85100
    },
    {
      "epoch": 0.8697960267064132,
      "grad_norm": 11.99728775024414,
      "learning_rate": 1.4227814099985685e-05,
      "loss": 2.7722,
      "step": 85200
    },
    {
      "epoch": 0.8708169140617025,
      "grad_norm": 15.925355911254883,
      "learning_rate": 1.4220996584424704e-05,
      "loss": 2.8253,
      "step": 85300
    },
    {
      "epoch": 0.8718378014169916,
      "grad_norm": 18.818998336791992,
      "learning_rate": 1.4214179068863726e-05,
      "loss": 2.6949,
      "step": 85400
    },
    {
      "epoch": 0.8728586887722809,
      "grad_norm": 17.891124725341797,
      "learning_rate": 1.4207361553302747e-05,
      "loss": 2.8304,
      "step": 85500
    },
    {
      "epoch": 0.8738795761275701,
      "grad_norm": 17.05271339416504,
      "learning_rate": 1.4200544037741768e-05,
      "loss": 2.6277,
      "step": 85600
    },
    {
      "epoch": 0.8749004634828593,
      "grad_norm": 17.368623733520508,
      "learning_rate": 1.4193726522180788e-05,
      "loss": 2.7451,
      "step": 85700
    },
    {
      "epoch": 0.8759213508381485,
      "grad_norm": 15.578076362609863,
      "learning_rate": 1.4186909006619809e-05,
      "loss": 2.818,
      "step": 85800
    },
    {
      "epoch": 0.8769422381934378,
      "grad_norm": 13.417828559875488,
      "learning_rate": 1.4180091491058829e-05,
      "loss": 2.6823,
      "step": 85900
    },
    {
      "epoch": 0.8779631255487269,
      "grad_norm": 19.69481658935547,
      "learning_rate": 1.417327397549785e-05,
      "loss": 2.7608,
      "step": 86000
    },
    {
      "epoch": 0.8789840129040162,
      "grad_norm": 13.334464073181152,
      "learning_rate": 1.416645645993687e-05,
      "loss": 2.7364,
      "step": 86100
    },
    {
      "epoch": 0.8800049002593053,
      "grad_norm": 14.887540817260742,
      "learning_rate": 1.4159638944375891e-05,
      "loss": 2.7994,
      "step": 86200
    },
    {
      "epoch": 0.8810257876145946,
      "grad_norm": 14.45700740814209,
      "learning_rate": 1.4152821428814914e-05,
      "loss": 2.8049,
      "step": 86300
    },
    {
      "epoch": 0.8820466749698839,
      "grad_norm": 15.16387939453125,
      "learning_rate": 1.4146003913253934e-05,
      "loss": 2.7874,
      "step": 86400
    },
    {
      "epoch": 0.883067562325173,
      "grad_norm": 18.783781051635742,
      "learning_rate": 1.4139186397692955e-05,
      "loss": 2.7868,
      "step": 86500
    },
    {
      "epoch": 0.8840884496804623,
      "grad_norm": 19.621170043945312,
      "learning_rate": 1.4132368882131974e-05,
      "loss": 2.7905,
      "step": 86600
    },
    {
      "epoch": 0.8851093370357515,
      "grad_norm": 15.699993133544922,
      "learning_rate": 1.4125551366570996e-05,
      "loss": 2.69,
      "step": 86700
    },
    {
      "epoch": 0.8861302243910407,
      "grad_norm": 16.94206428527832,
      "learning_rate": 1.4118733851010015e-05,
      "loss": 2.753,
      "step": 86800
    },
    {
      "epoch": 0.8871511117463299,
      "grad_norm": 17.60269546508789,
      "learning_rate": 1.4111916335449037e-05,
      "loss": 2.7793,
      "step": 86900
    },
    {
      "epoch": 0.8881719991016191,
      "grad_norm": 19.28875160217285,
      "learning_rate": 1.4105098819888056e-05,
      "loss": 2.7203,
      "step": 87000
    },
    {
      "epoch": 0.8891928864569083,
      "grad_norm": 17.269947052001953,
      "learning_rate": 1.4098281304327078e-05,
      "loss": 2.7622,
      "step": 87100
    },
    {
      "epoch": 0.8902137738121976,
      "grad_norm": 13.386167526245117,
      "learning_rate": 1.40914637887661e-05,
      "loss": 2.7327,
      "step": 87200
    },
    {
      "epoch": 0.8912346611674867,
      "grad_norm": 15.802058219909668,
      "learning_rate": 1.408471444836073e-05,
      "loss": 2.7156,
      "step": 87300
    },
    {
      "epoch": 0.892255548522776,
      "grad_norm": 20.366113662719727,
      "learning_rate": 1.4077896932799751e-05,
      "loss": 2.7556,
      "step": 87400
    },
    {
      "epoch": 0.8932764358780653,
      "grad_norm": 16.385944366455078,
      "learning_rate": 1.4071079417238771e-05,
      "loss": 2.8024,
      "step": 87500
    },
    {
      "epoch": 0.8942973232333544,
      "grad_norm": 21.130962371826172,
      "learning_rate": 1.4064261901677792e-05,
      "loss": 2.7671,
      "step": 87600
    },
    {
      "epoch": 0.8953182105886437,
      "grad_norm": 20.460126876831055,
      "learning_rate": 1.4057444386116812e-05,
      "loss": 2.7394,
      "step": 87700
    },
    {
      "epoch": 0.8963390979439328,
      "grad_norm": 16.486446380615234,
      "learning_rate": 1.4050626870555833e-05,
      "loss": 2.7482,
      "step": 87800
    },
    {
      "epoch": 0.8973599852992221,
      "grad_norm": 14.396146774291992,
      "learning_rate": 1.4043809354994853e-05,
      "loss": 2.7474,
      "step": 87900
    },
    {
      "epoch": 0.8983808726545113,
      "grad_norm": 13.563628196716309,
      "learning_rate": 1.4037060014589486e-05,
      "loss": 2.7523,
      "step": 88000
    },
    {
      "epoch": 0.8994017600098005,
      "grad_norm": 12.778898239135742,
      "learning_rate": 1.4030242499028505e-05,
      "loss": 2.7099,
      "step": 88100
    },
    {
      "epoch": 0.9004226473650897,
      "grad_norm": 14.199111938476562,
      "learning_rate": 1.4023424983467527e-05,
      "loss": 2.8523,
      "step": 88200
    },
    {
      "epoch": 0.901443534720379,
      "grad_norm": 19.200178146362305,
      "learning_rate": 1.4016607467906546e-05,
      "loss": 2.6437,
      "step": 88300
    },
    {
      "epoch": 0.9024644220756681,
      "grad_norm": 15.65255355834961,
      "learning_rate": 1.4009789952345567e-05,
      "loss": 2.7243,
      "step": 88400
    },
    {
      "epoch": 0.9034853094309574,
      "grad_norm": 16.94332504272461,
      "learning_rate": 1.4002972436784587e-05,
      "loss": 2.6688,
      "step": 88500
    },
    {
      "epoch": 0.9045061967862466,
      "grad_norm": 16.54188346862793,
      "learning_rate": 1.3996154921223608e-05,
      "loss": 2.7396,
      "step": 88600
    },
    {
      "epoch": 0.9055270841415358,
      "grad_norm": 12.675368309020996,
      "learning_rate": 1.3989337405662628e-05,
      "loss": 2.6814,
      "step": 88700
    },
    {
      "epoch": 0.9065479714968251,
      "grad_norm": 13.95958137512207,
      "learning_rate": 1.3982519890101651e-05,
      "loss": 2.7823,
      "step": 88800
    },
    {
      "epoch": 0.9075688588521142,
      "grad_norm": 13.352057456970215,
      "learning_rate": 1.3975702374540672e-05,
      "loss": 2.7762,
      "step": 88900
    },
    {
      "epoch": 0.9085897462074035,
      "grad_norm": 19.10782814025879,
      "learning_rate": 1.3968884858979692e-05,
      "loss": 2.7329,
      "step": 89000
    },
    {
      "epoch": 0.9096106335626927,
      "grad_norm": 16.0939998626709,
      "learning_rate": 1.3962067343418713e-05,
      "loss": 2.732,
      "step": 89100
    },
    {
      "epoch": 0.9106315209179819,
      "grad_norm": 13.525065422058105,
      "learning_rate": 1.3955249827857733e-05,
      "loss": 2.7083,
      "step": 89200
    },
    {
      "epoch": 0.9116524082732711,
      "grad_norm": 17.85895347595215,
      "learning_rate": 1.3948432312296754e-05,
      "loss": 2.7465,
      "step": 89300
    },
    {
      "epoch": 0.9126732956285604,
      "grad_norm": 12.94868278503418,
      "learning_rate": 1.3941614796735774e-05,
      "loss": 2.742,
      "step": 89400
    },
    {
      "epoch": 0.9136941829838495,
      "grad_norm": 15.512364387512207,
      "learning_rate": 1.3934797281174795e-05,
      "loss": 2.8342,
      "step": 89500
    },
    {
      "epoch": 0.9147150703391388,
      "grad_norm": 15.224939346313477,
      "learning_rate": 1.3927979765613816e-05,
      "loss": 2.7608,
      "step": 89600
    },
    {
      "epoch": 0.915735957694428,
      "grad_norm": 13.96111011505127,
      "learning_rate": 1.3921162250052838e-05,
      "loss": 2.735,
      "step": 89700
    },
    {
      "epoch": 0.9167568450497172,
      "grad_norm": 14.966227531433105,
      "learning_rate": 1.3914344734491857e-05,
      "loss": 2.7926,
      "step": 89800
    },
    {
      "epoch": 0.9177777324050065,
      "grad_norm": 17.933570861816406,
      "learning_rate": 1.3907527218930879e-05,
      "loss": 2.7535,
      "step": 89900
    },
    {
      "epoch": 0.9187986197602956,
      "grad_norm": 13.028018951416016,
      "learning_rate": 1.3900709703369898e-05,
      "loss": 2.684,
      "step": 90000
    },
    {
      "epoch": 0.9198195071155849,
      "grad_norm": 16.742998123168945,
      "learning_rate": 1.389389218780892e-05,
      "loss": 2.7202,
      "step": 90100
    },
    {
      "epoch": 0.9208403944708741,
      "grad_norm": 15.515795707702637,
      "learning_rate": 1.388707467224794e-05,
      "loss": 2.712,
      "step": 90200
    },
    {
      "epoch": 0.9218612818261633,
      "grad_norm": 14.67276382446289,
      "learning_rate": 1.388025715668696e-05,
      "loss": 2.7298,
      "step": 90300
    },
    {
      "epoch": 0.9228821691814525,
      "grad_norm": 13.75927734375,
      "learning_rate": 1.3873439641125984e-05,
      "loss": 2.699,
      "step": 90400
    },
    {
      "epoch": 0.9239030565367418,
      "grad_norm": 17.645841598510742,
      "learning_rate": 1.3866622125565003e-05,
      "loss": 2.6965,
      "step": 90500
    },
    {
      "epoch": 0.9249239438920309,
      "grad_norm": 12.931138038635254,
      "learning_rate": 1.3859804610004025e-05,
      "loss": 2.8046,
      "step": 90600
    },
    {
      "epoch": 0.9259448312473202,
      "grad_norm": 15.940465927124023,
      "learning_rate": 1.3852987094443044e-05,
      "loss": 2.8299,
      "step": 90700
    },
    {
      "epoch": 0.9269657186026093,
      "grad_norm": 14.62129020690918,
      "learning_rate": 1.3846169578882065e-05,
      "loss": 2.7913,
      "step": 90800
    },
    {
      "epoch": 0.9279866059578986,
      "grad_norm": 16.96279525756836,
      "learning_rate": 1.3839352063321085e-05,
      "loss": 2.7711,
      "step": 90900
    },
    {
      "epoch": 0.9290074933131878,
      "grad_norm": 18.080459594726562,
      "learning_rate": 1.3832534547760106e-05,
      "loss": 2.6105,
      "step": 91000
    },
    {
      "epoch": 0.930028380668477,
      "grad_norm": 15.929235458374023,
      "learning_rate": 1.3825717032199126e-05,
      "loss": 2.8019,
      "step": 91100
    },
    {
      "epoch": 0.9310492680237663,
      "grad_norm": 12.31887435913086,
      "learning_rate": 1.3818899516638147e-05,
      "loss": 2.6419,
      "step": 91200
    },
    {
      "epoch": 0.9320701553790555,
      "grad_norm": 11.990604400634766,
      "learning_rate": 1.3812082001077169e-05,
      "loss": 2.6541,
      "step": 91300
    },
    {
      "epoch": 0.9330910427343447,
      "grad_norm": 16.798856735229492,
      "learning_rate": 1.380526448551619e-05,
      "loss": 2.7143,
      "step": 91400
    },
    {
      "epoch": 0.9341119300896339,
      "grad_norm": 14.423622131347656,
      "learning_rate": 1.379844696995521e-05,
      "loss": 2.6521,
      "step": 91500
    },
    {
      "epoch": 0.9351328174449232,
      "grad_norm": 14.498114585876465,
      "learning_rate": 1.3791629454394231e-05,
      "loss": 2.6627,
      "step": 91600
    },
    {
      "epoch": 0.9361537048002123,
      "grad_norm": 19.068838119506836,
      "learning_rate": 1.378481193883325e-05,
      "loss": 2.7859,
      "step": 91700
    },
    {
      "epoch": 0.9371745921555016,
      "grad_norm": 14.244826316833496,
      "learning_rate": 1.3777994423272272e-05,
      "loss": 2.6443,
      "step": 91800
    },
    {
      "epoch": 0.9381954795107907,
      "grad_norm": 14.657922744750977,
      "learning_rate": 1.3771176907711291e-05,
      "loss": 2.6893,
      "step": 91900
    },
    {
      "epoch": 0.93921636686608,
      "grad_norm": 12.137784004211426,
      "learning_rate": 1.3764427567305922e-05,
      "loss": 2.7556,
      "step": 92000
    },
    {
      "epoch": 0.9402372542213692,
      "grad_norm": 17.406660079956055,
      "learning_rate": 1.3757610051744946e-05,
      "loss": 2.8078,
      "step": 92100
    },
    {
      "epoch": 0.9412581415766584,
      "grad_norm": 16.349740982055664,
      "learning_rate": 1.3750792536183965e-05,
      "loss": 2.7659,
      "step": 92200
    },
    {
      "epoch": 0.9422790289319477,
      "grad_norm": 15.305404663085938,
      "learning_rate": 1.3743975020622986e-05,
      "loss": 2.7704,
      "step": 92300
    },
    {
      "epoch": 0.9432999162872369,
      "grad_norm": 16.63550567626953,
      "learning_rate": 1.3737157505062006e-05,
      "loss": 2.7242,
      "step": 92400
    },
    {
      "epoch": 0.9443208036425261,
      "grad_norm": 17.80816078186035,
      "learning_rate": 1.3730339989501027e-05,
      "loss": 2.6992,
      "step": 92500
    },
    {
      "epoch": 0.9453416909978153,
      "grad_norm": 15.294818878173828,
      "learning_rate": 1.3723522473940047e-05,
      "loss": 2.7774,
      "step": 92600
    },
    {
      "epoch": 0.9463625783531046,
      "grad_norm": 14.591899871826172,
      "learning_rate": 1.3716704958379068e-05,
      "loss": 2.7351,
      "step": 92700
    },
    {
      "epoch": 0.9473834657083937,
      "grad_norm": 16.534385681152344,
      "learning_rate": 1.3709955617973698e-05,
      "loss": 2.6068,
      "step": 92800
    },
    {
      "epoch": 0.948404353063683,
      "grad_norm": 12.921042442321777,
      "learning_rate": 1.370313810241272e-05,
      "loss": 2.7577,
      "step": 92900
    },
    {
      "epoch": 0.9494252404189721,
      "grad_norm": 17.15852165222168,
      "learning_rate": 1.3696320586851742e-05,
      "loss": 2.735,
      "step": 93000
    },
    {
      "epoch": 0.9504461277742614,
      "grad_norm": 12.73473834991455,
      "learning_rate": 1.3689503071290762e-05,
      "loss": 2.6822,
      "step": 93100
    },
    {
      "epoch": 0.9514670151295506,
      "grad_norm": 24.951766967773438,
      "learning_rate": 1.3682685555729783e-05,
      "loss": 2.6945,
      "step": 93200
    },
    {
      "epoch": 0.9524879024848398,
      "grad_norm": 19.49590301513672,
      "learning_rate": 1.3675868040168803e-05,
      "loss": 2.6745,
      "step": 93300
    },
    {
      "epoch": 0.953508789840129,
      "grad_norm": 13.932289123535156,
      "learning_rate": 1.3669050524607824e-05,
      "loss": 2.7204,
      "step": 93400
    },
    {
      "epoch": 0.9545296771954183,
      "grad_norm": 15.833104133605957,
      "learning_rate": 1.3662233009046843e-05,
      "loss": 2.7128,
      "step": 93500
    },
    {
      "epoch": 0.9555505645507075,
      "grad_norm": 17.448976516723633,
      "learning_rate": 1.3655415493485865e-05,
      "loss": 2.7447,
      "step": 93600
    },
    {
      "epoch": 0.9565714519059967,
      "grad_norm": 16.419437408447266,
      "learning_rate": 1.3648597977924886e-05,
      "loss": 2.6859,
      "step": 93700
    },
    {
      "epoch": 0.957592339261286,
      "grad_norm": 17.221345901489258,
      "learning_rate": 1.3641780462363907e-05,
      "loss": 2.7322,
      "step": 93800
    },
    {
      "epoch": 0.9586132266165751,
      "grad_norm": 13.370222091674805,
      "learning_rate": 1.3634962946802927e-05,
      "loss": 2.7089,
      "step": 93900
    },
    {
      "epoch": 0.9596341139718644,
      "grad_norm": 15.337357521057129,
      "learning_rate": 1.3628145431241948e-05,
      "loss": 2.6772,
      "step": 94000
    },
    {
      "epoch": 0.9606550013271535,
      "grad_norm": 17.337322235107422,
      "learning_rate": 1.3621327915680968e-05,
      "loss": 2.7144,
      "step": 94100
    },
    {
      "epoch": 0.9616758886824428,
      "grad_norm": 13.324163436889648,
      "learning_rate": 1.361451040011999e-05,
      "loss": 2.6904,
      "step": 94200
    },
    {
      "epoch": 0.962696776037732,
      "grad_norm": 16.701379776000977,
      "learning_rate": 1.3607692884559009e-05,
      "loss": 2.7357,
      "step": 94300
    },
    {
      "epoch": 0.9637176633930212,
      "grad_norm": 16.481380462646484,
      "learning_rate": 1.360087536899803e-05,
      "loss": 2.7732,
      "step": 94400
    },
    {
      "epoch": 0.9647385507483104,
      "grad_norm": 19.530433654785156,
      "learning_rate": 1.3594057853437053e-05,
      "loss": 2.7553,
      "step": 94500
    },
    {
      "epoch": 0.9657594381035997,
      "grad_norm": 19.112096786499023,
      "learning_rate": 1.3587240337876073e-05,
      "loss": 2.6723,
      "step": 94600
    },
    {
      "epoch": 0.9667803254588888,
      "grad_norm": 15.564789772033691,
      "learning_rate": 1.3580422822315094e-05,
      "loss": 2.738,
      "step": 94700
    },
    {
      "epoch": 0.9678012128141781,
      "grad_norm": 13.976773262023926,
      "learning_rate": 1.3573605306754114e-05,
      "loss": 2.7716,
      "step": 94800
    },
    {
      "epoch": 0.9688221001694673,
      "grad_norm": 14.526467323303223,
      "learning_rate": 1.3566855966348745e-05,
      "loss": 2.7395,
      "step": 94900
    },
    {
      "epoch": 0.9698429875247565,
      "grad_norm": 14.483025550842285,
      "learning_rate": 1.3560038450787764e-05,
      "loss": 2.6944,
      "step": 95000
    },
    {
      "epoch": 0.9708638748800458,
      "grad_norm": 13.6704683303833,
      "learning_rate": 1.3553220935226786e-05,
      "loss": 2.784,
      "step": 95100
    },
    {
      "epoch": 0.9718847622353349,
      "grad_norm": 20.28403091430664,
      "learning_rate": 1.3546403419665805e-05,
      "loss": 2.85,
      "step": 95200
    },
    {
      "epoch": 0.9729056495906242,
      "grad_norm": 12.036255836486816,
      "learning_rate": 1.3539585904104828e-05,
      "loss": 2.6838,
      "step": 95300
    },
    {
      "epoch": 0.9739265369459134,
      "grad_norm": 16.527671813964844,
      "learning_rate": 1.3532768388543848e-05,
      "loss": 2.7201,
      "step": 95400
    },
    {
      "epoch": 0.9749474243012026,
      "grad_norm": 13.14719295501709,
      "learning_rate": 1.352595087298287e-05,
      "loss": 2.6806,
      "step": 95500
    },
    {
      "epoch": 0.9759683116564918,
      "grad_norm": 15.33080005645752,
      "learning_rate": 1.3519133357421889e-05,
      "loss": 2.7279,
      "step": 95600
    },
    {
      "epoch": 0.976989199011781,
      "grad_norm": 14.793830871582031,
      "learning_rate": 1.351231584186091e-05,
      "loss": 2.7359,
      "step": 95700
    },
    {
      "epoch": 0.9780100863670702,
      "grad_norm": 17.11341667175293,
      "learning_rate": 1.350549832629993e-05,
      "loss": 2.6101,
      "step": 95800
    },
    {
      "epoch": 0.9790309737223595,
      "grad_norm": 22.678190231323242,
      "learning_rate": 1.3498680810738951e-05,
      "loss": 2.6895,
      "step": 95900
    },
    {
      "epoch": 0.9800518610776487,
      "grad_norm": 16.93073844909668,
      "learning_rate": 1.349186329517797e-05,
      "loss": 2.6835,
      "step": 96000
    },
    {
      "epoch": 0.9810727484329379,
      "grad_norm": 18.590553283691406,
      "learning_rate": 1.3485045779616992e-05,
      "loss": 2.7112,
      "step": 96100
    },
    {
      "epoch": 0.9820936357882272,
      "grad_norm": 12.570894241333008,
      "learning_rate": 1.3478228264056015e-05,
      "loss": 2.7317,
      "step": 96200
    },
    {
      "epoch": 0.9831145231435163,
      "grad_norm": 13.752522468566895,
      "learning_rate": 1.3471410748495035e-05,
      "loss": 2.761,
      "step": 96300
    },
    {
      "epoch": 0.9841354104988056,
      "grad_norm": 16.440929412841797,
      "learning_rate": 1.3464593232934056e-05,
      "loss": 2.7786,
      "step": 96400
    },
    {
      "epoch": 0.9851562978540948,
      "grad_norm": 16.86958122253418,
      "learning_rate": 1.3457775717373076e-05,
      "loss": 2.6835,
      "step": 96500
    },
    {
      "epoch": 0.986177185209384,
      "grad_norm": 19.534317016601562,
      "learning_rate": 1.3450958201812097e-05,
      "loss": 2.6794,
      "step": 96600
    },
    {
      "epoch": 0.9871980725646732,
      "grad_norm": 15.4544677734375,
      "learning_rate": 1.3444140686251117e-05,
      "loss": 2.7406,
      "step": 96700
    },
    {
      "epoch": 0.9882189599199624,
      "grad_norm": 15.699579238891602,
      "learning_rate": 1.3437323170690138e-05,
      "loss": 2.7219,
      "step": 96800
    },
    {
      "epoch": 0.9892398472752516,
      "grad_norm": 15.54893684387207,
      "learning_rate": 1.3430505655129158e-05,
      "loss": 2.7313,
      "step": 96900
    },
    {
      "epoch": 0.9902607346305409,
      "grad_norm": 14.46015739440918,
      "learning_rate": 1.342368813956818e-05,
      "loss": 2.826,
      "step": 97000
    },
    {
      "epoch": 0.99128162198583,
      "grad_norm": 14.009592056274414,
      "learning_rate": 1.3416870624007202e-05,
      "loss": 2.6804,
      "step": 97100
    },
    {
      "epoch": 0.9923025093411193,
      "grad_norm": 16.325536727905273,
      "learning_rate": 1.3410053108446221e-05,
      "loss": 2.7295,
      "step": 97200
    },
    {
      "epoch": 0.9933233966964086,
      "grad_norm": 17.455963134765625,
      "learning_rate": 1.3403235592885243e-05,
      "loss": 2.7001,
      "step": 97300
    },
    {
      "epoch": 0.9943442840516977,
      "grad_norm": 17.278329849243164,
      "learning_rate": 1.3396486252479872e-05,
      "loss": 2.6798,
      "step": 97400
    },
    {
      "epoch": 0.995365171406987,
      "grad_norm": 16.016996383666992,
      "learning_rate": 1.3389668736918894e-05,
      "loss": 2.7202,
      "step": 97500
    },
    {
      "epoch": 0.9963860587622761,
      "grad_norm": 19.962141036987305,
      "learning_rate": 1.3382851221357913e-05,
      "loss": 2.7653,
      "step": 97600
    },
    {
      "epoch": 0.9974069461175654,
      "grad_norm": 19.518753051757812,
      "learning_rate": 1.3376033705796934e-05,
      "loss": 2.6902,
      "step": 97700
    },
    {
      "epoch": 0.9984278334728546,
      "grad_norm": 13.972554206848145,
      "learning_rate": 1.3369216190235956e-05,
      "loss": 2.5986,
      "step": 97800
    },
    {
      "epoch": 0.9994487208281438,
      "grad_norm": 15.952523231506348,
      "learning_rate": 1.3362398674674977e-05,
      "loss": 2.6978,
      "step": 97900
    },
    {
      "epoch": 1.000469608183433,
      "grad_norm": 14.189123153686523,
      "learning_rate": 1.3355581159113997e-05,
      "loss": 2.7226,
      "step": 98000
    },
    {
      "epoch": 1.0014904955387223,
      "grad_norm": 14.373671531677246,
      "learning_rate": 1.3348763643553018e-05,
      "loss": 2.6585,
      "step": 98100
    },
    {
      "epoch": 1.0025113828940115,
      "grad_norm": 13.890620231628418,
      "learning_rate": 1.3341946127992038e-05,
      "loss": 2.7976,
      "step": 98200
    },
    {
      "epoch": 1.0035322702493006,
      "grad_norm": 12.162806510925293,
      "learning_rate": 1.3335128612431059e-05,
      "loss": 2.6231,
      "step": 98300
    },
    {
      "epoch": 1.0045531576045899,
      "grad_norm": 14.451201438903809,
      "learning_rate": 1.3328311096870079e-05,
      "loss": 2.7058,
      "step": 98400
    },
    {
      "epoch": 1.0055740449598791,
      "grad_norm": 15.556547164916992,
      "learning_rate": 1.33214935813091e-05,
      "loss": 2.6602,
      "step": 98500
    },
    {
      "epoch": 1.0065949323151684,
      "grad_norm": 18.57959747314453,
      "learning_rate": 1.3314676065748123e-05,
      "loss": 2.6891,
      "step": 98600
    },
    {
      "epoch": 1.0076158196704577,
      "grad_norm": 14.482206344604492,
      "learning_rate": 1.3307858550187142e-05,
      "loss": 2.7151,
      "step": 98700
    },
    {
      "epoch": 1.0086367070257467,
      "grad_norm": 14.673521995544434,
      "learning_rate": 1.3301041034626164e-05,
      "loss": 2.7023,
      "step": 98800
    },
    {
      "epoch": 1.009657594381036,
      "grad_norm": 19.475187301635742,
      "learning_rate": 1.3294223519065183e-05,
      "loss": 2.6867,
      "step": 98900
    },
    {
      "epoch": 1.0106784817363252,
      "grad_norm": 13.457782745361328,
      "learning_rate": 1.3287406003504205e-05,
      "loss": 2.6206,
      "step": 99000
    },
    {
      "epoch": 1.0116993690916145,
      "grad_norm": 15.97851276397705,
      "learning_rate": 1.3280588487943224e-05,
      "loss": 2.6394,
      "step": 99100
    },
    {
      "epoch": 1.0127202564469036,
      "grad_norm": 15.329133033752441,
      "learning_rate": 1.3273770972382246e-05,
      "loss": 2.6062,
      "step": 99200
    },
    {
      "epoch": 1.0137411438021928,
      "grad_norm": 12.090629577636719,
      "learning_rate": 1.3266953456821265e-05,
      "loss": 2.6897,
      "step": 99300
    },
    {
      "epoch": 1.014762031157482,
      "grad_norm": 16.64583396911621,
      "learning_rate": 1.3260135941260287e-05,
      "loss": 2.7644,
      "step": 99400
    },
    {
      "epoch": 1.0157829185127714,
      "grad_norm": 13.299752235412598,
      "learning_rate": 1.3253318425699308e-05,
      "loss": 2.6375,
      "step": 99500
    },
    {
      "epoch": 1.0168038058680606,
      "grad_norm": 16.59261131286621,
      "learning_rate": 1.324650091013833e-05,
      "loss": 2.6301,
      "step": 99600
    },
    {
      "epoch": 1.0178246932233497,
      "grad_norm": 13.80134391784668,
      "learning_rate": 1.3239683394577349e-05,
      "loss": 2.7283,
      "step": 99700
    },
    {
      "epoch": 1.018845580578639,
      "grad_norm": 12.608922004699707,
      "learning_rate": 1.323286587901637e-05,
      "loss": 2.6519,
      "step": 99800
    },
    {
      "epoch": 1.0198664679339282,
      "grad_norm": 15.062024116516113,
      "learning_rate": 1.322604836345539e-05,
      "loss": 2.6916,
      "step": 99900
    },
    {
      "epoch": 1.0208873552892175,
      "grad_norm": 13.094698905944824,
      "learning_rate": 1.3219230847894411e-05,
      "loss": 2.5419,
      "step": 100000
    },
    {
      "epoch": 1.0219082426445065,
      "grad_norm": 17.504968643188477,
      "learning_rate": 1.321241333233343e-05,
      "loss": 2.6204,
      "step": 100100
    },
    {
      "epoch": 1.0229291299997958,
      "grad_norm": 20.111751556396484,
      "learning_rate": 1.3205595816772452e-05,
      "loss": 2.727,
      "step": 100200
    },
    {
      "epoch": 1.023950017355085,
      "grad_norm": 18.31135368347168,
      "learning_rate": 1.3198778301211475e-05,
      "loss": 2.6651,
      "step": 100300
    },
    {
      "epoch": 1.0249709047103743,
      "grad_norm": 14.924792289733887,
      "learning_rate": 1.3191960785650495e-05,
      "loss": 2.7952,
      "step": 100400
    },
    {
      "epoch": 1.0259917920656634,
      "grad_norm": 14.922550201416016,
      "learning_rate": 1.3185143270089516e-05,
      "loss": 2.7146,
      "step": 100500
    },
    {
      "epoch": 1.0270126794209526,
      "grad_norm": 17.908308029174805,
      "learning_rate": 1.3178325754528536e-05,
      "loss": 2.6601,
      "step": 100600
    },
    {
      "epoch": 1.028033566776242,
      "grad_norm": 15.582962036132812,
      "learning_rate": 1.3171508238967557e-05,
      "loss": 2.7219,
      "step": 100700
    },
    {
      "epoch": 1.0290544541315312,
      "grad_norm": 16.329225540161133,
      "learning_rate": 1.3164690723406577e-05,
      "loss": 2.6583,
      "step": 100800
    },
    {
      "epoch": 1.0300753414868205,
      "grad_norm": 13.182218551635742,
      "learning_rate": 1.3157873207845598e-05,
      "loss": 2.6899,
      "step": 100900
    },
    {
      "epoch": 1.0310962288421095,
      "grad_norm": 14.003838539123535,
      "learning_rate": 1.3151055692284617e-05,
      "loss": 2.7835,
      "step": 101000
    },
    {
      "epoch": 1.0321171161973988,
      "grad_norm": 16.24382781982422,
      "learning_rate": 1.314430635187925e-05,
      "loss": 2.6565,
      "step": 101100
    },
    {
      "epoch": 1.033138003552688,
      "grad_norm": 14.999068260192871,
      "learning_rate": 1.313748883631827e-05,
      "loss": 2.7576,
      "step": 101200
    },
    {
      "epoch": 1.0341588909079773,
      "grad_norm": 16.291067123413086,
      "learning_rate": 1.3130671320757291e-05,
      "loss": 2.638,
      "step": 101300
    },
    {
      "epoch": 1.0351797782632663,
      "grad_norm": 13.739884376525879,
      "learning_rate": 1.312385380519631e-05,
      "loss": 2.6925,
      "step": 101400
    },
    {
      "epoch": 1.0362006656185556,
      "grad_norm": 16.2017822265625,
      "learning_rate": 1.3117036289635332e-05,
      "loss": 2.7218,
      "step": 101500
    },
    {
      "epoch": 1.0372215529738449,
      "grad_norm": 14.373492240905762,
      "learning_rate": 1.3110218774074352e-05,
      "loss": 2.6353,
      "step": 101600
    },
    {
      "epoch": 1.0382424403291342,
      "grad_norm": 20.98880958557129,
      "learning_rate": 1.3103401258513373e-05,
      "loss": 2.6794,
      "step": 101700
    },
    {
      "epoch": 1.0392633276844232,
      "grad_norm": 13.772106170654297,
      "learning_rate": 1.3096583742952393e-05,
      "loss": 2.6107,
      "step": 101800
    },
    {
      "epoch": 1.0402842150397125,
      "grad_norm": 19.885498046875,
      "learning_rate": 1.3089766227391416e-05,
      "loss": 2.7037,
      "step": 101900
    },
    {
      "epoch": 1.0413051023950017,
      "grad_norm": 17.970199584960938,
      "learning_rate": 1.3082948711830437e-05,
      "loss": 2.6661,
      "step": 102000
    },
    {
      "epoch": 1.042325989750291,
      "grad_norm": 14.814911842346191,
      "learning_rate": 1.3076131196269457e-05,
      "loss": 2.6705,
      "step": 102100
    },
    {
      "epoch": 1.0433468771055803,
      "grad_norm": 15.26167106628418,
      "learning_rate": 1.3069313680708478e-05,
      "loss": 2.6812,
      "step": 102200
    },
    {
      "epoch": 1.0443677644608693,
      "grad_norm": 13.724377632141113,
      "learning_rate": 1.3062564340303107e-05,
      "loss": 2.649,
      "step": 102300
    },
    {
      "epoch": 1.0453886518161586,
      "grad_norm": 16.382455825805664,
      "learning_rate": 1.3055746824742129e-05,
      "loss": 2.7453,
      "step": 102400
    },
    {
      "epoch": 1.0464095391714479,
      "grad_norm": 14.921904563903809,
      "learning_rate": 1.3048929309181148e-05,
      "loss": 2.6211,
      "step": 102500
    },
    {
      "epoch": 1.0474304265267371,
      "grad_norm": 19.932903289794922,
      "learning_rate": 1.304211179362017e-05,
      "loss": 2.6979,
      "step": 102600
    },
    {
      "epoch": 1.0484513138820262,
      "grad_norm": 15.348441123962402,
      "learning_rate": 1.3035294278059193e-05,
      "loss": 2.7088,
      "step": 102700
    },
    {
      "epoch": 1.0494722012373154,
      "grad_norm": 13.458380699157715,
      "learning_rate": 1.3028476762498212e-05,
      "loss": 2.6838,
      "step": 102800
    },
    {
      "epoch": 1.0504930885926047,
      "grad_norm": 17.88050651550293,
      "learning_rate": 1.3021659246937233e-05,
      "loss": 2.6561,
      "step": 102900
    },
    {
      "epoch": 1.051513975947894,
      "grad_norm": 14.784140586853027,
      "learning_rate": 1.3014841731376253e-05,
      "loss": 2.7019,
      "step": 103000
    },
    {
      "epoch": 1.052534863303183,
      "grad_norm": 14.98931884765625,
      "learning_rate": 1.3008024215815274e-05,
      "loss": 2.7044,
      "step": 103100
    },
    {
      "epoch": 1.0535557506584723,
      "grad_norm": 17.058732986450195,
      "learning_rate": 1.3001206700254294e-05,
      "loss": 2.7424,
      "step": 103200
    },
    {
      "epoch": 1.0545766380137616,
      "grad_norm": 12.747899055480957,
      "learning_rate": 1.2994389184693315e-05,
      "loss": 2.7314,
      "step": 103300
    },
    {
      "epoch": 1.0555975253690508,
      "grad_norm": 18.637521743774414,
      "learning_rate": 1.2987571669132335e-05,
      "loss": 2.7623,
      "step": 103400
    },
    {
      "epoch": 1.05661841272434,
      "grad_norm": 15.031943321228027,
      "learning_rate": 1.2980754153571356e-05,
      "loss": 2.703,
      "step": 103500
    },
    {
      "epoch": 1.0576393000796291,
      "grad_norm": 13.790451049804688,
      "learning_rate": 1.2973936638010378e-05,
      "loss": 2.7204,
      "step": 103600
    },
    {
      "epoch": 1.0586601874349184,
      "grad_norm": 13.970345497131348,
      "learning_rate": 1.2967119122449399e-05,
      "loss": 2.7302,
      "step": 103700
    },
    {
      "epoch": 1.0596810747902077,
      "grad_norm": 18.087324142456055,
      "learning_rate": 1.2960301606888418e-05,
      "loss": 2.6103,
      "step": 103800
    },
    {
      "epoch": 1.060701962145497,
      "grad_norm": 14.156326293945312,
      "learning_rate": 1.295348409132744e-05,
      "loss": 2.6891,
      "step": 103900
    },
    {
      "epoch": 1.061722849500786,
      "grad_norm": 19.914569854736328,
      "learning_rate": 1.294666657576646e-05,
      "loss": 2.6552,
      "step": 104000
    },
    {
      "epoch": 1.0627437368560753,
      "grad_norm": 14.171313285827637,
      "learning_rate": 1.293984906020548e-05,
      "loss": 2.6972,
      "step": 104100
    },
    {
      "epoch": 1.0637646242113645,
      "grad_norm": 15.366087913513184,
      "learning_rate": 1.29330315446445e-05,
      "loss": 2.6868,
      "step": 104200
    },
    {
      "epoch": 1.0647855115666538,
      "grad_norm": 20.5633602142334,
      "learning_rate": 1.2926214029083522e-05,
      "loss": 2.6567,
      "step": 104300
    },
    {
      "epoch": 1.0658063989219428,
      "grad_norm": 16.713369369506836,
      "learning_rate": 1.2919396513522545e-05,
      "loss": 2.5911,
      "step": 104400
    },
    {
      "epoch": 1.0668272862772321,
      "grad_norm": 17.95933723449707,
      "learning_rate": 1.2912578997961564e-05,
      "loss": 2.7398,
      "step": 104500
    },
    {
      "epoch": 1.0678481736325214,
      "grad_norm": 20.410959243774414,
      "learning_rate": 1.2905761482400586e-05,
      "loss": 2.6217,
      "step": 104600
    },
    {
      "epoch": 1.0688690609878106,
      "grad_norm": 15.160783767700195,
      "learning_rate": 1.2898943966839605e-05,
      "loss": 2.7285,
      "step": 104700
    },
    {
      "epoch": 1.0698899483431,
      "grad_norm": 16.58269500732422,
      "learning_rate": 1.2892126451278627e-05,
      "loss": 2.6873,
      "step": 104800
    },
    {
      "epoch": 1.070910835698389,
      "grad_norm": 13.534834861755371,
      "learning_rate": 1.2885308935717646e-05,
      "loss": 2.7457,
      "step": 104900
    },
    {
      "epoch": 1.0719317230536782,
      "grad_norm": 15.368654251098633,
      "learning_rate": 1.2878491420156667e-05,
      "loss": 2.6143,
      "step": 105000
    },
    {
      "epoch": 1.0729526104089675,
      "grad_norm": 19.038341522216797,
      "learning_rate": 1.2871673904595687e-05,
      "loss": 2.7567,
      "step": 105100
    },
    {
      "epoch": 1.0739734977642568,
      "grad_norm": 21.61431312561035,
      "learning_rate": 1.286485638903471e-05,
      "loss": 2.7002,
      "step": 105200
    },
    {
      "epoch": 1.0749943851195458,
      "grad_norm": 17.42059898376465,
      "learning_rate": 1.285803887347373e-05,
      "loss": 2.6614,
      "step": 105300
    },
    {
      "epoch": 1.076015272474835,
      "grad_norm": 12.331452369689941,
      "learning_rate": 1.2851221357912751e-05,
      "loss": 2.6262,
      "step": 105400
    },
    {
      "epoch": 1.0770361598301244,
      "grad_norm": 16.865442276000977,
      "learning_rate": 1.284440384235177e-05,
      "loss": 2.6368,
      "step": 105500
    },
    {
      "epoch": 1.0780570471854136,
      "grad_norm": 13.792327880859375,
      "learning_rate": 1.2837586326790792e-05,
      "loss": 2.7102,
      "step": 105600
    },
    {
      "epoch": 1.0790779345407029,
      "grad_norm": 17.4376220703125,
      "learning_rate": 1.2830768811229812e-05,
      "loss": 2.616,
      "step": 105700
    },
    {
      "epoch": 1.080098821895992,
      "grad_norm": 19.8478946685791,
      "learning_rate": 1.2823951295668833e-05,
      "loss": 2.6499,
      "step": 105800
    },
    {
      "epoch": 1.0811197092512812,
      "grad_norm": 20.779539108276367,
      "learning_rate": 1.2817133780107852e-05,
      "loss": 2.709,
      "step": 105900
    },
    {
      "epoch": 1.0821405966065705,
      "grad_norm": 13.254252433776855,
      "learning_rate": 1.2810316264546875e-05,
      "loss": 2.6027,
      "step": 106000
    },
    {
      "epoch": 1.0831614839618597,
      "grad_norm": 17.746414184570312,
      "learning_rate": 1.2803498748985897e-05,
      "loss": 2.6527,
      "step": 106100
    },
    {
      "epoch": 1.0841823713171488,
      "grad_norm": 20.168254852294922,
      "learning_rate": 1.2796681233424916e-05,
      "loss": 2.6083,
      "step": 106200
    },
    {
      "epoch": 1.085203258672438,
      "grad_norm": 16.572895050048828,
      "learning_rate": 1.2789863717863938e-05,
      "loss": 2.5731,
      "step": 106300
    },
    {
      "epoch": 1.0862241460277273,
      "grad_norm": 13.524317741394043,
      "learning_rate": 1.2783046202302957e-05,
      "loss": 2.6182,
      "step": 106400
    },
    {
      "epoch": 1.0872450333830166,
      "grad_norm": 14.842093467712402,
      "learning_rate": 1.2776228686741979e-05,
      "loss": 2.628,
      "step": 106500
    },
    {
      "epoch": 1.0882659207383056,
      "grad_norm": 14.063566207885742,
      "learning_rate": 1.2769411171180998e-05,
      "loss": 2.6815,
      "step": 106600
    },
    {
      "epoch": 1.089286808093595,
      "grad_norm": 15.02513313293457,
      "learning_rate": 1.276259365562002e-05,
      "loss": 2.7398,
      "step": 106700
    },
    {
      "epoch": 1.0903076954488842,
      "grad_norm": 17.469215393066406,
      "learning_rate": 1.275577614005904e-05,
      "loss": 2.5887,
      "step": 106800
    },
    {
      "epoch": 1.0913285828041734,
      "grad_norm": 16.754270553588867,
      "learning_rate": 1.2748958624498062e-05,
      "loss": 2.6743,
      "step": 106900
    },
    {
      "epoch": 1.0923494701594625,
      "grad_norm": 23.276092529296875,
      "learning_rate": 1.2742141108937084e-05,
      "loss": 2.7116,
      "step": 107000
    },
    {
      "epoch": 1.0933703575147518,
      "grad_norm": 12.235844612121582,
      "learning_rate": 1.2735323593376103e-05,
      "loss": 2.7107,
      "step": 107100
    },
    {
      "epoch": 1.094391244870041,
      "grad_norm": 14.7138671875,
      "learning_rate": 1.2728506077815124e-05,
      "loss": 2.737,
      "step": 107200
    },
    {
      "epoch": 1.0954121322253303,
      "grad_norm": 17.189289093017578,
      "learning_rate": 1.2721688562254144e-05,
      "loss": 2.7694,
      "step": 107300
    },
    {
      "epoch": 1.0964330195806196,
      "grad_norm": 13.14093017578125,
      "learning_rate": 1.2714871046693165e-05,
      "loss": 2.7303,
      "step": 107400
    },
    {
      "epoch": 1.0974539069359086,
      "grad_norm": 15.55985164642334,
      "learning_rate": 1.2708053531132185e-05,
      "loss": 2.6757,
      "step": 107500
    },
    {
      "epoch": 1.0984747942911979,
      "grad_norm": 16.639795303344727,
      "learning_rate": 1.2701304190726816e-05,
      "loss": 2.626,
      "step": 107600
    },
    {
      "epoch": 1.0994956816464871,
      "grad_norm": 15.47292423248291,
      "learning_rate": 1.2694486675165837e-05,
      "loss": 2.6684,
      "step": 107700
    },
    {
      "epoch": 1.1005165690017764,
      "grad_norm": 15.233752250671387,
      "learning_rate": 1.2687669159604859e-05,
      "loss": 2.6758,
      "step": 107800
    },
    {
      "epoch": 1.1015374563570655,
      "grad_norm": 17.615854263305664,
      "learning_rate": 1.2680851644043878e-05,
      "loss": 2.6812,
      "step": 107900
    },
    {
      "epoch": 1.1025583437123547,
      "grad_norm": 14.691423416137695,
      "learning_rate": 1.26740341284829e-05,
      "loss": 2.6023,
      "step": 108000
    },
    {
      "epoch": 1.103579231067644,
      "grad_norm": 13.856743812561035,
      "learning_rate": 1.266721661292192e-05,
      "loss": 2.636,
      "step": 108100
    },
    {
      "epoch": 1.1046001184229333,
      "grad_norm": 13.863636016845703,
      "learning_rate": 1.266039909736094e-05,
      "loss": 2.6724,
      "step": 108200
    },
    {
      "epoch": 1.1056210057782225,
      "grad_norm": 11.483281135559082,
      "learning_rate": 1.265358158179996e-05,
      "loss": 2.6205,
      "step": 108300
    },
    {
      "epoch": 1.1066418931335116,
      "grad_norm": 13.849525451660156,
      "learning_rate": 1.2646764066238982e-05,
      "loss": 2.5574,
      "step": 108400
    },
    {
      "epoch": 1.1076627804888008,
      "grad_norm": 12.604640007019043,
      "learning_rate": 1.2639946550678005e-05,
      "loss": 2.6563,
      "step": 108500
    },
    {
      "epoch": 1.1086836678440901,
      "grad_norm": 14.231229782104492,
      "learning_rate": 1.2633129035117024e-05,
      "loss": 2.7059,
      "step": 108600
    },
    {
      "epoch": 1.1097045551993794,
      "grad_norm": 15.593302726745605,
      "learning_rate": 1.2626311519556045e-05,
      "loss": 2.6566,
      "step": 108700
    },
    {
      "epoch": 1.1107254425546684,
      "grad_norm": 18.26386260986328,
      "learning_rate": 1.2619494003995065e-05,
      "loss": 2.7148,
      "step": 108800
    },
    {
      "epoch": 1.1117463299099577,
      "grad_norm": 17.641151428222656,
      "learning_rate": 1.2612676488434086e-05,
      "loss": 2.6821,
      "step": 108900
    },
    {
      "epoch": 1.112767217265247,
      "grad_norm": 13.38265323638916,
      "learning_rate": 1.2605858972873106e-05,
      "loss": 2.7081,
      "step": 109000
    },
    {
      "epoch": 1.1137881046205362,
      "grad_norm": 13.681373596191406,
      "learning_rate": 1.2599041457312127e-05,
      "loss": 2.695,
      "step": 109100
    },
    {
      "epoch": 1.1148089919758255,
      "grad_norm": 16.387712478637695,
      "learning_rate": 1.2592223941751147e-05,
      "loss": 2.691,
      "step": 109200
    },
    {
      "epoch": 1.1158298793311145,
      "grad_norm": 15.17844295501709,
      "learning_rate": 1.258540642619017e-05,
      "loss": 2.6448,
      "step": 109300
    },
    {
      "epoch": 1.1168507666864038,
      "grad_norm": 15.179935455322266,
      "learning_rate": 1.257858891062919e-05,
      "loss": 2.618,
      "step": 109400
    },
    {
      "epoch": 1.117871654041693,
      "grad_norm": 20.053558349609375,
      "learning_rate": 1.2571771395068211e-05,
      "loss": 2.7098,
      "step": 109500
    },
    {
      "epoch": 1.1188925413969824,
      "grad_norm": 14.716965675354004,
      "learning_rate": 1.256495387950723e-05,
      "loss": 2.7471,
      "step": 109600
    },
    {
      "epoch": 1.1199134287522714,
      "grad_norm": 15.008387565612793,
      "learning_rate": 1.2558136363946252e-05,
      "loss": 2.6507,
      "step": 109700
    },
    {
      "epoch": 1.1209343161075607,
      "grad_norm": 13.847582817077637,
      "learning_rate": 1.2551318848385271e-05,
      "loss": 2.6719,
      "step": 109800
    },
    {
      "epoch": 1.12195520346285,
      "grad_norm": 18.413915634155273,
      "learning_rate": 1.2544501332824293e-05,
      "loss": 2.7717,
      "step": 109900
    },
    {
      "epoch": 1.1229760908181392,
      "grad_norm": 14.551399230957031,
      "learning_rate": 1.2537683817263312e-05,
      "loss": 2.6947,
      "step": 110000
    },
    {
      "epoch": 1.1239969781734283,
      "grad_norm": 18.445335388183594,
      "learning_rate": 1.2530866301702335e-05,
      "loss": 2.646,
      "step": 110100
    },
    {
      "epoch": 1.1250178655287175,
      "grad_norm": 13.22516918182373,
      "learning_rate": 1.2524048786141357e-05,
      "loss": 2.7435,
      "step": 110200
    },
    {
      "epoch": 1.1260387528840068,
      "grad_norm": 28.22221565246582,
      "learning_rate": 1.2517231270580376e-05,
      "loss": 2.6418,
      "step": 110300
    },
    {
      "epoch": 1.127059640239296,
      "grad_norm": 12.93868350982666,
      "learning_rate": 1.2510413755019398e-05,
      "loss": 2.6348,
      "step": 110400
    },
    {
      "epoch": 1.128080527594585,
      "grad_norm": 17.57606315612793,
      "learning_rate": 1.2503596239458417e-05,
      "loss": 2.6161,
      "step": 110500
    },
    {
      "epoch": 1.1291014149498744,
      "grad_norm": 18.451976776123047,
      "learning_rate": 1.2496778723897439e-05,
      "loss": 2.8174,
      "step": 110600
    },
    {
      "epoch": 1.1301223023051636,
      "grad_norm": 18.169864654541016,
      "learning_rate": 1.2489961208336458e-05,
      "loss": 2.6729,
      "step": 110700
    },
    {
      "epoch": 1.131143189660453,
      "grad_norm": 16.802398681640625,
      "learning_rate": 1.248314369277548e-05,
      "loss": 2.6315,
      "step": 110800
    },
    {
      "epoch": 1.1321640770157422,
      "grad_norm": 14.455904960632324,
      "learning_rate": 1.2476394352370109e-05,
      "loss": 2.565,
      "step": 110900
    },
    {
      "epoch": 1.1331849643710312,
      "grad_norm": 17.2816104888916,
      "learning_rate": 1.2469576836809132e-05,
      "loss": 2.6728,
      "step": 111000
    },
    {
      "epoch": 1.1342058517263205,
      "grad_norm": 14.007305145263672,
      "learning_rate": 1.2462759321248153e-05,
      "loss": 2.6993,
      "step": 111100
    },
    {
      "epoch": 1.1352267390816098,
      "grad_norm": 13.110416412353516,
      "learning_rate": 1.2455941805687173e-05,
      "loss": 2.7222,
      "step": 111200
    },
    {
      "epoch": 1.136247626436899,
      "grad_norm": 18.67574691772461,
      "learning_rate": 1.2449124290126194e-05,
      "loss": 2.7018,
      "step": 111300
    },
    {
      "epoch": 1.137268513792188,
      "grad_norm": 14.88494873046875,
      "learning_rate": 1.2442306774565214e-05,
      "loss": 2.685,
      "step": 111400
    },
    {
      "epoch": 1.1382894011474773,
      "grad_norm": 16.65949249267578,
      "learning_rate": 1.2435489259004235e-05,
      "loss": 2.687,
      "step": 111500
    },
    {
      "epoch": 1.1393102885027666,
      "grad_norm": 13.813456535339355,
      "learning_rate": 1.2428671743443255e-05,
      "loss": 2.7089,
      "step": 111600
    },
    {
      "epoch": 1.1403311758580559,
      "grad_norm": 15.012353897094727,
      "learning_rate": 1.2421854227882274e-05,
      "loss": 2.6908,
      "step": 111700
    },
    {
      "epoch": 1.1413520632133451,
      "grad_norm": 14.086837768554688,
      "learning_rate": 1.2415036712321297e-05,
      "loss": 2.6665,
      "step": 111800
    },
    {
      "epoch": 1.1423729505686342,
      "grad_norm": 20.673534393310547,
      "learning_rate": 1.2408219196760319e-05,
      "loss": 2.7189,
      "step": 111900
    },
    {
      "epoch": 1.1433938379239235,
      "grad_norm": 19.028539657592773,
      "learning_rate": 1.2401401681199338e-05,
      "loss": 2.7372,
      "step": 112000
    },
    {
      "epoch": 1.1444147252792127,
      "grad_norm": 12.529641151428223,
      "learning_rate": 1.239458416563836e-05,
      "loss": 2.6667,
      "step": 112100
    },
    {
      "epoch": 1.145435612634502,
      "grad_norm": 17.98381233215332,
      "learning_rate": 1.238776665007738e-05,
      "loss": 2.7151,
      "step": 112200
    },
    {
      "epoch": 1.146456499989791,
      "grad_norm": 17.2744083404541,
      "learning_rate": 1.23809491345164e-05,
      "loss": 2.521,
      "step": 112300
    },
    {
      "epoch": 1.1474773873450803,
      "grad_norm": 17.002592086791992,
      "learning_rate": 1.237413161895542e-05,
      "loss": 2.6766,
      "step": 112400
    },
    {
      "epoch": 1.1484982747003696,
      "grad_norm": 18.320541381835938,
      "learning_rate": 1.2367314103394441e-05,
      "loss": 2.7064,
      "step": 112500
    },
    {
      "epoch": 1.1495191620556588,
      "grad_norm": 12.609657287597656,
      "learning_rate": 1.2360496587833464e-05,
      "loss": 2.7974,
      "step": 112600
    },
    {
      "epoch": 1.1505400494109481,
      "grad_norm": 17.27069664001465,
      "learning_rate": 1.2353679072272484e-05,
      "loss": 2.7286,
      "step": 112700
    },
    {
      "epoch": 1.1515609367662372,
      "grad_norm": 16.938735961914062,
      "learning_rate": 1.2346861556711505e-05,
      "loss": 2.6655,
      "step": 112800
    },
    {
      "epoch": 1.1525818241215264,
      "grad_norm": 17.49481773376465,
      "learning_rate": 1.2340044041150525e-05,
      "loss": 2.5815,
      "step": 112900
    },
    {
      "epoch": 1.1536027114768157,
      "grad_norm": 16.97555160522461,
      "learning_rate": 1.2333294700745156e-05,
      "loss": 2.6739,
      "step": 113000
    },
    {
      "epoch": 1.1546235988321047,
      "grad_norm": 12.968306541442871,
      "learning_rate": 1.2326477185184176e-05,
      "loss": 2.6683,
      "step": 113100
    },
    {
      "epoch": 1.155644486187394,
      "grad_norm": 15.123319625854492,
      "learning_rate": 1.2319659669623197e-05,
      "loss": 2.7704,
      "step": 113200
    },
    {
      "epoch": 1.1566653735426833,
      "grad_norm": 12.8020601272583,
      "learning_rate": 1.2312842154062217e-05,
      "loss": 2.6802,
      "step": 113300
    },
    {
      "epoch": 1.1576862608979726,
      "grad_norm": 18.046743392944336,
      "learning_rate": 1.230602463850124e-05,
      "loss": 2.6786,
      "step": 113400
    },
    {
      "epoch": 1.1587071482532618,
      "grad_norm": 20.09453010559082,
      "learning_rate": 1.229920712294026e-05,
      "loss": 2.7043,
      "step": 113500
    },
    {
      "epoch": 1.1597280356085509,
      "grad_norm": 18.8071346282959,
      "learning_rate": 1.229238960737928e-05,
      "loss": 2.597,
      "step": 113600
    },
    {
      "epoch": 1.1607489229638401,
      "grad_norm": 18.403785705566406,
      "learning_rate": 1.22855720918183e-05,
      "loss": 2.67,
      "step": 113700
    },
    {
      "epoch": 1.1617698103191294,
      "grad_norm": 19.249441146850586,
      "learning_rate": 1.2278754576257321e-05,
      "loss": 2.6862,
      "step": 113800
    },
    {
      "epoch": 1.1627906976744187,
      "grad_norm": 14.416962623596191,
      "learning_rate": 1.2271937060696341e-05,
      "loss": 2.6921,
      "step": 113900
    },
    {
      "epoch": 1.1638115850297077,
      "grad_norm": 16.543764114379883,
      "learning_rate": 1.2265119545135362e-05,
      "loss": 2.686,
      "step": 114000
    },
    {
      "epoch": 1.164832472384997,
      "grad_norm": 16.216100692749023,
      "learning_rate": 1.2258302029574382e-05,
      "loss": 2.6613,
      "step": 114100
    },
    {
      "epoch": 1.1658533597402863,
      "grad_norm": 13.984763145446777,
      "learning_rate": 1.2251484514013405e-05,
      "loss": 2.6964,
      "step": 114200
    },
    {
      "epoch": 1.1668742470955755,
      "grad_norm": 13.727093696594238,
      "learning_rate": 1.2244666998452426e-05,
      "loss": 2.6384,
      "step": 114300
    },
    {
      "epoch": 1.1678951344508648,
      "grad_norm": 15.865257263183594,
      "learning_rate": 1.2237849482891446e-05,
      "loss": 2.6754,
      "step": 114400
    },
    {
      "epoch": 1.1689160218061538,
      "grad_norm": 15.287528991699219,
      "learning_rate": 1.2231031967330467e-05,
      "loss": 2.6454,
      "step": 114500
    },
    {
      "epoch": 1.169936909161443,
      "grad_norm": 13.4335298538208,
      "learning_rate": 1.2224214451769487e-05,
      "loss": 2.7074,
      "step": 114600
    },
    {
      "epoch": 1.1709577965167324,
      "grad_norm": 11.389141082763672,
      "learning_rate": 1.2217396936208508e-05,
      "loss": 2.5461,
      "step": 114700
    },
    {
      "epoch": 1.1719786838720216,
      "grad_norm": 15.574409484863281,
      "learning_rate": 1.2210579420647528e-05,
      "loss": 2.6568,
      "step": 114800
    },
    {
      "epoch": 1.1729995712273107,
      "grad_norm": 20.386333465576172,
      "learning_rate": 1.2203761905086549e-05,
      "loss": 2.6126,
      "step": 114900
    },
    {
      "epoch": 1.1740204585826,
      "grad_norm": 13.27336311340332,
      "learning_rate": 1.2196944389525569e-05,
      "loss": 2.6259,
      "step": 115000
    },
    {
      "epoch": 1.1750413459378892,
      "grad_norm": 16.109622955322266,
      "learning_rate": 1.2190126873964592e-05,
      "loss": 2.6365,
      "step": 115100
    },
    {
      "epoch": 1.1760622332931785,
      "grad_norm": 13.401632308959961,
      "learning_rate": 1.2183309358403611e-05,
      "loss": 2.6709,
      "step": 115200
    },
    {
      "epoch": 1.1770831206484678,
      "grad_norm": 17.060550689697266,
      "learning_rate": 1.2176491842842633e-05,
      "loss": 2.6633,
      "step": 115300
    },
    {
      "epoch": 1.1781040080037568,
      "grad_norm": 17.954124450683594,
      "learning_rate": 1.2169674327281652e-05,
      "loss": 2.6413,
      "step": 115400
    },
    {
      "epoch": 1.179124895359046,
      "grad_norm": 18.438514709472656,
      "learning_rate": 1.2162856811720674e-05,
      "loss": 2.62,
      "step": 115500
    },
    {
      "epoch": 1.1801457827143353,
      "grad_norm": 16.658994674682617,
      "learning_rate": 1.2156039296159693e-05,
      "loss": 2.6188,
      "step": 115600
    },
    {
      "epoch": 1.1811666700696246,
      "grad_norm": 16.888790130615234,
      "learning_rate": 1.2149221780598715e-05,
      "loss": 2.5997,
      "step": 115700
    },
    {
      "epoch": 1.1821875574249137,
      "grad_norm": 19.164779663085938,
      "learning_rate": 1.2142404265037734e-05,
      "loss": 2.6214,
      "step": 115800
    },
    {
      "epoch": 1.183208444780203,
      "grad_norm": 15.12706470489502,
      "learning_rate": 1.2135586749476757e-05,
      "loss": 2.7492,
      "step": 115900
    },
    {
      "epoch": 1.1842293321354922,
      "grad_norm": 25.437419891357422,
      "learning_rate": 1.2128769233915778e-05,
      "loss": 2.7357,
      "step": 116000
    },
    {
      "epoch": 1.1852502194907815,
      "grad_norm": 13.53628158569336,
      "learning_rate": 1.2121951718354798e-05,
      "loss": 2.6785,
      "step": 116100
    },
    {
      "epoch": 1.1862711068460705,
      "grad_norm": 20.466291427612305,
      "learning_rate": 1.211513420279382e-05,
      "loss": 2.6651,
      "step": 116200
    },
    {
      "epoch": 1.1872919942013598,
      "grad_norm": 16.73162269592285,
      "learning_rate": 1.2108316687232839e-05,
      "loss": 2.7038,
      "step": 116300
    },
    {
      "epoch": 1.188312881556649,
      "grad_norm": 12.950576782226562,
      "learning_rate": 1.210149917167186e-05,
      "loss": 2.6911,
      "step": 116400
    },
    {
      "epoch": 1.1893337689119383,
      "grad_norm": 15.325053215026855,
      "learning_rate": 1.209468165611088e-05,
      "loss": 2.6407,
      "step": 116500
    },
    {
      "epoch": 1.1903546562672274,
      "grad_norm": 15.346906661987305,
      "learning_rate": 1.2087864140549901e-05,
      "loss": 2.7138,
      "step": 116600
    },
    {
      "epoch": 1.1913755436225166,
      "grad_norm": 17.151535034179688,
      "learning_rate": 1.2081046624988924e-05,
      "loss": 2.6541,
      "step": 116700
    },
    {
      "epoch": 1.192396430977806,
      "grad_norm": 16.12610626220703,
      "learning_rate": 1.2074229109427944e-05,
      "loss": 2.6274,
      "step": 116800
    },
    {
      "epoch": 1.1934173183330952,
      "grad_norm": 16.957561492919922,
      "learning_rate": 1.2067411593866965e-05,
      "loss": 2.6569,
      "step": 116900
    },
    {
      "epoch": 1.1944382056883844,
      "grad_norm": 15.64954662322998,
      "learning_rate": 1.2060594078305985e-05,
      "loss": 2.701,
      "step": 117000
    },
    {
      "epoch": 1.1954590930436735,
      "grad_norm": 16.25994300842285,
      "learning_rate": 1.2053844737900616e-05,
      "loss": 2.6751,
      "step": 117100
    },
    {
      "epoch": 1.1964799803989628,
      "grad_norm": 14.154044151306152,
      "learning_rate": 1.2047027222339636e-05,
      "loss": 2.6313,
      "step": 117200
    },
    {
      "epoch": 1.197500867754252,
      "grad_norm": 13.182552337646484,
      "learning_rate": 1.2040209706778657e-05,
      "loss": 2.6205,
      "step": 117300
    },
    {
      "epoch": 1.1985217551095413,
      "grad_norm": 16.562532424926758,
      "learning_rate": 1.2033460366373286e-05,
      "loss": 2.7273,
      "step": 117400
    },
    {
      "epoch": 1.1995426424648303,
      "grad_norm": 12.787728309631348,
      "learning_rate": 1.202664285081231e-05,
      "loss": 2.7511,
      "step": 117500
    },
    {
      "epoch": 1.2005635298201196,
      "grad_norm": 14.407898902893066,
      "learning_rate": 1.2019825335251329e-05,
      "loss": 2.6891,
      "step": 117600
    },
    {
      "epoch": 1.2015844171754089,
      "grad_norm": 15.722762107849121,
      "learning_rate": 1.201300781969035e-05,
      "loss": 2.6201,
      "step": 117700
    },
    {
      "epoch": 1.2026053045306981,
      "grad_norm": 21.78653907775879,
      "learning_rate": 1.200619030412937e-05,
      "loss": 2.5945,
      "step": 117800
    },
    {
      "epoch": 1.2036261918859874,
      "grad_norm": 12.310991287231445,
      "learning_rate": 1.1999372788568391e-05,
      "loss": 2.7452,
      "step": 117900
    },
    {
      "epoch": 1.2046470792412765,
      "grad_norm": 13.505036354064941,
      "learning_rate": 1.199255527300741e-05,
      "loss": 2.6547,
      "step": 118000
    },
    {
      "epoch": 1.2056679665965657,
      "grad_norm": 14.954200744628906,
      "learning_rate": 1.1985805932602042e-05,
      "loss": 2.6893,
      "step": 118100
    },
    {
      "epoch": 1.206688853951855,
      "grad_norm": 16.39198875427246,
      "learning_rate": 1.1978988417041061e-05,
      "loss": 2.6578,
      "step": 118200
    },
    {
      "epoch": 1.2077097413071443,
      "grad_norm": 16.188661575317383,
      "learning_rate": 1.1972170901480084e-05,
      "loss": 2.6775,
      "step": 118300
    },
    {
      "epoch": 1.2087306286624333,
      "grad_norm": 14.852530479431152,
      "learning_rate": 1.1965353385919106e-05,
      "loss": 2.6172,
      "step": 118400
    },
    {
      "epoch": 1.2097515160177226,
      "grad_norm": 15.755767822265625,
      "learning_rate": 1.1958535870358125e-05,
      "loss": 2.6892,
      "step": 118500
    },
    {
      "epoch": 1.2107724033730118,
      "grad_norm": 15.270221710205078,
      "learning_rate": 1.1951718354797147e-05,
      "loss": 2.6318,
      "step": 118600
    },
    {
      "epoch": 1.211793290728301,
      "grad_norm": 14.597475051879883,
      "learning_rate": 1.1944900839236166e-05,
      "loss": 2.6331,
      "step": 118700
    },
    {
      "epoch": 1.2128141780835904,
      "grad_norm": 17.677345275878906,
      "learning_rate": 1.1938083323675188e-05,
      "loss": 2.6812,
      "step": 118800
    },
    {
      "epoch": 1.2138350654388794,
      "grad_norm": 15.975272178649902,
      "learning_rate": 1.1931265808114207e-05,
      "loss": 2.6147,
      "step": 118900
    },
    {
      "epoch": 1.2148559527941687,
      "grad_norm": 13.670641899108887,
      "learning_rate": 1.1924448292553229e-05,
      "loss": 2.6359,
      "step": 119000
    },
    {
      "epoch": 1.215876840149458,
      "grad_norm": 17.998027801513672,
      "learning_rate": 1.1917630776992248e-05,
      "loss": 2.6025,
      "step": 119100
    },
    {
      "epoch": 1.216897727504747,
      "grad_norm": 17.543622970581055,
      "learning_rate": 1.1910813261431271e-05,
      "loss": 2.6638,
      "step": 119200
    },
    {
      "epoch": 1.2179186148600363,
      "grad_norm": 18.720947265625,
      "learning_rate": 1.190399574587029e-05,
      "loss": 2.5873,
      "step": 119300
    },
    {
      "epoch": 1.2189395022153255,
      "grad_norm": 17.88300895690918,
      "learning_rate": 1.1897178230309312e-05,
      "loss": 2.6056,
      "step": 119400
    },
    {
      "epoch": 1.2199603895706148,
      "grad_norm": 16.06914520263672,
      "learning_rate": 1.1890360714748332e-05,
      "loss": 2.6746,
      "step": 119500
    },
    {
      "epoch": 1.220981276925904,
      "grad_norm": 13.72918701171875,
      "learning_rate": 1.1883543199187353e-05,
      "loss": 2.6116,
      "step": 119600
    },
    {
      "epoch": 1.2220021642811931,
      "grad_norm": 11.212284088134766,
      "learning_rate": 1.1876725683626373e-05,
      "loss": 2.6285,
      "step": 119700
    },
    {
      "epoch": 1.2230230516364824,
      "grad_norm": 15.902050971984863,
      "learning_rate": 1.1869908168065394e-05,
      "loss": 2.6513,
      "step": 119800
    },
    {
      "epoch": 1.2240439389917717,
      "grad_norm": 13.739212989807129,
      "learning_rate": 1.1863090652504414e-05,
      "loss": 2.6961,
      "step": 119900
    },
    {
      "epoch": 1.225064826347061,
      "grad_norm": 13.769827842712402,
      "learning_rate": 1.1856273136943437e-05,
      "loss": 2.6434,
      "step": 120000
    },
    {
      "epoch": 1.22608571370235,
      "grad_norm": 13.455000877380371,
      "learning_rate": 1.1849455621382458e-05,
      "loss": 2.6612,
      "step": 120100
    },
    {
      "epoch": 1.2271066010576392,
      "grad_norm": 15.06938648223877,
      "learning_rate": 1.1842638105821478e-05,
      "loss": 2.6594,
      "step": 120200
    },
    {
      "epoch": 1.2281274884129285,
      "grad_norm": 14.482141494750977,
      "learning_rate": 1.1835888765416109e-05,
      "loss": 2.6273,
      "step": 120300
    },
    {
      "epoch": 1.2291483757682178,
      "grad_norm": 15.40147876739502,
      "learning_rate": 1.1829071249855128e-05,
      "loss": 2.7008,
      "step": 120400
    },
    {
      "epoch": 1.230169263123507,
      "grad_norm": 16.864606857299805,
      "learning_rate": 1.182225373429415e-05,
      "loss": 2.6209,
      "step": 120500
    },
    {
      "epoch": 1.231190150478796,
      "grad_norm": 15.03254508972168,
      "learning_rate": 1.1815436218733169e-05,
      "loss": 2.6379,
      "step": 120600
    },
    {
      "epoch": 1.2322110378340854,
      "grad_norm": 15.866860389709473,
      "learning_rate": 1.180861870317219e-05,
      "loss": 2.5796,
      "step": 120700
    },
    {
      "epoch": 1.2332319251893746,
      "grad_norm": 14.363838195800781,
      "learning_rate": 1.1801801187611213e-05,
      "loss": 2.7094,
      "step": 120800
    },
    {
      "epoch": 1.234252812544664,
      "grad_norm": 14.85093879699707,
      "learning_rate": 1.1794983672050233e-05,
      "loss": 2.7297,
      "step": 120900
    },
    {
      "epoch": 1.235273699899953,
      "grad_norm": 14.377129554748535,
      "learning_rate": 1.1788166156489254e-05,
      "loss": 2.6375,
      "step": 121000
    },
    {
      "epoch": 1.2362945872552422,
      "grad_norm": 15.519455909729004,
      "learning_rate": 1.1781348640928274e-05,
      "loss": 2.6595,
      "step": 121100
    },
    {
      "epoch": 1.2373154746105315,
      "grad_norm": 14.926533699035645,
      "learning_rate": 1.1774531125367295e-05,
      "loss": 2.6538,
      "step": 121200
    },
    {
      "epoch": 1.2383363619658208,
      "grad_norm": 16.94647216796875,
      "learning_rate": 1.1767713609806315e-05,
      "loss": 2.5365,
      "step": 121300
    },
    {
      "epoch": 1.23935724932111,
      "grad_norm": 19.184982299804688,
      "learning_rate": 1.1760896094245335e-05,
      "loss": 2.7186,
      "step": 121400
    },
    {
      "epoch": 1.240378136676399,
      "grad_norm": 16.028535842895508,
      "learning_rate": 1.1754078578684356e-05,
      "loss": 2.6566,
      "step": 121500
    },
    {
      "epoch": 1.2413990240316883,
      "grad_norm": 15.175613403320312,
      "learning_rate": 1.1747261063123379e-05,
      "loss": 2.7136,
      "step": 121600
    },
    {
      "epoch": 1.2424199113869776,
      "grad_norm": 21.210548400878906,
      "learning_rate": 1.1740443547562398e-05,
      "loss": 2.6296,
      "step": 121700
    },
    {
      "epoch": 1.2434407987422669,
      "grad_norm": 11.16498851776123,
      "learning_rate": 1.173362603200142e-05,
      "loss": 2.6269,
      "step": 121800
    },
    {
      "epoch": 1.244461686097556,
      "grad_norm": 16.563791275024414,
      "learning_rate": 1.172680851644044e-05,
      "loss": 2.613,
      "step": 121900
    },
    {
      "epoch": 1.2454825734528452,
      "grad_norm": 15.186291694641113,
      "learning_rate": 1.171999100087946e-05,
      "loss": 2.6144,
      "step": 122000
    },
    {
      "epoch": 1.2465034608081345,
      "grad_norm": 14.589505195617676,
      "learning_rate": 1.171317348531848e-05,
      "loss": 2.594,
      "step": 122100
    },
    {
      "epoch": 1.2475243481634237,
      "grad_norm": 13.508179664611816,
      "learning_rate": 1.1706355969757502e-05,
      "loss": 2.658,
      "step": 122200
    },
    {
      "epoch": 1.2485452355187128,
      "grad_norm": 18.65370750427246,
      "learning_rate": 1.1699538454196521e-05,
      "loss": 2.6391,
      "step": 122300
    },
    {
      "epoch": 1.249566122874002,
      "grad_norm": 18.87504768371582,
      "learning_rate": 1.1692720938635544e-05,
      "loss": 2.6246,
      "step": 122400
    },
    {
      "epoch": 1.2505870102292913,
      "grad_norm": 13.024774551391602,
      "learning_rate": 1.1685903423074566e-05,
      "loss": 2.7111,
      "step": 122500
    },
    {
      "epoch": 1.2516078975845806,
      "grad_norm": 15.451045036315918,
      "learning_rate": 1.1679085907513585e-05,
      "loss": 2.6299,
      "step": 122600
    },
    {
      "epoch": 1.2526287849398696,
      "grad_norm": 12.73755931854248,
      "learning_rate": 1.1672268391952607e-05,
      "loss": 2.6479,
      "step": 122700
    },
    {
      "epoch": 1.253649672295159,
      "grad_norm": 13.091963768005371,
      "learning_rate": 1.1665450876391626e-05,
      "loss": 2.6594,
      "step": 122800
    },
    {
      "epoch": 1.2546705596504482,
      "grad_norm": 18.141334533691406,
      "learning_rate": 1.1658633360830647e-05,
      "loss": 2.6655,
      "step": 122900
    },
    {
      "epoch": 1.2556914470057374,
      "grad_norm": 17.8697509765625,
      "learning_rate": 1.1651815845269667e-05,
      "loss": 2.569,
      "step": 123000
    },
    {
      "epoch": 1.2567123343610267,
      "grad_norm": 16.021785736083984,
      "learning_rate": 1.1644998329708688e-05,
      "loss": 2.6404,
      "step": 123100
    },
    {
      "epoch": 1.2577332217163157,
      "grad_norm": 14.657705307006836,
      "learning_rate": 1.1638180814147708e-05,
      "loss": 2.6842,
      "step": 123200
    },
    {
      "epoch": 1.258754109071605,
      "grad_norm": 17.583890914916992,
      "learning_rate": 1.1631363298586731e-05,
      "loss": 2.6501,
      "step": 123300
    },
    {
      "epoch": 1.2597749964268943,
      "grad_norm": 14.900742530822754,
      "learning_rate": 1.162454578302575e-05,
      "loss": 2.6918,
      "step": 123400
    },
    {
      "epoch": 1.2607958837821835,
      "grad_norm": 20.70740509033203,
      "learning_rate": 1.1617728267464772e-05,
      "loss": 2.7263,
      "step": 123500
    },
    {
      "epoch": 1.2618167711374726,
      "grad_norm": 15.18718147277832,
      "learning_rate": 1.1610910751903792e-05,
      "loss": 2.6618,
      "step": 123600
    },
    {
      "epoch": 1.2628376584927619,
      "grad_norm": 15.258893013000488,
      "learning_rate": 1.1604093236342813e-05,
      "loss": 2.6725,
      "step": 123700
    },
    {
      "epoch": 1.2638585458480511,
      "grad_norm": 13.070425033569336,
      "learning_rate": 1.1597275720781833e-05,
      "loss": 2.6213,
      "step": 123800
    },
    {
      "epoch": 1.2648794332033404,
      "grad_norm": 15.025675773620605,
      "learning_rate": 1.1590458205220854e-05,
      "loss": 2.6357,
      "step": 123900
    },
    {
      "epoch": 1.2659003205586297,
      "grad_norm": 16.148412704467773,
      "learning_rate": 1.1583640689659873e-05,
      "loss": 2.7378,
      "step": 124000
    },
    {
      "epoch": 1.2669212079139187,
      "grad_norm": 17.948699951171875,
      "learning_rate": 1.1576823174098896e-05,
      "loss": 2.6215,
      "step": 124100
    },
    {
      "epoch": 1.267942095269208,
      "grad_norm": 16.18260383605957,
      "learning_rate": 1.1570005658537918e-05,
      "loss": 2.6049,
      "step": 124200
    },
    {
      "epoch": 1.2689629826244972,
      "grad_norm": 16.917186737060547,
      "learning_rate": 1.1563188142976937e-05,
      "loss": 2.6269,
      "step": 124300
    },
    {
      "epoch": 1.2699838699797863,
      "grad_norm": 12.659624099731445,
      "learning_rate": 1.1556370627415959e-05,
      "loss": 2.6628,
      "step": 124400
    },
    {
      "epoch": 1.2710047573350756,
      "grad_norm": 16.974058151245117,
      "learning_rate": 1.1549553111854978e-05,
      "loss": 2.5769,
      "step": 124500
    },
    {
      "epoch": 1.2720256446903648,
      "grad_norm": 14.935291290283203,
      "learning_rate": 1.1542735596294e-05,
      "loss": 2.7002,
      "step": 124600
    },
    {
      "epoch": 1.273046532045654,
      "grad_norm": 13.880718231201172,
      "learning_rate": 1.153591808073302e-05,
      "loss": 2.6702,
      "step": 124700
    },
    {
      "epoch": 1.2740674194009434,
      "grad_norm": 19.65109634399414,
      "learning_rate": 1.152910056517204e-05,
      "loss": 2.6126,
      "step": 124800
    },
    {
      "epoch": 1.2750883067562326,
      "grad_norm": 19.914226531982422,
      "learning_rate": 1.1522283049611062e-05,
      "loss": 2.6435,
      "step": 124900
    },
    {
      "epoch": 1.2761091941115217,
      "grad_norm": 18.548988342285156,
      "learning_rate": 1.1515465534050083e-05,
      "loss": 2.7096,
      "step": 125000
    },
    {
      "epoch": 1.277130081466811,
      "grad_norm": 12.587187767028809,
      "learning_rate": 1.1508648018489103e-05,
      "loss": 2.649,
      "step": 125100
    },
    {
      "epoch": 1.2781509688221002,
      "grad_norm": 13.94179916381836,
      "learning_rate": 1.1501830502928124e-05,
      "loss": 2.685,
      "step": 125200
    },
    {
      "epoch": 1.2791718561773893,
      "grad_norm": 13.807997703552246,
      "learning_rate": 1.1495012987367144e-05,
      "loss": 2.6793,
      "step": 125300
    },
    {
      "epoch": 1.2801927435326785,
      "grad_norm": 14.993317604064941,
      "learning_rate": 1.1488195471806165e-05,
      "loss": 2.6314,
      "step": 125400
    },
    {
      "epoch": 1.2812136308879678,
      "grad_norm": 16.664108276367188,
      "learning_rate": 1.1481377956245185e-05,
      "loss": 2.6511,
      "step": 125500
    },
    {
      "epoch": 1.282234518243257,
      "grad_norm": 13.06315803527832,
      "learning_rate": 1.1474560440684206e-05,
      "loss": 2.6784,
      "step": 125600
    },
    {
      "epoch": 1.2832554055985463,
      "grad_norm": 14.66010856628418,
      "learning_rate": 1.1467742925123229e-05,
      "loss": 2.6516,
      "step": 125700
    },
    {
      "epoch": 1.2842762929538356,
      "grad_norm": 14.506097793579102,
      "learning_rate": 1.1460925409562249e-05,
      "loss": 2.6544,
      "step": 125800
    },
    {
      "epoch": 1.2852971803091247,
      "grad_norm": 15.57618236541748,
      "learning_rate": 1.145410789400127e-05,
      "loss": 2.5913,
      "step": 125900
    },
    {
      "epoch": 1.286318067664414,
      "grad_norm": 13.908544540405273,
      "learning_rate": 1.144729037844029e-05,
      "loss": 2.6525,
      "step": 126000
    },
    {
      "epoch": 1.2873389550197032,
      "grad_norm": 12.919052124023438,
      "learning_rate": 1.144047286287931e-05,
      "loss": 2.6566,
      "step": 126100
    },
    {
      "epoch": 1.2883598423749922,
      "grad_norm": 16.288368225097656,
      "learning_rate": 1.143372352247394e-05,
      "loss": 2.5711,
      "step": 126200
    },
    {
      "epoch": 1.2893807297302815,
      "grad_norm": 12.736344337463379,
      "learning_rate": 1.1426906006912962e-05,
      "loss": 2.6428,
      "step": 126300
    },
    {
      "epoch": 1.2904016170855708,
      "grad_norm": 13.236665725708008,
      "learning_rate": 1.1420088491351981e-05,
      "loss": 2.6479,
      "step": 126400
    },
    {
      "epoch": 1.29142250444086,
      "grad_norm": 17.634532928466797,
      "learning_rate": 1.1413270975791002e-05,
      "loss": 2.6813,
      "step": 126500
    },
    {
      "epoch": 1.2924433917961493,
      "grad_norm": 15.320383071899414,
      "learning_rate": 1.1406453460230025e-05,
      "loss": 2.5437,
      "step": 126600
    },
    {
      "epoch": 1.2934642791514384,
      "grad_norm": 14.774575233459473,
      "learning_rate": 1.1399635944669045e-05,
      "loss": 2.5837,
      "step": 126700
    },
    {
      "epoch": 1.2944851665067276,
      "grad_norm": 13.310240745544434,
      "learning_rate": 1.1392818429108066e-05,
      "loss": 2.6274,
      "step": 126800
    },
    {
      "epoch": 1.295506053862017,
      "grad_norm": 15.954071998596191,
      "learning_rate": 1.1386000913547086e-05,
      "loss": 2.6851,
      "step": 126900
    },
    {
      "epoch": 1.2965269412173062,
      "grad_norm": 15.615145683288574,
      "learning_rate": 1.1379183397986107e-05,
      "loss": 2.6342,
      "step": 127000
    },
    {
      "epoch": 1.2975478285725952,
      "grad_norm": 14.670668601989746,
      "learning_rate": 1.1372365882425127e-05,
      "loss": 2.6785,
      "step": 127100
    },
    {
      "epoch": 1.2985687159278845,
      "grad_norm": 18.582128524780273,
      "learning_rate": 1.1365548366864148e-05,
      "loss": 2.6307,
      "step": 127200
    },
    {
      "epoch": 1.2995896032831737,
      "grad_norm": 16.861188888549805,
      "learning_rate": 1.1358730851303168e-05,
      "loss": 2.6543,
      "step": 127300
    },
    {
      "epoch": 1.300610490638463,
      "grad_norm": 16.495784759521484,
      "learning_rate": 1.1351913335742191e-05,
      "loss": 2.6338,
      "step": 127400
    },
    {
      "epoch": 1.3016313779937523,
      "grad_norm": 19.457807540893555,
      "learning_rate": 1.134509582018121e-05,
      "loss": 2.6702,
      "step": 127500
    },
    {
      "epoch": 1.3026522653490413,
      "grad_norm": 16.021944046020508,
      "learning_rate": 1.1338278304620232e-05,
      "loss": 2.7755,
      "step": 127600
    },
    {
      "epoch": 1.3036731527043306,
      "grad_norm": 16.126907348632812,
      "learning_rate": 1.1331460789059251e-05,
      "loss": 2.6555,
      "step": 127700
    },
    {
      "epoch": 1.3046940400596199,
      "grad_norm": 17.97529411315918,
      "learning_rate": 1.1324643273498273e-05,
      "loss": 2.6332,
      "step": 127800
    },
    {
      "epoch": 1.305714927414909,
      "grad_norm": 10.763903617858887,
      "learning_rate": 1.1317825757937292e-05,
      "loss": 2.7095,
      "step": 127900
    },
    {
      "epoch": 1.3067358147701982,
      "grad_norm": 17.239124298095703,
      "learning_rate": 1.1311008242376314e-05,
      "loss": 2.6567,
      "step": 128000
    },
    {
      "epoch": 1.3077567021254874,
      "grad_norm": 16.385435104370117,
      "learning_rate": 1.1304190726815333e-05,
      "loss": 2.6202,
      "step": 128100
    },
    {
      "epoch": 1.3087775894807767,
      "grad_norm": 13.148412704467773,
      "learning_rate": 1.1297373211254356e-05,
      "loss": 2.5776,
      "step": 128200
    },
    {
      "epoch": 1.309798476836066,
      "grad_norm": 17.392799377441406,
      "learning_rate": 1.1290555695693378e-05,
      "loss": 2.6361,
      "step": 128300
    },
    {
      "epoch": 1.3108193641913553,
      "grad_norm": 16.230005264282227,
      "learning_rate": 1.1283738180132397e-05,
      "loss": 2.6546,
      "step": 128400
    },
    {
      "epoch": 1.3118402515466443,
      "grad_norm": 15.998665809631348,
      "learning_rate": 1.1276920664571419e-05,
      "loss": 2.6476,
      "step": 128500
    },
    {
      "epoch": 1.3128611389019336,
      "grad_norm": 17.883350372314453,
      "learning_rate": 1.1270103149010438e-05,
      "loss": 2.6355,
      "step": 128600
    },
    {
      "epoch": 1.3138820262572228,
      "grad_norm": 13.555705070495605,
      "learning_rate": 1.126328563344946e-05,
      "loss": 2.6163,
      "step": 128700
    },
    {
      "epoch": 1.3149029136125119,
      "grad_norm": 13.378191947937012,
      "learning_rate": 1.1256468117888479e-05,
      "loss": 2.606,
      "step": 128800
    },
    {
      "epoch": 1.3159238009678011,
      "grad_norm": 18.67751121520996,
      "learning_rate": 1.12496506023275e-05,
      "loss": 2.5478,
      "step": 128900
    },
    {
      "epoch": 1.3169446883230904,
      "grad_norm": 20.716707229614258,
      "learning_rate": 1.1242833086766522e-05,
      "loss": 2.6208,
      "step": 129000
    },
    {
      "epoch": 1.3179655756783797,
      "grad_norm": 15.615215301513672,
      "learning_rate": 1.1236015571205543e-05,
      "loss": 2.6369,
      "step": 129100
    },
    {
      "epoch": 1.318986463033669,
      "grad_norm": 17.559228897094727,
      "learning_rate": 1.1229198055644563e-05,
      "loss": 2.5794,
      "step": 129200
    },
    {
      "epoch": 1.320007350388958,
      "grad_norm": 18.596559524536133,
      "learning_rate": 1.1222380540083584e-05,
      "loss": 2.668,
      "step": 129300
    },
    {
      "epoch": 1.3210282377442473,
      "grad_norm": 14.36577033996582,
      "learning_rate": 1.1215563024522604e-05,
      "loss": 2.6461,
      "step": 129400
    },
    {
      "epoch": 1.3220491250995365,
      "grad_norm": 19.616058349609375,
      "learning_rate": 1.1208745508961625e-05,
      "loss": 2.7291,
      "step": 129500
    },
    {
      "epoch": 1.3230700124548258,
      "grad_norm": 16.776371002197266,
      "learning_rate": 1.1201927993400645e-05,
      "loss": 2.708,
      "step": 129600
    },
    {
      "epoch": 1.3240908998101149,
      "grad_norm": 15.877242088317871,
      "learning_rate": 1.1195110477839666e-05,
      "loss": 2.6334,
      "step": 129700
    },
    {
      "epoch": 1.3251117871654041,
      "grad_norm": 18.410484313964844,
      "learning_rate": 1.1188292962278685e-05,
      "loss": 2.667,
      "step": 129800
    },
    {
      "epoch": 1.3261326745206934,
      "grad_norm": 16.439708709716797,
      "learning_rate": 1.1181475446717708e-05,
      "loss": 2.6531,
      "step": 129900
    },
    {
      "epoch": 1.3271535618759827,
      "grad_norm": 16.111677169799805,
      "learning_rate": 1.117472610631234e-05,
      "loss": 2.6918,
      "step": 130000
    },
    {
      "epoch": 1.328174449231272,
      "grad_norm": 16.943883895874023,
      "learning_rate": 1.116790859075136e-05,
      "loss": 2.6302,
      "step": 130100
    },
    {
      "epoch": 1.329195336586561,
      "grad_norm": 12.608589172363281,
      "learning_rate": 1.116109107519038e-05,
      "loss": 2.6352,
      "step": 130200
    },
    {
      "epoch": 1.3302162239418502,
      "grad_norm": 15.766541481018066,
      "learning_rate": 1.11542735596294e-05,
      "loss": 2.5999,
      "step": 130300
    },
    {
      "epoch": 1.3312371112971395,
      "grad_norm": 16.595712661743164,
      "learning_rate": 1.1147456044068421e-05,
      "loss": 2.709,
      "step": 130400
    },
    {
      "epoch": 1.3322579986524286,
      "grad_norm": 14.96491813659668,
      "learning_rate": 1.1140638528507441e-05,
      "loss": 2.6289,
      "step": 130500
    },
    {
      "epoch": 1.3332788860077178,
      "grad_norm": 16.495521545410156,
      "learning_rate": 1.1133821012946462e-05,
      "loss": 2.7412,
      "step": 130600
    },
    {
      "epoch": 1.334299773363007,
      "grad_norm": 12.391351699829102,
      "learning_rate": 1.1127003497385485e-05,
      "loss": 2.6348,
      "step": 130700
    },
    {
      "epoch": 1.3353206607182964,
      "grad_norm": 15.406747817993164,
      "learning_rate": 1.1120185981824505e-05,
      "loss": 2.5587,
      "step": 130800
    },
    {
      "epoch": 1.3363415480735856,
      "grad_norm": 16.41693687438965,
      "learning_rate": 1.1113368466263526e-05,
      "loss": 2.6847,
      "step": 130900
    },
    {
      "epoch": 1.337362435428875,
      "grad_norm": 17.353363037109375,
      "learning_rate": 1.1106550950702546e-05,
      "loss": 2.6424,
      "step": 131000
    },
    {
      "epoch": 1.338383322784164,
      "grad_norm": 19.458858489990234,
      "learning_rate": 1.1099733435141567e-05,
      "loss": 2.6156,
      "step": 131100
    },
    {
      "epoch": 1.3394042101394532,
      "grad_norm": 19.151243209838867,
      "learning_rate": 1.1092915919580587e-05,
      "loss": 2.6062,
      "step": 131200
    },
    {
      "epoch": 1.3404250974947425,
      "grad_norm": 16.941715240478516,
      "learning_rate": 1.1086098404019608e-05,
      "loss": 2.5932,
      "step": 131300
    },
    {
      "epoch": 1.3414459848500315,
      "grad_norm": 19.08378791809082,
      "learning_rate": 1.1079280888458628e-05,
      "loss": 2.5479,
      "step": 131400
    },
    {
      "epoch": 1.3424668722053208,
      "grad_norm": 14.560981750488281,
      "learning_rate": 1.107246337289765e-05,
      "loss": 2.605,
      "step": 131500
    },
    {
      "epoch": 1.34348775956061,
      "grad_norm": 11.8026704788208,
      "learning_rate": 1.106564585733667e-05,
      "loss": 2.5602,
      "step": 131600
    },
    {
      "epoch": 1.3445086469158993,
      "grad_norm": 12.853595733642578,
      "learning_rate": 1.1058828341775692e-05,
      "loss": 2.633,
      "step": 131700
    },
    {
      "epoch": 1.3455295342711886,
      "grad_norm": 15.421479225158691,
      "learning_rate": 1.1052010826214711e-05,
      "loss": 2.7102,
      "step": 131800
    },
    {
      "epoch": 1.3465504216264779,
      "grad_norm": 17.566001892089844,
      "learning_rate": 1.1045193310653733e-05,
      "loss": 2.662,
      "step": 131900
    },
    {
      "epoch": 1.347571308981767,
      "grad_norm": 20.664419174194336,
      "learning_rate": 1.1038375795092752e-05,
      "loss": 2.5327,
      "step": 132000
    },
    {
      "epoch": 1.3485921963370562,
      "grad_norm": 14.529899597167969,
      "learning_rate": 1.1031558279531774e-05,
      "loss": 2.6573,
      "step": 132100
    },
    {
      "epoch": 1.3496130836923454,
      "grad_norm": 17.444293975830078,
      "learning_rate": 1.1024740763970793e-05,
      "loss": 2.5374,
      "step": 132200
    },
    {
      "epoch": 1.3506339710476345,
      "grad_norm": 20.049053192138672,
      "learning_rate": 1.1017923248409816e-05,
      "loss": 2.6002,
      "step": 132300
    },
    {
      "epoch": 1.3516548584029238,
      "grad_norm": 23.508407592773438,
      "learning_rate": 1.1011105732848838e-05,
      "loss": 2.5935,
      "step": 132400
    },
    {
      "epoch": 1.352675745758213,
      "grad_norm": 14.78873348236084,
      "learning_rate": 1.1004288217287857e-05,
      "loss": 2.6011,
      "step": 132500
    },
    {
      "epoch": 1.3536966331135023,
      "grad_norm": 14.771771430969238,
      "learning_rate": 1.0997470701726878e-05,
      "loss": 2.5931,
      "step": 132600
    },
    {
      "epoch": 1.3547175204687916,
      "grad_norm": 19.492755889892578,
      "learning_rate": 1.0990653186165898e-05,
      "loss": 2.6568,
      "step": 132700
    },
    {
      "epoch": 1.3557384078240806,
      "grad_norm": 16.004093170166016,
      "learning_rate": 1.098383567060492e-05,
      "loss": 2.652,
      "step": 132800
    },
    {
      "epoch": 1.3567592951793699,
      "grad_norm": 16.26226043701172,
      "learning_rate": 1.0977018155043939e-05,
      "loss": 2.6573,
      "step": 132900
    },
    {
      "epoch": 1.3577801825346592,
      "grad_norm": 17.24172019958496,
      "learning_rate": 1.097020063948296e-05,
      "loss": 2.7289,
      "step": 133000
    },
    {
      "epoch": 1.3588010698899484,
      "grad_norm": 18.482276916503906,
      "learning_rate": 1.0963383123921982e-05,
      "loss": 2.6778,
      "step": 133100
    },
    {
      "epoch": 1.3598219572452375,
      "grad_norm": 14.423125267028809,
      "learning_rate": 1.0956565608361003e-05,
      "loss": 2.571,
      "step": 133200
    },
    {
      "epoch": 1.3608428446005267,
      "grad_norm": 16.71898078918457,
      "learning_rate": 1.0949748092800023e-05,
      "loss": 2.5863,
      "step": 133300
    },
    {
      "epoch": 1.361863731955816,
      "grad_norm": 15.182494163513184,
      "learning_rate": 1.0942930577239044e-05,
      "loss": 2.5909,
      "step": 133400
    },
    {
      "epoch": 1.3628846193111053,
      "grad_norm": 16.74852180480957,
      "learning_rate": 1.0936113061678063e-05,
      "loss": 2.6015,
      "step": 133500
    },
    {
      "epoch": 1.3639055066663945,
      "grad_norm": 15.852076530456543,
      "learning_rate": 1.0929295546117085e-05,
      "loss": 2.6282,
      "step": 133600
    },
    {
      "epoch": 1.3649263940216836,
      "grad_norm": 15.965361595153809,
      "learning_rate": 1.0922478030556104e-05,
      "loss": 2.6753,
      "step": 133700
    },
    {
      "epoch": 1.3659472813769729,
      "grad_norm": 13.131738662719727,
      "learning_rate": 1.0915660514995126e-05,
      "loss": 2.5279,
      "step": 133800
    },
    {
      "epoch": 1.3669681687322621,
      "grad_norm": 18.15433692932129,
      "learning_rate": 1.0908911174589755e-05,
      "loss": 2.5569,
      "step": 133900
    },
    {
      "epoch": 1.3679890560875512,
      "grad_norm": 15.507404327392578,
      "learning_rate": 1.0902093659028778e-05,
      "loss": 2.6939,
      "step": 134000
    },
    {
      "epoch": 1.3690099434428404,
      "grad_norm": 18.344980239868164,
      "learning_rate": 1.089534431862341e-05,
      "loss": 2.6578,
      "step": 134100
    },
    {
      "epoch": 1.3700308307981297,
      "grad_norm": 16.36617660522461,
      "learning_rate": 1.0888526803062429e-05,
      "loss": 2.6001,
      "step": 134200
    },
    {
      "epoch": 1.371051718153419,
      "grad_norm": 13.82064437866211,
      "learning_rate": 1.088170928750145e-05,
      "loss": 2.5556,
      "step": 134300
    },
    {
      "epoch": 1.3720726055087082,
      "grad_norm": 13.540210723876953,
      "learning_rate": 1.087489177194047e-05,
      "loss": 2.5639,
      "step": 134400
    },
    {
      "epoch": 1.3730934928639975,
      "grad_norm": 11.868975639343262,
      "learning_rate": 1.0868074256379491e-05,
      "loss": 2.6649,
      "step": 134500
    },
    {
      "epoch": 1.3741143802192866,
      "grad_norm": 15.599665641784668,
      "learning_rate": 1.086125674081851e-05,
      "loss": 2.6149,
      "step": 134600
    },
    {
      "epoch": 1.3751352675745758,
      "grad_norm": 14.603610038757324,
      "learning_rate": 1.0854439225257532e-05,
      "loss": 2.7521,
      "step": 134700
    },
    {
      "epoch": 1.376156154929865,
      "grad_norm": 23.421585083007812,
      "learning_rate": 1.0847621709696553e-05,
      "loss": 2.7083,
      "step": 134800
    },
    {
      "epoch": 1.3771770422851541,
      "grad_norm": 14.034978866577148,
      "learning_rate": 1.0840804194135575e-05,
      "loss": 2.5854,
      "step": 134900
    },
    {
      "epoch": 1.3781979296404434,
      "grad_norm": 19.514291763305664,
      "learning_rate": 1.0833986678574594e-05,
      "loss": 2.6419,
      "step": 135000
    },
    {
      "epoch": 1.3792188169957327,
      "grad_norm": 18.3742618560791,
      "learning_rate": 1.0827169163013616e-05,
      "loss": 2.6663,
      "step": 135100
    },
    {
      "epoch": 1.380239704351022,
      "grad_norm": 18.932178497314453,
      "learning_rate": 1.0820351647452635e-05,
      "loss": 2.6309,
      "step": 135200
    },
    {
      "epoch": 1.3812605917063112,
      "grad_norm": 15.814729690551758,
      "learning_rate": 1.0813534131891656e-05,
      "loss": 2.621,
      "step": 135300
    },
    {
      "epoch": 1.3822814790616003,
      "grad_norm": 14.980219841003418,
      "learning_rate": 1.0806716616330676e-05,
      "loss": 2.5749,
      "step": 135400
    },
    {
      "epoch": 1.3833023664168895,
      "grad_norm": 20.221986770629883,
      "learning_rate": 1.0799899100769697e-05,
      "loss": 2.6755,
      "step": 135500
    },
    {
      "epoch": 1.3843232537721788,
      "grad_norm": 16.471637725830078,
      "learning_rate": 1.079308158520872e-05,
      "loss": 2.5286,
      "step": 135600
    },
    {
      "epoch": 1.385344141127468,
      "grad_norm": 16.76689910888672,
      "learning_rate": 1.078626406964774e-05,
      "loss": 2.6663,
      "step": 135700
    },
    {
      "epoch": 1.3863650284827571,
      "grad_norm": 22.369482040405273,
      "learning_rate": 1.0779446554086761e-05,
      "loss": 2.5519,
      "step": 135800
    },
    {
      "epoch": 1.3873859158380464,
      "grad_norm": 15.446617126464844,
      "learning_rate": 1.0772629038525781e-05,
      "loss": 2.695,
      "step": 135900
    },
    {
      "epoch": 1.3884068031933356,
      "grad_norm": 14.668137550354004,
      "learning_rate": 1.0765811522964802e-05,
      "loss": 2.6213,
      "step": 136000
    },
    {
      "epoch": 1.389427690548625,
      "grad_norm": 14.802506446838379,
      "learning_rate": 1.0758994007403822e-05,
      "loss": 2.6216,
      "step": 136100
    },
    {
      "epoch": 1.3904485779039142,
      "grad_norm": 13.210110664367676,
      "learning_rate": 1.0752176491842843e-05,
      "loss": 2.6781,
      "step": 136200
    },
    {
      "epoch": 1.3914694652592032,
      "grad_norm": 14.691317558288574,
      "learning_rate": 1.0745358976281863e-05,
      "loss": 2.6799,
      "step": 136300
    },
    {
      "epoch": 1.3924903526144925,
      "grad_norm": 17.93927001953125,
      "learning_rate": 1.0738541460720886e-05,
      "loss": 2.6479,
      "step": 136400
    },
    {
      "epoch": 1.3935112399697818,
      "grad_norm": 15.000755310058594,
      "learning_rate": 1.0731723945159907e-05,
      "loss": 2.5802,
      "step": 136500
    },
    {
      "epoch": 1.3945321273250708,
      "grad_norm": 15.751976013183594,
      "learning_rate": 1.0724906429598927e-05,
      "loss": 2.6848,
      "step": 136600
    },
    {
      "epoch": 1.39555301468036,
      "grad_norm": 15.609201431274414,
      "learning_rate": 1.0718088914037948e-05,
      "loss": 2.5731,
      "step": 136700
    },
    {
      "epoch": 1.3965739020356494,
      "grad_norm": 17.26260757446289,
      "learning_rate": 1.0711271398476968e-05,
      "loss": 2.6823,
      "step": 136800
    },
    {
      "epoch": 1.3975947893909386,
      "grad_norm": 15.903536796569824,
      "learning_rate": 1.0704453882915989e-05,
      "loss": 2.6378,
      "step": 136900
    },
    {
      "epoch": 1.3986156767462279,
      "grad_norm": 14.803718566894531,
      "learning_rate": 1.0697636367355009e-05,
      "loss": 2.5778,
      "step": 137000
    },
    {
      "epoch": 1.3996365641015172,
      "grad_norm": 14.83111572265625,
      "learning_rate": 1.069081885179403e-05,
      "loss": 2.5993,
      "step": 137100
    },
    {
      "epoch": 1.4006574514568062,
      "grad_norm": 14.570027351379395,
      "learning_rate": 1.0684069511388661e-05,
      "loss": 2.5629,
      "step": 137200
    },
    {
      "epoch": 1.4016783388120955,
      "grad_norm": 17.667665481567383,
      "learning_rate": 1.0677251995827682e-05,
      "loss": 2.6343,
      "step": 137300
    },
    {
      "epoch": 1.4026992261673847,
      "grad_norm": 24.518457412719727,
      "learning_rate": 1.0670434480266702e-05,
      "loss": 2.6697,
      "step": 137400
    },
    {
      "epoch": 1.4037201135226738,
      "grad_norm": 16.217817306518555,
      "learning_rate": 1.0663616964705723e-05,
      "loss": 2.5701,
      "step": 137500
    },
    {
      "epoch": 1.404741000877963,
      "grad_norm": 17.173349380493164,
      "learning_rate": 1.0656799449144743e-05,
      "loss": 2.5759,
      "step": 137600
    },
    {
      "epoch": 1.4057618882332523,
      "grad_norm": 18.105640411376953,
      "learning_rate": 1.0649981933583764e-05,
      "loss": 2.7454,
      "step": 137700
    },
    {
      "epoch": 1.4067827755885416,
      "grad_norm": 14.526585578918457,
      "learning_rate": 1.0643164418022784e-05,
      "loss": 2.6465,
      "step": 137800
    },
    {
      "epoch": 1.4078036629438309,
      "grad_norm": 15.374485969543457,
      "learning_rate": 1.0636346902461805e-05,
      "loss": 2.6061,
      "step": 137900
    },
    {
      "epoch": 1.4088245502991201,
      "grad_norm": 16.810712814331055,
      "learning_rate": 1.0629529386900825e-05,
      "loss": 2.6349,
      "step": 138000
    },
    {
      "epoch": 1.4098454376544092,
      "grad_norm": 16.826650619506836,
      "learning_rate": 1.0622711871339848e-05,
      "loss": 2.6002,
      "step": 138100
    },
    {
      "epoch": 1.4108663250096984,
      "grad_norm": 13.852069854736328,
      "learning_rate": 1.0615894355778869e-05,
      "loss": 2.6189,
      "step": 138200
    },
    {
      "epoch": 1.4118872123649877,
      "grad_norm": 21.02276039123535,
      "learning_rate": 1.0609076840217889e-05,
      "loss": 2.6183,
      "step": 138300
    },
    {
      "epoch": 1.4129080997202768,
      "grad_norm": 13.175315856933594,
      "learning_rate": 1.060225932465691e-05,
      "loss": 2.6348,
      "step": 138400
    },
    {
      "epoch": 1.413928987075566,
      "grad_norm": 14.999202728271484,
      "learning_rate": 1.059544180909593e-05,
      "loss": 2.5773,
      "step": 138500
    },
    {
      "epoch": 1.4149498744308553,
      "grad_norm": 13.161272048950195,
      "learning_rate": 1.0588624293534951e-05,
      "loss": 2.5613,
      "step": 138600
    },
    {
      "epoch": 1.4159707617861446,
      "grad_norm": 18.541963577270508,
      "learning_rate": 1.058180677797397e-05,
      "loss": 2.596,
      "step": 138700
    },
    {
      "epoch": 1.4169916491414338,
      "grad_norm": 17.10597038269043,
      "learning_rate": 1.0574989262412992e-05,
      "loss": 2.6476,
      "step": 138800
    },
    {
      "epoch": 1.4180125364967229,
      "grad_norm": 19.231945037841797,
      "learning_rate": 1.0568171746852013e-05,
      "loss": 2.6395,
      "step": 138900
    },
    {
      "epoch": 1.4190334238520121,
      "grad_norm": 27.317615509033203,
      "learning_rate": 1.0561354231291034e-05,
      "loss": 2.6306,
      "step": 139000
    },
    {
      "epoch": 1.4200543112073014,
      "grad_norm": 15.457980155944824,
      "learning_rate": 1.0554536715730054e-05,
      "loss": 2.5357,
      "step": 139100
    },
    {
      "epoch": 1.4210751985625907,
      "grad_norm": 18.370119094848633,
      "learning_rate": 1.0547719200169075e-05,
      "loss": 2.6814,
      "step": 139200
    },
    {
      "epoch": 1.4220960859178797,
      "grad_norm": 12.394265174865723,
      "learning_rate": 1.0540901684608095e-05,
      "loss": 2.5636,
      "step": 139300
    },
    {
      "epoch": 1.423116973273169,
      "grad_norm": 16.996562957763672,
      "learning_rate": 1.0534084169047116e-05,
      "loss": 2.5543,
      "step": 139400
    },
    {
      "epoch": 1.4241378606284583,
      "grad_norm": 20.144071578979492,
      "learning_rate": 1.0527266653486136e-05,
      "loss": 2.5799,
      "step": 139500
    },
    {
      "epoch": 1.4251587479837475,
      "grad_norm": 16.956802368164062,
      "learning_rate": 1.0520449137925157e-05,
      "loss": 2.6662,
      "step": 139600
    },
    {
      "epoch": 1.4261796353390368,
      "grad_norm": 16.299114227294922,
      "learning_rate": 1.051363162236418e-05,
      "loss": 2.609,
      "step": 139700
    },
    {
      "epoch": 1.4272005226943258,
      "grad_norm": 15.712653160095215,
      "learning_rate": 1.05068141068032e-05,
      "loss": 2.6311,
      "step": 139800
    },
    {
      "epoch": 1.4282214100496151,
      "grad_norm": 20.51664161682129,
      "learning_rate": 1.0499996591242221e-05,
      "loss": 2.6101,
      "step": 139900
    },
    {
      "epoch": 1.4292422974049044,
      "grad_norm": 19.105741500854492,
      "learning_rate": 1.049317907568124e-05,
      "loss": 2.6122,
      "step": 140000
    },
    {
      "epoch": 1.4302631847601934,
      "grad_norm": 14.028569221496582,
      "learning_rate": 1.0486361560120262e-05,
      "loss": 2.5257,
      "step": 140100
    },
    {
      "epoch": 1.4312840721154827,
      "grad_norm": 17.45443344116211,
      "learning_rate": 1.0479544044559282e-05,
      "loss": 2.5472,
      "step": 140200
    },
    {
      "epoch": 1.432304959470772,
      "grad_norm": 13.817238807678223,
      "learning_rate": 1.0472726528998303e-05,
      "loss": 2.5996,
      "step": 140300
    },
    {
      "epoch": 1.4333258468260612,
      "grad_norm": 15.124445915222168,
      "learning_rate": 1.0465909013437323e-05,
      "loss": 2.6043,
      "step": 140400
    },
    {
      "epoch": 1.4343467341813505,
      "grad_norm": 14.779153823852539,
      "learning_rate": 1.0459091497876346e-05,
      "loss": 2.5699,
      "step": 140500
    },
    {
      "epoch": 1.4353676215366398,
      "grad_norm": 14.270906448364258,
      "learning_rate": 1.0452273982315367e-05,
      "loss": 2.5461,
      "step": 140600
    },
    {
      "epoch": 1.4363885088919288,
      "grad_norm": 14.2079439163208,
      "learning_rate": 1.0445456466754387e-05,
      "loss": 2.5567,
      "step": 140700
    },
    {
      "epoch": 1.437409396247218,
      "grad_norm": 17.358686447143555,
      "learning_rate": 1.0438638951193408e-05,
      "loss": 2.6249,
      "step": 140800
    },
    {
      "epoch": 1.4384302836025074,
      "grad_norm": 18.135791778564453,
      "learning_rate": 1.0431821435632428e-05,
      "loss": 2.6001,
      "step": 140900
    },
    {
      "epoch": 1.4394511709577964,
      "grad_norm": 14.381906509399414,
      "learning_rate": 1.0425003920071449e-05,
      "loss": 2.6102,
      "step": 141000
    },
    {
      "epoch": 1.4404720583130857,
      "grad_norm": 15.410303115844727,
      "learning_rate": 1.0418186404510468e-05,
      "loss": 2.6013,
      "step": 141100
    },
    {
      "epoch": 1.441492945668375,
      "grad_norm": 17.737045288085938,
      "learning_rate": 1.041136888894949e-05,
      "loss": 2.6035,
      "step": 141200
    },
    {
      "epoch": 1.4425138330236642,
      "grad_norm": 14.647753715515137,
      "learning_rate": 1.040455137338851e-05,
      "loss": 2.6182,
      "step": 141300
    },
    {
      "epoch": 1.4435347203789535,
      "grad_norm": 17.738754272460938,
      "learning_rate": 1.0397733857827532e-05,
      "loss": 2.4815,
      "step": 141400
    },
    {
      "epoch": 1.4445556077342427,
      "grad_norm": 12.981168746948242,
      "learning_rate": 1.0390916342266552e-05,
      "loss": 2.7601,
      "step": 141500
    },
    {
      "epoch": 1.4455764950895318,
      "grad_norm": 16.10959243774414,
      "learning_rate": 1.0384098826705573e-05,
      "loss": 2.6179,
      "step": 141600
    },
    {
      "epoch": 1.446597382444821,
      "grad_norm": 14.504443168640137,
      "learning_rate": 1.0377281311144593e-05,
      "loss": 2.6962,
      "step": 141700
    },
    {
      "epoch": 1.4476182698001103,
      "grad_norm": 19.49420738220215,
      "learning_rate": 1.0370463795583614e-05,
      "loss": 2.6578,
      "step": 141800
    },
    {
      "epoch": 1.4486391571553994,
      "grad_norm": 19.387184143066406,
      "learning_rate": 1.0363646280022634e-05,
      "loss": 2.5611,
      "step": 141900
    },
    {
      "epoch": 1.4496600445106886,
      "grad_norm": 12.63144302368164,
      "learning_rate": 1.0356828764461655e-05,
      "loss": 2.5483,
      "step": 142000
    },
    {
      "epoch": 1.450680931865978,
      "grad_norm": 13.351675033569336,
      "learning_rate": 1.0350011248900675e-05,
      "loss": 2.6414,
      "step": 142100
    },
    {
      "epoch": 1.4517018192212672,
      "grad_norm": 19.514389038085938,
      "learning_rate": 1.0343193733339698e-05,
      "loss": 2.6562,
      "step": 142200
    },
    {
      "epoch": 1.4527227065765564,
      "grad_norm": 17.724288940429688,
      "learning_rate": 1.033637621777872e-05,
      "loss": 2.5485,
      "step": 142300
    },
    {
      "epoch": 1.4537435939318455,
      "grad_norm": 18.1168155670166,
      "learning_rate": 1.0329558702217739e-05,
      "loss": 2.6095,
      "step": 142400
    },
    {
      "epoch": 1.4547644812871348,
      "grad_norm": 16.717885971069336,
      "learning_rate": 1.032274118665676e-05,
      "loss": 2.6028,
      "step": 142500
    },
    {
      "epoch": 1.455785368642424,
      "grad_norm": 14.133957862854004,
      "learning_rate": 1.031592367109578e-05,
      "loss": 2.5365,
      "step": 142600
    },
    {
      "epoch": 1.4568062559977133,
      "grad_norm": 15.519341468811035,
      "learning_rate": 1.0309106155534801e-05,
      "loss": 2.6276,
      "step": 142700
    },
    {
      "epoch": 1.4578271433530023,
      "grad_norm": 14.687871932983398,
      "learning_rate": 1.030228863997382e-05,
      "loss": 2.5179,
      "step": 142800
    },
    {
      "epoch": 1.4588480307082916,
      "grad_norm": 13.9746732711792,
      "learning_rate": 1.0295471124412842e-05,
      "loss": 2.6355,
      "step": 142900
    },
    {
      "epoch": 1.4598689180635809,
      "grad_norm": 17.704010009765625,
      "learning_rate": 1.0288653608851863e-05,
      "loss": 2.6409,
      "step": 143000
    },
    {
      "epoch": 1.4608898054188701,
      "grad_norm": 12.951737403869629,
      "learning_rate": 1.0281836093290885e-05,
      "loss": 2.5561,
      "step": 143100
    },
    {
      "epoch": 1.4619106927741594,
      "grad_norm": 15.716792106628418,
      "learning_rate": 1.0275018577729904e-05,
      "loss": 2.6831,
      "step": 143200
    },
    {
      "epoch": 1.4629315801294485,
      "grad_norm": 16.89064598083496,
      "learning_rate": 1.0268269237324535e-05,
      "loss": 2.6291,
      "step": 143300
    },
    {
      "epoch": 1.4639524674847377,
      "grad_norm": 17.843421936035156,
      "learning_rate": 1.0261451721763555e-05,
      "loss": 2.5047,
      "step": 143400
    },
    {
      "epoch": 1.464973354840027,
      "grad_norm": 16.638572692871094,
      "learning_rate": 1.0254634206202576e-05,
      "loss": 2.6681,
      "step": 143500
    },
    {
      "epoch": 1.465994242195316,
      "grad_norm": 17.032512664794922,
      "learning_rate": 1.0247816690641596e-05,
      "loss": 2.5524,
      "step": 143600
    },
    {
      "epoch": 1.4670151295506053,
      "grad_norm": 19.86747932434082,
      "learning_rate": 1.0240999175080617e-05,
      "loss": 2.6653,
      "step": 143700
    },
    {
      "epoch": 1.4680360169058946,
      "grad_norm": 17.427181243896484,
      "learning_rate": 1.023418165951964e-05,
      "loss": 2.5635,
      "step": 143800
    },
    {
      "epoch": 1.4690569042611838,
      "grad_norm": 16.603933334350586,
      "learning_rate": 1.022736414395866e-05,
      "loss": 2.6376,
      "step": 143900
    },
    {
      "epoch": 1.4700777916164731,
      "grad_norm": 17.06143569946289,
      "learning_rate": 1.0220546628397681e-05,
      "loss": 2.4991,
      "step": 144000
    },
    {
      "epoch": 1.4710986789717624,
      "grad_norm": 16.06920623779297,
      "learning_rate": 1.02137291128367e-05,
      "loss": 2.7356,
      "step": 144100
    },
    {
      "epoch": 1.4721195663270514,
      "grad_norm": 14.150336265563965,
      "learning_rate": 1.0206911597275722e-05,
      "loss": 2.5695,
      "step": 144200
    },
    {
      "epoch": 1.4731404536823407,
      "grad_norm": 15.816849708557129,
      "learning_rate": 1.0200094081714742e-05,
      "loss": 2.671,
      "step": 144300
    },
    {
      "epoch": 1.47416134103763,
      "grad_norm": 19.911897659301758,
      "learning_rate": 1.0193276566153763e-05,
      "loss": 2.5785,
      "step": 144400
    },
    {
      "epoch": 1.475182228392919,
      "grad_norm": 15.183740615844727,
      "learning_rate": 1.0186459050592783e-05,
      "loss": 2.5213,
      "step": 144500
    },
    {
      "epoch": 1.4762031157482083,
      "grad_norm": 17.528093338012695,
      "learning_rate": 1.0179709710187415e-05,
      "loss": 2.5556,
      "step": 144600
    },
    {
      "epoch": 1.4772240031034976,
      "grad_norm": 13.658198356628418,
      "learning_rate": 1.0172892194626437e-05,
      "loss": 2.603,
      "step": 144700
    },
    {
      "epoch": 1.4782448904587868,
      "grad_norm": 19.341121673583984,
      "learning_rate": 1.0166074679065456e-05,
      "loss": 2.5557,
      "step": 144800
    },
    {
      "epoch": 1.479265777814076,
      "grad_norm": 17.04296112060547,
      "learning_rate": 1.0159257163504478e-05,
      "loss": 2.5605,
      "step": 144900
    },
    {
      "epoch": 1.4802866651693651,
      "grad_norm": 17.720285415649414,
      "learning_rate": 1.0152439647943497e-05,
      "loss": 2.6014,
      "step": 145000
    },
    {
      "epoch": 1.4813075525246544,
      "grad_norm": 15.964351654052734,
      "learning_rate": 1.0145622132382519e-05,
      "loss": 2.6274,
      "step": 145100
    },
    {
      "epoch": 1.4823284398799437,
      "grad_norm": 12.490713119506836,
      "learning_rate": 1.0138804616821538e-05,
      "loss": 2.5834,
      "step": 145200
    },
    {
      "epoch": 1.483349327235233,
      "grad_norm": 14.09849739074707,
      "learning_rate": 1.013198710126056e-05,
      "loss": 2.6806,
      "step": 145300
    },
    {
      "epoch": 1.484370214590522,
      "grad_norm": 17.683687210083008,
      "learning_rate": 1.0125169585699579e-05,
      "loss": 2.5867,
      "step": 145400
    },
    {
      "epoch": 1.4853911019458113,
      "grad_norm": 15.048090934753418,
      "learning_rate": 1.0118352070138602e-05,
      "loss": 2.7063,
      "step": 145500
    },
    {
      "epoch": 1.4864119893011005,
      "grad_norm": 18.408702850341797,
      "learning_rate": 1.0111534554577622e-05,
      "loss": 2.7061,
      "step": 145600
    },
    {
      "epoch": 1.4874328766563898,
      "grad_norm": 13.107159614562988,
      "learning_rate": 1.0104717039016643e-05,
      "loss": 2.6079,
      "step": 145700
    },
    {
      "epoch": 1.488453764011679,
      "grad_norm": 19.200927734375,
      "learning_rate": 1.0097899523455663e-05,
      "loss": 2.5799,
      "step": 145800
    },
    {
      "epoch": 1.489474651366968,
      "grad_norm": 15.905786514282227,
      "learning_rate": 1.0091082007894684e-05,
      "loss": 2.6833,
      "step": 145900
    },
    {
      "epoch": 1.4904955387222574,
      "grad_norm": 15.028644561767578,
      "learning_rate": 1.0084264492333704e-05,
      "loss": 2.6395,
      "step": 146000
    },
    {
      "epoch": 1.4915164260775466,
      "grad_norm": 16.58001708984375,
      "learning_rate": 1.0077446976772725e-05,
      "loss": 2.6365,
      "step": 146100
    },
    {
      "epoch": 1.4925373134328357,
      "grad_norm": 15.776930809020996,
      "learning_rate": 1.0070629461211744e-05,
      "loss": 2.5596,
      "step": 146200
    },
    {
      "epoch": 1.493558200788125,
      "grad_norm": 14.769721984863281,
      "learning_rate": 1.0063811945650767e-05,
      "loss": 2.58,
      "step": 146300
    },
    {
      "epoch": 1.4945790881434142,
      "grad_norm": 13.950451850891113,
      "learning_rate": 1.0056994430089789e-05,
      "loss": 2.5996,
      "step": 146400
    },
    {
      "epoch": 1.4955999754987035,
      "grad_norm": 16.37868881225586,
      "learning_rate": 1.0050176914528808e-05,
      "loss": 2.5738,
      "step": 146500
    },
    {
      "epoch": 1.4966208628539928,
      "grad_norm": 15.98035717010498,
      "learning_rate": 1.004335939896783e-05,
      "loss": 2.6554,
      "step": 146600
    },
    {
      "epoch": 1.497641750209282,
      "grad_norm": 13.098751068115234,
      "learning_rate": 1.003654188340685e-05,
      "loss": 2.5483,
      "step": 146700
    },
    {
      "epoch": 1.498662637564571,
      "grad_norm": 15.875631332397461,
      "learning_rate": 1.002972436784587e-05,
      "loss": 2.5966,
      "step": 146800
    },
    {
      "epoch": 1.4996835249198603,
      "grad_norm": 18.733856201171875,
      "learning_rate": 1.002290685228489e-05,
      "loss": 2.6052,
      "step": 146900
    },
    {
      "epoch": 1.5007044122751496,
      "grad_norm": 16.349254608154297,
      "learning_rate": 1.0016089336723912e-05,
      "loss": 2.6573,
      "step": 147000
    },
    {
      "epoch": 1.5017252996304387,
      "grad_norm": 18.982837677001953,
      "learning_rate": 1.0009271821162933e-05,
      "loss": 2.6091,
      "step": 147100
    },
    {
      "epoch": 1.502746186985728,
      "grad_norm": 15.00268840789795,
      "learning_rate": 1.0002454305601954e-05,
      "loss": 2.6447,
      "step": 147200
    },
    {
      "epoch": 1.5037670743410172,
      "grad_norm": 15.024251937866211,
      "learning_rate": 9.995636790040974e-06,
      "loss": 2.6238,
      "step": 147300
    },
    {
      "epoch": 1.5047879616963065,
      "grad_norm": 16.7282657623291,
      "learning_rate": 9.988819274479995e-06,
      "loss": 2.5924,
      "step": 147400
    },
    {
      "epoch": 1.5058088490515957,
      "grad_norm": 13.331927299499512,
      "learning_rate": 9.982001758919015e-06,
      "loss": 2.5809,
      "step": 147500
    },
    {
      "epoch": 1.506829736406885,
      "grad_norm": 17.295164108276367,
      "learning_rate": 9.975184243358036e-06,
      "loss": 2.5816,
      "step": 147600
    },
    {
      "epoch": 1.507850623762174,
      "grad_norm": 19.477649688720703,
      "learning_rate": 9.968366727797057e-06,
      "loss": 2.6432,
      "step": 147700
    },
    {
      "epoch": 1.5088715111174633,
      "grad_norm": 19.224628448486328,
      "learning_rate": 9.961549212236079e-06,
      "loss": 2.5186,
      "step": 147800
    },
    {
      "epoch": 1.5098923984727524,
      "grad_norm": 22.129047393798828,
      "learning_rate": 9.954731696675098e-06,
      "loss": 2.6414,
      "step": 147900
    },
    {
      "epoch": 1.5109132858280416,
      "grad_norm": 19.835283279418945,
      "learning_rate": 9.94791418111412e-06,
      "loss": 2.635,
      "step": 148000
    },
    {
      "epoch": 1.511934173183331,
      "grad_norm": 16.337512969970703,
      "learning_rate": 9.941096665553141e-06,
      "loss": 2.6913,
      "step": 148100
    },
    {
      "epoch": 1.5129550605386202,
      "grad_norm": 13.712019920349121,
      "learning_rate": 9.93427914999216e-06,
      "loss": 2.5708,
      "step": 148200
    },
    {
      "epoch": 1.5139759478939094,
      "grad_norm": 24.04102325439453,
      "learning_rate": 9.927461634431182e-06,
      "loss": 2.5925,
      "step": 148300
    },
    {
      "epoch": 1.5149968352491987,
      "grad_norm": 17.263412475585938,
      "learning_rate": 9.920644118870202e-06,
      "loss": 2.6787,
      "step": 148400
    },
    {
      "epoch": 1.516017722604488,
      "grad_norm": 17.660898208618164,
      "learning_rate": 9.913826603309223e-06,
      "loss": 2.6296,
      "step": 148500
    },
    {
      "epoch": 1.517038609959777,
      "grad_norm": 15.0759916305542,
      "learning_rate": 9.907009087748244e-06,
      "loss": 2.5489,
      "step": 148600
    },
    {
      "epoch": 1.5180594973150663,
      "grad_norm": 15.125422477722168,
      "learning_rate": 9.900259747342874e-06,
      "loss": 2.5366,
      "step": 148700
    },
    {
      "epoch": 1.5190803846703553,
      "grad_norm": 16.769573211669922,
      "learning_rate": 9.893442231781895e-06,
      "loss": 2.5876,
      "step": 148800
    },
    {
      "epoch": 1.5201012720256446,
      "grad_norm": 18.034923553466797,
      "learning_rate": 9.886692891376526e-06,
      "loss": 2.5291,
      "step": 148900
    },
    {
      "epoch": 1.5211221593809339,
      "grad_norm": 14.902382850646973,
      "learning_rate": 9.879875375815546e-06,
      "loss": 2.589,
      "step": 149000
    },
    {
      "epoch": 1.5221430467362231,
      "grad_norm": 14.241870880126953,
      "learning_rate": 9.873057860254567e-06,
      "loss": 2.584,
      "step": 149100
    },
    {
      "epoch": 1.5231639340915124,
      "grad_norm": 15.793540954589844,
      "learning_rate": 9.866240344693586e-06,
      "loss": 2.6039,
      "step": 149200
    },
    {
      "epoch": 1.5241848214468017,
      "grad_norm": 17.56740951538086,
      "learning_rate": 9.85942282913261e-06,
      "loss": 2.6306,
      "step": 149300
    },
    {
      "epoch": 1.525205708802091,
      "grad_norm": 15.06218147277832,
      "learning_rate": 9.852605313571629e-06,
      "loss": 2.5835,
      "step": 149400
    },
    {
      "epoch": 1.52622659615738,
      "grad_norm": 14.000741004943848,
      "learning_rate": 9.84578779801065e-06,
      "loss": 2.5525,
      "step": 149500
    },
    {
      "epoch": 1.5272474835126693,
      "grad_norm": 16.129972457885742,
      "learning_rate": 9.83897028244967e-06,
      "loss": 2.5324,
      "step": 149600
    },
    {
      "epoch": 1.5282683708679583,
      "grad_norm": 14.201109886169434,
      "learning_rate": 9.832152766888691e-06,
      "loss": 2.6345,
      "step": 149700
    },
    {
      "epoch": 1.5292892582232476,
      "grad_norm": 15.855996131896973,
      "learning_rate": 9.825335251327713e-06,
      "loss": 2.5586,
      "step": 149800
    },
    {
      "epoch": 1.5303101455785368,
      "grad_norm": 14.369845390319824,
      "learning_rate": 9.818517735766732e-06,
      "loss": 2.5706,
      "step": 149900
    },
    {
      "epoch": 1.531331032933826,
      "grad_norm": 18.75905418395996,
      "learning_rate": 9.811700220205754e-06,
      "loss": 2.7146,
      "step": 150000
    },
    {
      "epoch": 1.5323519202891154,
      "grad_norm": 16.706201553344727,
      "learning_rate": 9.804882704644773e-06,
      "loss": 2.5776,
      "step": 150100
    },
    {
      "epoch": 1.5333728076444046,
      "grad_norm": 17.384910583496094,
      "learning_rate": 9.798065189083795e-06,
      "loss": 2.7177,
      "step": 150200
    },
    {
      "epoch": 1.5343936949996937,
      "grad_norm": 13.348413467407227,
      "learning_rate": 9.791247673522816e-06,
      "loss": 2.6046,
      "step": 150300
    },
    {
      "epoch": 1.535414582354983,
      "grad_norm": 18.74423599243164,
      "learning_rate": 9.784430157961835e-06,
      "loss": 2.64,
      "step": 150400
    },
    {
      "epoch": 1.536435469710272,
      "grad_norm": 14.915519714355469,
      "learning_rate": 9.777612642400857e-06,
      "loss": 2.6225,
      "step": 150500
    },
    {
      "epoch": 1.5374563570655613,
      "grad_norm": 23.28787612915039,
      "learning_rate": 9.770795126839878e-06,
      "loss": 2.5685,
      "step": 150600
    },
    {
      "epoch": 1.5384772444208505,
      "grad_norm": 18.60660743713379,
      "learning_rate": 9.7639776112789e-06,
      "loss": 2.5583,
      "step": 150700
    },
    {
      "epoch": 1.5394981317761398,
      "grad_norm": 21.202035903930664,
      "learning_rate": 9.757160095717919e-06,
      "loss": 2.5727,
      "step": 150800
    },
    {
      "epoch": 1.540519019131429,
      "grad_norm": 15.652396202087402,
      "learning_rate": 9.75034258015694e-06,
      "loss": 2.4814,
      "step": 150900
    },
    {
      "epoch": 1.5415399064867183,
      "grad_norm": 14.628649711608887,
      "learning_rate": 9.743525064595962e-06,
      "loss": 2.6178,
      "step": 151000
    },
    {
      "epoch": 1.5425607938420076,
      "grad_norm": 19.39919090270996,
      "learning_rate": 9.736707549034981e-06,
      "loss": 2.5731,
      "step": 151100
    },
    {
      "epoch": 1.5435816811972967,
      "grad_norm": 18.206621170043945,
      "learning_rate": 9.729890033474003e-06,
      "loss": 2.5875,
      "step": 151200
    },
    {
      "epoch": 1.544602568552586,
      "grad_norm": 19.288240432739258,
      "learning_rate": 9.723072517913022e-06,
      "loss": 2.5974,
      "step": 151300
    },
    {
      "epoch": 1.545623455907875,
      "grad_norm": 16.864213943481445,
      "learning_rate": 9.716255002352043e-06,
      "loss": 2.6398,
      "step": 151400
    },
    {
      "epoch": 1.5466443432631642,
      "grad_norm": 17.037202835083008,
      "learning_rate": 9.709505661946675e-06,
      "loss": 2.679,
      "step": 151500
    },
    {
      "epoch": 1.5476652306184535,
      "grad_norm": 17.361356735229492,
      "learning_rate": 9.702688146385694e-06,
      "loss": 2.654,
      "step": 151600
    },
    {
      "epoch": 1.5486861179737428,
      "grad_norm": 12.804218292236328,
      "learning_rate": 9.695870630824716e-06,
      "loss": 2.6409,
      "step": 151700
    },
    {
      "epoch": 1.549707005329032,
      "grad_norm": 16.98335838317871,
      "learning_rate": 9.689053115263737e-06,
      "loss": 2.6157,
      "step": 151800
    },
    {
      "epoch": 1.5507278926843213,
      "grad_norm": 13.922494888305664,
      "learning_rate": 9.682235599702756e-06,
      "loss": 2.6272,
      "step": 151900
    },
    {
      "epoch": 1.5517487800396106,
      "grad_norm": 16.421096801757812,
      "learning_rate": 9.675418084141778e-06,
      "loss": 2.647,
      "step": 152000
    },
    {
      "epoch": 1.5527696673948996,
      "grad_norm": 17.541379928588867,
      "learning_rate": 9.668600568580797e-06,
      "loss": 2.6188,
      "step": 152100
    },
    {
      "epoch": 1.553790554750189,
      "grad_norm": 17.4263858795166,
      "learning_rate": 9.66178305301982e-06,
      "loss": 2.58,
      "step": 152200
    },
    {
      "epoch": 1.554811442105478,
      "grad_norm": 15.47477912902832,
      "learning_rate": 9.65496553745884e-06,
      "loss": 2.6169,
      "step": 152300
    },
    {
      "epoch": 1.5558323294607672,
      "grad_norm": 14.585724830627441,
      "learning_rate": 9.648148021897861e-06,
      "loss": 2.5638,
      "step": 152400
    },
    {
      "epoch": 1.5568532168160565,
      "grad_norm": 13.54595947265625,
      "learning_rate": 9.641330506336881e-06,
      "loss": 2.6209,
      "step": 152500
    },
    {
      "epoch": 1.5578741041713458,
      "grad_norm": 13.090356826782227,
      "learning_rate": 9.634512990775902e-06,
      "loss": 2.5439,
      "step": 152600
    },
    {
      "epoch": 1.558894991526635,
      "grad_norm": 15.373411178588867,
      "learning_rate": 9.627695475214924e-06,
      "loss": 2.6732,
      "step": 152700
    },
    {
      "epoch": 1.5599158788819243,
      "grad_norm": 17.0583438873291,
      "learning_rate": 9.620877959653943e-06,
      "loss": 2.5636,
      "step": 152800
    },
    {
      "epoch": 1.5609367662372133,
      "grad_norm": 16.9965763092041,
      "learning_rate": 9.614060444092964e-06,
      "loss": 2.5707,
      "step": 152900
    },
    {
      "epoch": 1.5619576535925026,
      "grad_norm": 14.596939086914062,
      "learning_rate": 9.607242928531986e-06,
      "loss": 2.6265,
      "step": 153000
    },
    {
      "epoch": 1.5629785409477919,
      "grad_norm": 15.261251449584961,
      "learning_rate": 9.600425412971005e-06,
      "loss": 2.4914,
      "step": 153100
    },
    {
      "epoch": 1.563999428303081,
      "grad_norm": 14.259224891662598,
      "learning_rate": 9.593607897410027e-06,
      "loss": 2.5961,
      "step": 153200
    },
    {
      "epoch": 1.5650203156583702,
      "grad_norm": 13.296297073364258,
      "learning_rate": 9.586790381849046e-06,
      "loss": 2.5452,
      "step": 153300
    },
    {
      "epoch": 1.5660412030136595,
      "grad_norm": 14.938032150268555,
      "learning_rate": 9.579972866288068e-06,
      "loss": 2.6627,
      "step": 153400
    },
    {
      "epoch": 1.5670620903689487,
      "grad_norm": 13.948494911193848,
      "learning_rate": 9.573155350727089e-06,
      "loss": 2.5532,
      "step": 153500
    },
    {
      "epoch": 1.568082977724238,
      "grad_norm": 18.368877410888672,
      "learning_rate": 9.56633783516611e-06,
      "loss": 2.5524,
      "step": 153600
    },
    {
      "epoch": 1.5691038650795273,
      "grad_norm": 17.206640243530273,
      "learning_rate": 9.55952031960513e-06,
      "loss": 2.5647,
      "step": 153700
    },
    {
      "epoch": 1.5701247524348163,
      "grad_norm": 15.759201049804688,
      "learning_rate": 9.552702804044151e-06,
      "loss": 2.5763,
      "step": 153800
    },
    {
      "epoch": 1.5711456397901056,
      "grad_norm": 17.69225311279297,
      "learning_rate": 9.545885288483173e-06,
      "loss": 2.493,
      "step": 153900
    },
    {
      "epoch": 1.5721665271453946,
      "grad_norm": 18.91421890258789,
      "learning_rate": 9.539067772922192e-06,
      "loss": 2.6147,
      "step": 154000
    },
    {
      "epoch": 1.573187414500684,
      "grad_norm": 14.936031341552734,
      "learning_rate": 9.532250257361213e-06,
      "loss": 2.5563,
      "step": 154100
    },
    {
      "epoch": 1.5742083018559732,
      "grad_norm": 16.476232528686523,
      "learning_rate": 9.525432741800233e-06,
      "loss": 2.463,
      "step": 154200
    },
    {
      "epoch": 1.5752291892112624,
      "grad_norm": 17.03766632080078,
      "learning_rate": 9.518615226239254e-06,
      "loss": 2.5869,
      "step": 154300
    },
    {
      "epoch": 1.5762500765665517,
      "grad_norm": 17.672222137451172,
      "learning_rate": 9.511797710678276e-06,
      "loss": 2.6687,
      "step": 154400
    },
    {
      "epoch": 1.577270963921841,
      "grad_norm": 15.148205757141113,
      "learning_rate": 9.504980195117295e-06,
      "loss": 2.6125,
      "step": 154500
    },
    {
      "epoch": 1.5782918512771302,
      "grad_norm": 14.380772590637207,
      "learning_rate": 9.498230854711926e-06,
      "loss": 2.627,
      "step": 154600
    },
    {
      "epoch": 1.5793127386324193,
      "grad_norm": 12.174538612365723,
      "learning_rate": 9.491413339150948e-06,
      "loss": 2.5738,
      "step": 154700
    },
    {
      "epoch": 1.5803336259877085,
      "grad_norm": 23.43400764465332,
      "learning_rate": 9.484595823589969e-06,
      "loss": 2.5559,
      "step": 154800
    },
    {
      "epoch": 1.5813545133429976,
      "grad_norm": 18.040891647338867,
      "learning_rate": 9.477778308028989e-06,
      "loss": 2.6006,
      "step": 154900
    },
    {
      "epoch": 1.5823754006982869,
      "grad_norm": 17.026880264282227,
      "learning_rate": 9.47096079246801e-06,
      "loss": 2.5723,
      "step": 155000
    },
    {
      "epoch": 1.5833962880535761,
      "grad_norm": 15.739038467407227,
      "learning_rate": 9.464143276907031e-06,
      "loss": 2.6117,
      "step": 155100
    },
    {
      "epoch": 1.5844171754088654,
      "grad_norm": 14.82617473602295,
      "learning_rate": 9.457325761346051e-06,
      "loss": 2.5186,
      "step": 155200
    },
    {
      "epoch": 1.5854380627641547,
      "grad_norm": 24.352922439575195,
      "learning_rate": 9.450508245785072e-06,
      "loss": 2.6374,
      "step": 155300
    },
    {
      "epoch": 1.586458950119444,
      "grad_norm": 21.286256790161133,
      "learning_rate": 9.443690730224092e-06,
      "loss": 2.6054,
      "step": 155400
    },
    {
      "epoch": 1.5874798374747332,
      "grad_norm": 19.19843101501465,
      "learning_rate": 9.436873214663113e-06,
      "loss": 2.5536,
      "step": 155500
    },
    {
      "epoch": 1.5885007248300222,
      "grad_norm": 16.680213928222656,
      "learning_rate": 9.430055699102134e-06,
      "loss": 2.6286,
      "step": 155600
    },
    {
      "epoch": 1.5895216121853115,
      "grad_norm": 17.016681671142578,
      "learning_rate": 9.423238183541154e-06,
      "loss": 2.6171,
      "step": 155700
    },
    {
      "epoch": 1.5905424995406006,
      "grad_norm": 16.688852310180664,
      "learning_rate": 9.416420667980175e-06,
      "loss": 2.6952,
      "step": 155800
    },
    {
      "epoch": 1.5915633868958898,
      "grad_norm": 15.375301361083984,
      "learning_rate": 9.409603152419197e-06,
      "loss": 2.6,
      "step": 155900
    },
    {
      "epoch": 1.592584274251179,
      "grad_norm": 16.636693954467773,
      "learning_rate": 9.402785636858216e-06,
      "loss": 2.558,
      "step": 156000
    },
    {
      "epoch": 1.5936051616064684,
      "grad_norm": 15.953221321105957,
      "learning_rate": 9.395968121297238e-06,
      "loss": 2.61,
      "step": 156100
    },
    {
      "epoch": 1.5946260489617576,
      "grad_norm": 14.58163070678711,
      "learning_rate": 9.389150605736257e-06,
      "loss": 2.5196,
      "step": 156200
    },
    {
      "epoch": 1.595646936317047,
      "grad_norm": 13.39345645904541,
      "learning_rate": 9.38233309017528e-06,
      "loss": 2.595,
      "step": 156300
    },
    {
      "epoch": 1.596667823672336,
      "grad_norm": 16.062114715576172,
      "learning_rate": 9.3755155746143e-06,
      "loss": 2.5998,
      "step": 156400
    },
    {
      "epoch": 1.5976887110276252,
      "grad_norm": 17.48065948486328,
      "learning_rate": 9.368698059053321e-06,
      "loss": 2.6361,
      "step": 156500
    },
    {
      "epoch": 1.5987095983829143,
      "grad_norm": 14.790212631225586,
      "learning_rate": 9.36188054349234e-06,
      "loss": 2.5996,
      "step": 156600
    },
    {
      "epoch": 1.5997304857382035,
      "grad_norm": 18.197269439697266,
      "learning_rate": 9.355063027931362e-06,
      "loss": 2.6074,
      "step": 156700
    },
    {
      "epoch": 1.6007513730934928,
      "grad_norm": 15.675747871398926,
      "learning_rate": 9.348245512370383e-06,
      "loss": 2.6551,
      "step": 156800
    },
    {
      "epoch": 1.601772260448782,
      "grad_norm": 20.194360733032227,
      "learning_rate": 9.341427996809403e-06,
      "loss": 2.5323,
      "step": 156900
    },
    {
      "epoch": 1.6027931478040713,
      "grad_norm": 15.138886451721191,
      "learning_rate": 9.334610481248424e-06,
      "loss": 2.6043,
      "step": 157000
    },
    {
      "epoch": 1.6038140351593606,
      "grad_norm": 17.125146865844727,
      "learning_rate": 9.327792965687444e-06,
      "loss": 2.6589,
      "step": 157100
    },
    {
      "epoch": 1.6048349225146499,
      "grad_norm": 15.706138610839844,
      "learning_rate": 9.320975450126465e-06,
      "loss": 2.5408,
      "step": 157200
    },
    {
      "epoch": 1.605855809869939,
      "grad_norm": 18.270906448364258,
      "learning_rate": 9.314157934565487e-06,
      "loss": 2.5987,
      "step": 157300
    },
    {
      "epoch": 1.6068766972252282,
      "grad_norm": 16.188997268676758,
      "learning_rate": 9.307340419004506e-06,
      "loss": 2.6016,
      "step": 157400
    },
    {
      "epoch": 1.6078975845805172,
      "grad_norm": 15.520553588867188,
      "learning_rate": 9.300522903443528e-06,
      "loss": 2.6094,
      "step": 157500
    },
    {
      "epoch": 1.6089184719358065,
      "grad_norm": 18.155994415283203,
      "learning_rate": 9.293705387882549e-06,
      "loss": 2.5917,
      "step": 157600
    },
    {
      "epoch": 1.6099393592910958,
      "grad_norm": 13.571857452392578,
      "learning_rate": 9.28688787232157e-06,
      "loss": 2.6041,
      "step": 157700
    },
    {
      "epoch": 1.610960246646385,
      "grad_norm": 18.55234146118164,
      "learning_rate": 9.28007035676059e-06,
      "loss": 2.627,
      "step": 157800
    },
    {
      "epoch": 1.6119811340016743,
      "grad_norm": 16.845970153808594,
      "learning_rate": 9.273252841199611e-06,
      "loss": 2.5415,
      "step": 157900
    },
    {
      "epoch": 1.6130020213569636,
      "grad_norm": 14.20939826965332,
      "learning_rate": 9.266503500794242e-06,
      "loss": 2.6495,
      "step": 158000
    },
    {
      "epoch": 1.6140229087122528,
      "grad_norm": 14.912593841552734,
      "learning_rate": 9.259685985233262e-06,
      "loss": 2.6303,
      "step": 158100
    },
    {
      "epoch": 1.615043796067542,
      "grad_norm": 15.350847244262695,
      "learning_rate": 9.252868469672283e-06,
      "loss": 2.5371,
      "step": 158200
    },
    {
      "epoch": 1.6160646834228312,
      "grad_norm": 15.019474029541016,
      "learning_rate": 9.246050954111303e-06,
      "loss": 2.5639,
      "step": 158300
    },
    {
      "epoch": 1.6170855707781202,
      "grad_norm": 13.60815143585205,
      "learning_rate": 9.239233438550324e-06,
      "loss": 2.5876,
      "step": 158400
    },
    {
      "epoch": 1.6181064581334095,
      "grad_norm": 13.822433471679688,
      "learning_rate": 9.232415922989345e-06,
      "loss": 2.5728,
      "step": 158500
    },
    {
      "epoch": 1.6191273454886987,
      "grad_norm": 15.644372940063477,
      "learning_rate": 9.225598407428365e-06,
      "loss": 2.5992,
      "step": 158600
    },
    {
      "epoch": 1.620148232843988,
      "grad_norm": 18.167728424072266,
      "learning_rate": 9.218780891867386e-06,
      "loss": 2.6443,
      "step": 158700
    },
    {
      "epoch": 1.6211691201992773,
      "grad_norm": 13.605279922485352,
      "learning_rate": 9.211963376306408e-06,
      "loss": 2.6505,
      "step": 158800
    },
    {
      "epoch": 1.6221900075545665,
      "grad_norm": 11.140557289123535,
      "learning_rate": 9.205145860745427e-06,
      "loss": 2.4693,
      "step": 158900
    },
    {
      "epoch": 1.6232108949098556,
      "grad_norm": 16.900423049926758,
      "learning_rate": 9.198328345184449e-06,
      "loss": 2.5727,
      "step": 159000
    },
    {
      "epoch": 1.6242317822651449,
      "grad_norm": 19.64659881591797,
      "learning_rate": 9.191510829623468e-06,
      "loss": 2.6066,
      "step": 159100
    },
    {
      "epoch": 1.6252526696204341,
      "grad_norm": 10.641379356384277,
      "learning_rate": 9.184693314062491e-06,
      "loss": 2.5204,
      "step": 159200
    },
    {
      "epoch": 1.6262735569757232,
      "grad_norm": 15.60853385925293,
      "learning_rate": 9.17787579850151e-06,
      "loss": 2.6415,
      "step": 159300
    },
    {
      "epoch": 1.6272944443310124,
      "grad_norm": 15.430231094360352,
      "learning_rate": 9.171058282940532e-06,
      "loss": 2.6548,
      "step": 159400
    },
    {
      "epoch": 1.6283153316863017,
      "grad_norm": 23.034696578979492,
      "learning_rate": 9.164240767379552e-06,
      "loss": 2.536,
      "step": 159500
    },
    {
      "epoch": 1.629336219041591,
      "grad_norm": 13.953497886657715,
      "learning_rate": 9.157423251818573e-06,
      "loss": 2.5786,
      "step": 159600
    },
    {
      "epoch": 1.6303571063968803,
      "grad_norm": 14.51716136932373,
      "learning_rate": 9.150605736257594e-06,
      "loss": 2.5371,
      "step": 159700
    },
    {
      "epoch": 1.6313779937521695,
      "grad_norm": 15.361984252929688,
      "learning_rate": 9.143788220696614e-06,
      "loss": 2.5081,
      "step": 159800
    },
    {
      "epoch": 1.6323988811074586,
      "grad_norm": 17.181598663330078,
      "learning_rate": 9.136970705135635e-06,
      "loss": 2.5973,
      "step": 159900
    },
    {
      "epoch": 1.6334197684627478,
      "grad_norm": 13.248844146728516,
      "learning_rate": 9.130153189574657e-06,
      "loss": 2.6921,
      "step": 160000
    },
    {
      "epoch": 1.6344406558180369,
      "grad_norm": 22.129377365112305,
      "learning_rate": 9.123335674013676e-06,
      "loss": 2.597,
      "step": 160100
    },
    {
      "epoch": 1.6354615431733261,
      "grad_norm": 17.679542541503906,
      "learning_rate": 9.116518158452697e-06,
      "loss": 2.6049,
      "step": 160200
    },
    {
      "epoch": 1.6364824305286154,
      "grad_norm": 14.755399703979492,
      "learning_rate": 9.109700642891717e-06,
      "loss": 2.5158,
      "step": 160300
    },
    {
      "epoch": 1.6375033178839047,
      "grad_norm": 15.844294548034668,
      "learning_rate": 9.10288312733074e-06,
      "loss": 2.5854,
      "step": 160400
    },
    {
      "epoch": 1.638524205239194,
      "grad_norm": 13.492409706115723,
      "learning_rate": 9.09606561176976e-06,
      "loss": 2.6012,
      "step": 160500
    },
    {
      "epoch": 1.6395450925944832,
      "grad_norm": 15.428160667419434,
      "learning_rate": 9.089248096208781e-06,
      "loss": 2.5503,
      "step": 160600
    },
    {
      "epoch": 1.6405659799497725,
      "grad_norm": 14.047137260437012,
      "learning_rate": 9.0824305806478e-06,
      "loss": 2.6569,
      "step": 160700
    },
    {
      "epoch": 1.6415868673050615,
      "grad_norm": 17.634960174560547,
      "learning_rate": 9.075613065086822e-06,
      "loss": 2.6481,
      "step": 160800
    },
    {
      "epoch": 1.6426077546603508,
      "grad_norm": 16.05670166015625,
      "learning_rate": 9.068795549525843e-06,
      "loss": 2.6037,
      "step": 160900
    },
    {
      "epoch": 1.6436286420156399,
      "grad_norm": 19.77638053894043,
      "learning_rate": 9.061978033964863e-06,
      "loss": 2.6401,
      "step": 161000
    },
    {
      "epoch": 1.6446495293709291,
      "grad_norm": 13.56062126159668,
      "learning_rate": 9.055160518403884e-06,
      "loss": 2.5456,
      "step": 161100
    },
    {
      "epoch": 1.6456704167262184,
      "grad_norm": 12.063549995422363,
      "learning_rate": 9.048343002842904e-06,
      "loss": 2.5422,
      "step": 161200
    },
    {
      "epoch": 1.6466913040815077,
      "grad_norm": 16.4349422454834,
      "learning_rate": 9.041593662437535e-06,
      "loss": 2.5187,
      "step": 161300
    },
    {
      "epoch": 1.647712191436797,
      "grad_norm": 15.167343139648438,
      "learning_rate": 9.034776146876556e-06,
      "loss": 2.5647,
      "step": 161400
    },
    {
      "epoch": 1.6487330787920862,
      "grad_norm": 16.013080596923828,
      "learning_rate": 9.027958631315576e-06,
      "loss": 2.5029,
      "step": 161500
    },
    {
      "epoch": 1.6497539661473755,
      "grad_norm": 16.375898361206055,
      "learning_rate": 9.021141115754597e-06,
      "loss": 2.5499,
      "step": 161600
    },
    {
      "epoch": 1.6507748535026645,
      "grad_norm": 16.600711822509766,
      "learning_rate": 9.014323600193618e-06,
      "loss": 2.5918,
      "step": 161700
    },
    {
      "epoch": 1.6517957408579538,
      "grad_norm": 14.377285957336426,
      "learning_rate": 9.00750608463264e-06,
      "loss": 2.5284,
      "step": 161800
    },
    {
      "epoch": 1.6528166282132428,
      "grad_norm": 14.42116928100586,
      "learning_rate": 9.00068856907166e-06,
      "loss": 2.5785,
      "step": 161900
    },
    {
      "epoch": 1.653837515568532,
      "grad_norm": 20.864402770996094,
      "learning_rate": 8.99387105351068e-06,
      "loss": 2.6039,
      "step": 162000
    },
    {
      "epoch": 1.6548584029238214,
      "grad_norm": 18.038654327392578,
      "learning_rate": 8.987053537949702e-06,
      "loss": 2.5887,
      "step": 162100
    },
    {
      "epoch": 1.6558792902791106,
      "grad_norm": 17.990367889404297,
      "learning_rate": 8.980236022388722e-06,
      "loss": 2.622,
      "step": 162200
    },
    {
      "epoch": 1.6569001776344,
      "grad_norm": 8.768378257751465,
      "learning_rate": 8.973418506827743e-06,
      "loss": 2.5617,
      "step": 162300
    },
    {
      "epoch": 1.6579210649896892,
      "grad_norm": 15.767806053161621,
      "learning_rate": 8.966600991266763e-06,
      "loss": 2.567,
      "step": 162400
    },
    {
      "epoch": 1.6589419523449782,
      "grad_norm": 17.111173629760742,
      "learning_rate": 8.959783475705784e-06,
      "loss": 2.5129,
      "step": 162500
    },
    {
      "epoch": 1.6599628397002675,
      "grad_norm": 12.185663223266602,
      "learning_rate": 8.952965960144805e-06,
      "loss": 2.6222,
      "step": 162600
    },
    {
      "epoch": 1.6609837270555565,
      "grad_norm": 19.92982292175293,
      "learning_rate": 8.946148444583825e-06,
      "loss": 2.4691,
      "step": 162700
    },
    {
      "epoch": 1.6620046144108458,
      "grad_norm": 19.330812454223633,
      "learning_rate": 8.939330929022846e-06,
      "loss": 2.5804,
      "step": 162800
    },
    {
      "epoch": 1.663025501766135,
      "grad_norm": 15.773531913757324,
      "learning_rate": 8.932513413461867e-06,
      "loss": 2.5578,
      "step": 162900
    },
    {
      "epoch": 1.6640463891214243,
      "grad_norm": 15.739643096923828,
      "learning_rate": 8.925695897900887e-06,
      "loss": 2.6001,
      "step": 163000
    },
    {
      "epoch": 1.6650672764767136,
      "grad_norm": 20.950586318969727,
      "learning_rate": 8.918878382339908e-06,
      "loss": 2.5506,
      "step": 163100
    },
    {
      "epoch": 1.6660881638320029,
      "grad_norm": 19.59847640991211,
      "learning_rate": 8.912060866778928e-06,
      "loss": 2.4455,
      "step": 163200
    },
    {
      "epoch": 1.6671090511872921,
      "grad_norm": 14.877300262451172,
      "learning_rate": 8.905243351217951e-06,
      "loss": 2.4803,
      "step": 163300
    },
    {
      "epoch": 1.6681299385425812,
      "grad_norm": 13.063863754272461,
      "learning_rate": 8.89842583565697e-06,
      "loss": 2.6556,
      "step": 163400
    },
    {
      "epoch": 1.6691508258978704,
      "grad_norm": 23.495052337646484,
      "learning_rate": 8.891608320095992e-06,
      "loss": 2.567,
      "step": 163500
    },
    {
      "epoch": 1.6701717132531595,
      "grad_norm": 13.442934036254883,
      "learning_rate": 8.884790804535012e-06,
      "loss": 2.4758,
      "step": 163600
    },
    {
      "epoch": 1.6711926006084488,
      "grad_norm": 15.452296257019043,
      "learning_rate": 8.877973288974033e-06,
      "loss": 2.5217,
      "step": 163700
    },
    {
      "epoch": 1.672213487963738,
      "grad_norm": 21.665559768676758,
      "learning_rate": 8.871155773413054e-06,
      "loss": 2.596,
      "step": 163800
    },
    {
      "epoch": 1.6732343753190273,
      "grad_norm": 14.648107528686523,
      "learning_rate": 8.864338257852074e-06,
      "loss": 2.5744,
      "step": 163900
    },
    {
      "epoch": 1.6742552626743166,
      "grad_norm": 15.826655387878418,
      "learning_rate": 8.857588917446705e-06,
      "loss": 2.5691,
      "step": 164000
    },
    {
      "epoch": 1.6752761500296058,
      "grad_norm": 15.124058723449707,
      "learning_rate": 8.850771401885726e-06,
      "loss": 2.6246,
      "step": 164100
    },
    {
      "epoch": 1.676297037384895,
      "grad_norm": 16.14518928527832,
      "learning_rate": 8.843953886324746e-06,
      "loss": 2.5485,
      "step": 164200
    },
    {
      "epoch": 1.6773179247401842,
      "grad_norm": 19.2921142578125,
      "learning_rate": 8.837136370763767e-06,
      "loss": 2.6756,
      "step": 164300
    },
    {
      "epoch": 1.6783388120954734,
      "grad_norm": 15.793198585510254,
      "learning_rate": 8.830318855202787e-06,
      "loss": 2.5624,
      "step": 164400
    },
    {
      "epoch": 1.6793596994507625,
      "grad_norm": 16.866676330566406,
      "learning_rate": 8.82350133964181e-06,
      "loss": 2.5038,
      "step": 164500
    },
    {
      "epoch": 1.6803805868060517,
      "grad_norm": 15.128950119018555,
      "learning_rate": 8.81668382408083e-06,
      "loss": 2.5243,
      "step": 164600
    },
    {
      "epoch": 1.681401474161341,
      "grad_norm": 15.326872825622559,
      "learning_rate": 8.80986630851985e-06,
      "loss": 2.6593,
      "step": 164700
    },
    {
      "epoch": 1.6824223615166303,
      "grad_norm": 17.111421585083008,
      "learning_rate": 8.80304879295887e-06,
      "loss": 2.593,
      "step": 164800
    },
    {
      "epoch": 1.6834432488719195,
      "grad_norm": 16.20134735107422,
      "learning_rate": 8.796231277397892e-06,
      "loss": 2.6425,
      "step": 164900
    },
    {
      "epoch": 1.6844641362272088,
      "grad_norm": 15.341605186462402,
      "learning_rate": 8.789413761836913e-06,
      "loss": 2.5606,
      "step": 165000
    },
    {
      "epoch": 1.6854850235824979,
      "grad_norm": 12.219618797302246,
      "learning_rate": 8.782596246275933e-06,
      "loss": 2.6176,
      "step": 165100
    },
    {
      "epoch": 1.6865059109377871,
      "grad_norm": 17.785913467407227,
      "learning_rate": 8.775778730714954e-06,
      "loss": 2.597,
      "step": 165200
    },
    {
      "epoch": 1.6875267982930764,
      "grad_norm": 16.405803680419922,
      "learning_rate": 8.768961215153973e-06,
      "loss": 2.5422,
      "step": 165300
    },
    {
      "epoch": 1.6885476856483654,
      "grad_norm": 11.832305908203125,
      "learning_rate": 8.762143699592995e-06,
      "loss": 2.5713,
      "step": 165400
    },
    {
      "epoch": 1.6895685730036547,
      "grad_norm": 12.824222564697266,
      "learning_rate": 8.755326184032016e-06,
      "loss": 2.4683,
      "step": 165500
    },
    {
      "epoch": 1.690589460358944,
      "grad_norm": 16.98276138305664,
      "learning_rate": 8.748508668471036e-06,
      "loss": 2.5397,
      "step": 165600
    },
    {
      "epoch": 1.6916103477142332,
      "grad_norm": 18.39681053161621,
      "learning_rate": 8.741691152910057e-06,
      "loss": 2.4994,
      "step": 165700
    },
    {
      "epoch": 1.6926312350695225,
      "grad_norm": 16.635271072387695,
      "learning_rate": 8.734873637349078e-06,
      "loss": 2.6312,
      "step": 165800
    },
    {
      "epoch": 1.6936521224248118,
      "grad_norm": 17.090810775756836,
      "learning_rate": 8.728056121788098e-06,
      "loss": 2.5401,
      "step": 165900
    },
    {
      "epoch": 1.6946730097801008,
      "grad_norm": 16.000741958618164,
      "learning_rate": 8.72123860622712e-06,
      "loss": 2.5662,
      "step": 166000
    },
    {
      "epoch": 1.69569389713539,
      "grad_norm": 24.9195556640625,
      "learning_rate": 8.714421090666139e-06,
      "loss": 2.4397,
      "step": 166100
    },
    {
      "epoch": 1.6967147844906791,
      "grad_norm": 14.474536895751953,
      "learning_rate": 8.707603575105162e-06,
      "loss": 2.4806,
      "step": 166200
    },
    {
      "epoch": 1.6977356718459684,
      "grad_norm": 16.391530990600586,
      "learning_rate": 8.700786059544182e-06,
      "loss": 2.6151,
      "step": 166300
    },
    {
      "epoch": 1.6987565592012577,
      "grad_norm": 17.411643981933594,
      "learning_rate": 8.693968543983203e-06,
      "loss": 2.5891,
      "step": 166400
    },
    {
      "epoch": 1.699777446556547,
      "grad_norm": 14.347616195678711,
      "learning_rate": 8.687151028422222e-06,
      "loss": 2.6207,
      "step": 166500
    },
    {
      "epoch": 1.7007983339118362,
      "grad_norm": 15.356328964233398,
      "learning_rate": 8.680333512861244e-06,
      "loss": 2.6902,
      "step": 166600
    },
    {
      "epoch": 1.7018192212671255,
      "grad_norm": 17.987581253051758,
      "learning_rate": 8.673515997300265e-06,
      "loss": 2.5345,
      "step": 166700
    },
    {
      "epoch": 1.7028401086224147,
      "grad_norm": 14.19888687133789,
      "learning_rate": 8.666698481739285e-06,
      "loss": 2.5429,
      "step": 166800
    },
    {
      "epoch": 1.7038609959777038,
      "grad_norm": 15.236259460449219,
      "learning_rate": 8.659880966178306e-06,
      "loss": 2.5755,
      "step": 166900
    },
    {
      "epoch": 1.704881883332993,
      "grad_norm": 16.86742401123047,
      "learning_rate": 8.653063450617327e-06,
      "loss": 2.5671,
      "step": 167000
    },
    {
      "epoch": 1.705902770688282,
      "grad_norm": 16.881668090820312,
      "learning_rate": 8.646245935056347e-06,
      "loss": 2.5627,
      "step": 167100
    },
    {
      "epoch": 1.7069236580435714,
      "grad_norm": 17.646892547607422,
      "learning_rate": 8.639428419495368e-06,
      "loss": 2.6073,
      "step": 167200
    },
    {
      "epoch": 1.7079445453988606,
      "grad_norm": 19.5123348236084,
      "learning_rate": 8.632610903934388e-06,
      "loss": 2.6462,
      "step": 167300
    },
    {
      "epoch": 1.70896543275415,
      "grad_norm": 15.267008781433105,
      "learning_rate": 8.625793388373411e-06,
      "loss": 2.5897,
      "step": 167400
    },
    {
      "epoch": 1.7099863201094392,
      "grad_norm": 15.814976692199707,
      "learning_rate": 8.61897587281243e-06,
      "loss": 2.5638,
      "step": 167500
    },
    {
      "epoch": 1.7110072074647285,
      "grad_norm": 19.426546096801758,
      "learning_rate": 8.612158357251452e-06,
      "loss": 2.6105,
      "step": 167600
    },
    {
      "epoch": 1.7120280948200177,
      "grad_norm": 16.3455867767334,
      "learning_rate": 8.605340841690471e-06,
      "loss": 2.543,
      "step": 167700
    },
    {
      "epoch": 1.7130489821753068,
      "grad_norm": 13.424938201904297,
      "learning_rate": 8.598523326129493e-06,
      "loss": 2.6317,
      "step": 167800
    },
    {
      "epoch": 1.714069869530596,
      "grad_norm": 21.251943588256836,
      "learning_rate": 8.591705810568514e-06,
      "loss": 2.5543,
      "step": 167900
    },
    {
      "epoch": 1.715090756885885,
      "grad_norm": 18.364986419677734,
      "learning_rate": 8.584888295007534e-06,
      "loss": 2.6201,
      "step": 168000
    },
    {
      "epoch": 1.7161116442411743,
      "grad_norm": 17.79834747314453,
      "learning_rate": 8.578138954602165e-06,
      "loss": 2.5869,
      "step": 168100
    },
    {
      "epoch": 1.7171325315964636,
      "grad_norm": 12.565311431884766,
      "learning_rate": 8.571321439041186e-06,
      "loss": 2.507,
      "step": 168200
    },
    {
      "epoch": 1.7181534189517529,
      "grad_norm": 16.4674129486084,
      "learning_rate": 8.564503923480206e-06,
      "loss": 2.5986,
      "step": 168300
    },
    {
      "epoch": 1.7191743063070422,
      "grad_norm": 16.795080184936523,
      "learning_rate": 8.557686407919227e-06,
      "loss": 2.5042,
      "step": 168400
    },
    {
      "epoch": 1.7201951936623314,
      "grad_norm": 18.792129516601562,
      "learning_rate": 8.550868892358247e-06,
      "loss": 2.6453,
      "step": 168500
    },
    {
      "epoch": 1.7212160810176205,
      "grad_norm": 12.912993431091309,
      "learning_rate": 8.544051376797268e-06,
      "loss": 2.5453,
      "step": 168600
    },
    {
      "epoch": 1.7222369683729097,
      "grad_norm": 17.95438003540039,
      "learning_rate": 8.53723386123629e-06,
      "loss": 2.5811,
      "step": 168700
    },
    {
      "epoch": 1.7232578557281988,
      "grad_norm": 15.42237663269043,
      "learning_rate": 8.53041634567531e-06,
      "loss": 2.5507,
      "step": 168800
    },
    {
      "epoch": 1.724278743083488,
      "grad_norm": 18.309551239013672,
      "learning_rate": 8.52359883011433e-06,
      "loss": 2.5638,
      "step": 168900
    },
    {
      "epoch": 1.7252996304387773,
      "grad_norm": 15.554271697998047,
      "learning_rate": 8.516781314553351e-06,
      "loss": 2.6733,
      "step": 169000
    },
    {
      "epoch": 1.7263205177940666,
      "grad_norm": 14.133153915405273,
      "learning_rate": 8.509963798992373e-06,
      "loss": 2.5951,
      "step": 169100
    },
    {
      "epoch": 1.7273414051493559,
      "grad_norm": 15.387654304504395,
      "learning_rate": 8.503146283431392e-06,
      "loss": 2.5198,
      "step": 169200
    },
    {
      "epoch": 1.7283622925046451,
      "grad_norm": 14.882752418518066,
      "learning_rate": 8.496328767870414e-06,
      "loss": 2.608,
      "step": 169300
    },
    {
      "epoch": 1.7293831798599344,
      "grad_norm": 12.342849731445312,
      "learning_rate": 8.489511252309433e-06,
      "loss": 2.5546,
      "step": 169400
    },
    {
      "epoch": 1.7304040672152234,
      "grad_norm": 17.086950302124023,
      "learning_rate": 8.482693736748455e-06,
      "loss": 2.4992,
      "step": 169500
    },
    {
      "epoch": 1.7314249545705127,
      "grad_norm": 18.10630989074707,
      "learning_rate": 8.475876221187476e-06,
      "loss": 2.5783,
      "step": 169600
    },
    {
      "epoch": 1.7324458419258018,
      "grad_norm": 14.78148365020752,
      "learning_rate": 8.469126880782105e-06,
      "loss": 2.6033,
      "step": 169700
    },
    {
      "epoch": 1.733466729281091,
      "grad_norm": 14.58439826965332,
      "learning_rate": 8.462309365221127e-06,
      "loss": 2.6165,
      "step": 169800
    },
    {
      "epoch": 1.7344876166363803,
      "grad_norm": 14.703314781188965,
      "learning_rate": 8.455491849660148e-06,
      "loss": 2.5496,
      "step": 169900
    },
    {
      "epoch": 1.7355085039916696,
      "grad_norm": 16.71588706970215,
      "learning_rate": 8.448674334099168e-06,
      "loss": 2.4741,
      "step": 170000
    },
    {
      "epoch": 1.7365293913469588,
      "grad_norm": 21.44644546508789,
      "learning_rate": 8.441856818538189e-06,
      "loss": 2.6304,
      "step": 170100
    },
    {
      "epoch": 1.737550278702248,
      "grad_norm": 13.769871711730957,
      "learning_rate": 8.435039302977209e-06,
      "loss": 2.5375,
      "step": 170200
    },
    {
      "epoch": 1.7385711660575374,
      "grad_norm": 17.351213455200195,
      "learning_rate": 8.428221787416232e-06,
      "loss": 2.5403,
      "step": 170300
    },
    {
      "epoch": 1.7395920534128264,
      "grad_norm": 18.094478607177734,
      "learning_rate": 8.421404271855251e-06,
      "loss": 2.5646,
      "step": 170400
    },
    {
      "epoch": 1.7406129407681157,
      "grad_norm": 14.969414710998535,
      "learning_rate": 8.414586756294272e-06,
      "loss": 2.5382,
      "step": 170500
    },
    {
      "epoch": 1.7416338281234047,
      "grad_norm": 24.618925094604492,
      "learning_rate": 8.407769240733292e-06,
      "loss": 2.5644,
      "step": 170600
    },
    {
      "epoch": 1.742654715478694,
      "grad_norm": 20.279579162597656,
      "learning_rate": 8.400951725172313e-06,
      "loss": 2.5597,
      "step": 170700
    },
    {
      "epoch": 1.7436756028339833,
      "grad_norm": 15.58038330078125,
      "learning_rate": 8.394134209611335e-06,
      "loss": 2.5878,
      "step": 170800
    },
    {
      "epoch": 1.7446964901892725,
      "grad_norm": 16.308351516723633,
      "learning_rate": 8.387316694050354e-06,
      "loss": 2.4652,
      "step": 170900
    },
    {
      "epoch": 1.7457173775445618,
      "grad_norm": 12.000746726989746,
      "learning_rate": 8.380499178489376e-06,
      "loss": 2.6122,
      "step": 171000
    },
    {
      "epoch": 1.746738264899851,
      "grad_norm": 13.232518196105957,
      "learning_rate": 8.373681662928397e-06,
      "loss": 2.6554,
      "step": 171100
    },
    {
      "epoch": 1.7477591522551401,
      "grad_norm": 13.031465530395508,
      "learning_rate": 8.366864147367417e-06,
      "loss": 2.4868,
      "step": 171200
    },
    {
      "epoch": 1.7487800396104294,
      "grad_norm": 16.084308624267578,
      "learning_rate": 8.360046631806438e-06,
      "loss": 2.5207,
      "step": 171300
    },
    {
      "epoch": 1.7498009269657187,
      "grad_norm": 14.865532875061035,
      "learning_rate": 8.353229116245458e-06,
      "loss": 2.5441,
      "step": 171400
    },
    {
      "epoch": 1.7508218143210077,
      "grad_norm": 13.75845718383789,
      "learning_rate": 8.34641160068448e-06,
      "loss": 2.6369,
      "step": 171500
    },
    {
      "epoch": 1.751842701676297,
      "grad_norm": 19.223182678222656,
      "learning_rate": 8.3395940851235e-06,
      "loss": 2.5937,
      "step": 171600
    },
    {
      "epoch": 1.7528635890315862,
      "grad_norm": 13.08997917175293,
      "learning_rate": 8.332776569562521e-06,
      "loss": 2.6031,
      "step": 171700
    },
    {
      "epoch": 1.7538844763868755,
      "grad_norm": 16.821815490722656,
      "learning_rate": 8.325959054001541e-06,
      "loss": 2.5889,
      "step": 171800
    },
    {
      "epoch": 1.7549053637421648,
      "grad_norm": 16.91221809387207,
      "learning_rate": 8.319141538440562e-06,
      "loss": 2.5008,
      "step": 171900
    },
    {
      "epoch": 1.755926251097454,
      "grad_norm": 13.007542610168457,
      "learning_rate": 8.312324022879584e-06,
      "loss": 2.6068,
      "step": 172000
    },
    {
      "epoch": 1.756947138452743,
      "grad_norm": 18.253297805786133,
      "learning_rate": 8.305642857629823e-06,
      "loss": 2.6345,
      "step": 172100
    },
    {
      "epoch": 1.7579680258080324,
      "grad_norm": 11.235268592834473,
      "learning_rate": 8.298825342068844e-06,
      "loss": 2.4806,
      "step": 172200
    },
    {
      "epoch": 1.7589889131633214,
      "grad_norm": 15.832426071166992,
      "learning_rate": 8.292007826507865e-06,
      "loss": 2.574,
      "step": 172300
    },
    {
      "epoch": 1.7600098005186107,
      "grad_norm": 18.54440689086914,
      "learning_rate": 8.285190310946885e-06,
      "loss": 2.5606,
      "step": 172400
    },
    {
      "epoch": 1.7610306878739,
      "grad_norm": 15.148491859436035,
      "learning_rate": 8.278372795385906e-06,
      "loss": 2.5821,
      "step": 172500
    },
    {
      "epoch": 1.7620515752291892,
      "grad_norm": 17.861604690551758,
      "learning_rate": 8.271555279824926e-06,
      "loss": 2.5702,
      "step": 172600
    },
    {
      "epoch": 1.7630724625844785,
      "grad_norm": 17.781055450439453,
      "learning_rate": 8.264737764263947e-06,
      "loss": 2.5483,
      "step": 172700
    },
    {
      "epoch": 1.7640933499397677,
      "grad_norm": 16.397207260131836,
      "learning_rate": 8.257920248702969e-06,
      "loss": 2.5848,
      "step": 172800
    },
    {
      "epoch": 1.765114237295057,
      "grad_norm": 15.734406471252441,
      "learning_rate": 8.251102733141988e-06,
      "loss": 2.5494,
      "step": 172900
    },
    {
      "epoch": 1.766135124650346,
      "grad_norm": 17.68377685546875,
      "learning_rate": 8.24428521758101e-06,
      "loss": 2.6317,
      "step": 173000
    },
    {
      "epoch": 1.7671560120056353,
      "grad_norm": 11.760193824768066,
      "learning_rate": 8.23746770202003e-06,
      "loss": 2.5885,
      "step": 173100
    },
    {
      "epoch": 1.7681768993609244,
      "grad_norm": 15.232609748840332,
      "learning_rate": 8.230650186459052e-06,
      "loss": 2.5666,
      "step": 173200
    },
    {
      "epoch": 1.7691977867162136,
      "grad_norm": 16.727806091308594,
      "learning_rate": 8.223832670898072e-06,
      "loss": 2.5731,
      "step": 173300
    },
    {
      "epoch": 1.770218674071503,
      "grad_norm": 12.961736679077148,
      "learning_rate": 8.217015155337093e-06,
      "loss": 2.5014,
      "step": 173400
    },
    {
      "epoch": 1.7712395614267922,
      "grad_norm": 13.45544147491455,
      "learning_rate": 8.210197639776113e-06,
      "loss": 2.5388,
      "step": 173500
    },
    {
      "epoch": 1.7722604487820814,
      "grad_norm": 10.688307762145996,
      "learning_rate": 8.203380124215134e-06,
      "loss": 2.5807,
      "step": 173600
    },
    {
      "epoch": 1.7732813361373707,
      "grad_norm": 19.598485946655273,
      "learning_rate": 8.196562608654155e-06,
      "loss": 2.5459,
      "step": 173700
    },
    {
      "epoch": 1.77430222349266,
      "grad_norm": 18.059141159057617,
      "learning_rate": 8.189745093093175e-06,
      "loss": 2.5433,
      "step": 173800
    },
    {
      "epoch": 1.775323110847949,
      "grad_norm": 18.71045684814453,
      "learning_rate": 8.182927577532196e-06,
      "loss": 2.4943,
      "step": 173900
    },
    {
      "epoch": 1.7763439982032383,
      "grad_norm": 19.08626365661621,
      "learning_rate": 8.176110061971218e-06,
      "loss": 2.4913,
      "step": 174000
    },
    {
      "epoch": 1.7773648855585273,
      "grad_norm": 13.265776634216309,
      "learning_rate": 8.169292546410237e-06,
      "loss": 2.6078,
      "step": 174100
    },
    {
      "epoch": 1.7783857729138166,
      "grad_norm": 13.479410171508789,
      "learning_rate": 8.162475030849259e-06,
      "loss": 2.5009,
      "step": 174200
    },
    {
      "epoch": 1.7794066602691059,
      "grad_norm": 22.092079162597656,
      "learning_rate": 8.155657515288278e-06,
      "loss": 2.5597,
      "step": 174300
    },
    {
      "epoch": 1.7804275476243951,
      "grad_norm": 18.261625289916992,
      "learning_rate": 8.148839999727301e-06,
      "loss": 2.5726,
      "step": 174400
    },
    {
      "epoch": 1.7814484349796844,
      "grad_norm": 19.108362197875977,
      "learning_rate": 8.14202248416632e-06,
      "loss": 2.5536,
      "step": 174500
    },
    {
      "epoch": 1.7824693223349737,
      "grad_norm": 16.247770309448242,
      "learning_rate": 8.135204968605342e-06,
      "loss": 2.5563,
      "step": 174600
    },
    {
      "epoch": 1.7834902096902627,
      "grad_norm": 18.96022605895996,
      "learning_rate": 8.128387453044362e-06,
      "loss": 2.5132,
      "step": 174700
    },
    {
      "epoch": 1.784511097045552,
      "grad_norm": 13.143843650817871,
      "learning_rate": 8.121569937483383e-06,
      "loss": 2.5285,
      "step": 174800
    },
    {
      "epoch": 1.7855319844008413,
      "grad_norm": 16.766010284423828,
      "learning_rate": 8.114752421922404e-06,
      "loss": 2.4953,
      "step": 174900
    },
    {
      "epoch": 1.7865528717561303,
      "grad_norm": 15.466413497924805,
      "learning_rate": 8.107934906361424e-06,
      "loss": 2.5152,
      "step": 175000
    },
    {
      "epoch": 1.7875737591114196,
      "grad_norm": 15.738883972167969,
      "learning_rate": 8.101117390800445e-06,
      "loss": 2.5259,
      "step": 175100
    },
    {
      "epoch": 1.7885946464667088,
      "grad_norm": 16.07573890686035,
      "learning_rate": 8.094299875239467e-06,
      "loss": 2.5586,
      "step": 175200
    },
    {
      "epoch": 1.7896155338219981,
      "grad_norm": 16.234525680541992,
      "learning_rate": 8.087482359678486e-06,
      "loss": 2.5358,
      "step": 175300
    },
    {
      "epoch": 1.7906364211772874,
      "grad_norm": 17.557889938354492,
      "learning_rate": 8.080664844117508e-06,
      "loss": 2.5408,
      "step": 175400
    },
    {
      "epoch": 1.7916573085325767,
      "grad_norm": 13.587234497070312,
      "learning_rate": 8.073847328556527e-06,
      "loss": 2.5366,
      "step": 175500
    },
    {
      "epoch": 1.7926781958878657,
      "grad_norm": 17.731515884399414,
      "learning_rate": 8.06702981299555e-06,
      "loss": 2.5579,
      "step": 175600
    },
    {
      "epoch": 1.793699083243155,
      "grad_norm": 14.156238555908203,
      "learning_rate": 8.06021229743457e-06,
      "loss": 2.6173,
      "step": 175700
    },
    {
      "epoch": 1.794719970598444,
      "grad_norm": 12.828946113586426,
      "learning_rate": 8.05339478187359e-06,
      "loss": 2.615,
      "step": 175800
    },
    {
      "epoch": 1.7957408579537333,
      "grad_norm": 18.377870559692383,
      "learning_rate": 8.04657726631261e-06,
      "loss": 2.5592,
      "step": 175900
    },
    {
      "epoch": 1.7967617453090226,
      "grad_norm": 12.327610969543457,
      "learning_rate": 8.039759750751632e-06,
      "loss": 2.6114,
      "step": 176000
    },
    {
      "epoch": 1.7977826326643118,
      "grad_norm": 15.534425735473633,
      "learning_rate": 8.032942235190653e-06,
      "loss": 2.564,
      "step": 176100
    },
    {
      "epoch": 1.798803520019601,
      "grad_norm": 16.95781135559082,
      "learning_rate": 8.026124719629673e-06,
      "loss": 2.6521,
      "step": 176200
    },
    {
      "epoch": 1.7998244073748904,
      "grad_norm": 17.966474533081055,
      "learning_rate": 8.019307204068694e-06,
      "loss": 2.4844,
      "step": 176300
    },
    {
      "epoch": 1.8008452947301796,
      "grad_norm": 13.626765251159668,
      "learning_rate": 8.012489688507714e-06,
      "loss": 2.5533,
      "step": 176400
    },
    {
      "epoch": 1.8018661820854687,
      "grad_norm": 16.976806640625,
      "learning_rate": 8.005672172946735e-06,
      "loss": 2.605,
      "step": 176500
    },
    {
      "epoch": 1.802887069440758,
      "grad_norm": 17.4260311126709,
      "learning_rate": 7.998854657385757e-06,
      "loss": 2.6523,
      "step": 176600
    },
    {
      "epoch": 1.803907956796047,
      "grad_norm": 14.522027015686035,
      "learning_rate": 7.992037141824776e-06,
      "loss": 2.6206,
      "step": 176700
    },
    {
      "epoch": 1.8049288441513363,
      "grad_norm": 16.479305267333984,
      "learning_rate": 7.985219626263797e-06,
      "loss": 2.5784,
      "step": 176800
    },
    {
      "epoch": 1.8059497315066255,
      "grad_norm": 18.410961151123047,
      "learning_rate": 7.978402110702819e-06,
      "loss": 2.588,
      "step": 176900
    },
    {
      "epoch": 1.8069706188619148,
      "grad_norm": 16.26646614074707,
      "learning_rate": 7.971652770297448e-06,
      "loss": 2.5193,
      "step": 177000
    },
    {
      "epoch": 1.807991506217204,
      "grad_norm": 14.335234642028809,
      "learning_rate": 7.96483525473647e-06,
      "loss": 2.5145,
      "step": 177100
    },
    {
      "epoch": 1.8090123935724933,
      "grad_norm": 22.112241744995117,
      "learning_rate": 7.958017739175489e-06,
      "loss": 2.6002,
      "step": 177200
    },
    {
      "epoch": 1.8100332809277824,
      "grad_norm": 17.118377685546875,
      "learning_rate": 7.951200223614512e-06,
      "loss": 2.4896,
      "step": 177300
    },
    {
      "epoch": 1.8110541682830716,
      "grad_norm": 14.004591941833496,
      "learning_rate": 7.944382708053532e-06,
      "loss": 2.4979,
      "step": 177400
    },
    {
      "epoch": 1.812075055638361,
      "grad_norm": 16.707080841064453,
      "learning_rate": 7.937565192492553e-06,
      "loss": 2.539,
      "step": 177500
    },
    {
      "epoch": 1.81309594299365,
      "grad_norm": 16.1993350982666,
      "learning_rate": 7.930747676931573e-06,
      "loss": 2.5685,
      "step": 177600
    },
    {
      "epoch": 1.8141168303489392,
      "grad_norm": 16.880510330200195,
      "learning_rate": 7.923930161370594e-06,
      "loss": 2.556,
      "step": 177700
    },
    {
      "epoch": 1.8151377177042285,
      "grad_norm": 16.69947624206543,
      "learning_rate": 7.917112645809615e-06,
      "loss": 2.4462,
      "step": 177800
    },
    {
      "epoch": 1.8161586050595178,
      "grad_norm": 15.986854553222656,
      "learning_rate": 7.910295130248635e-06,
      "loss": 2.5624,
      "step": 177900
    },
    {
      "epoch": 1.817179492414807,
      "grad_norm": 14.36291217803955,
      "learning_rate": 7.903477614687656e-06,
      "loss": 2.4934,
      "step": 178000
    },
    {
      "epoch": 1.8182003797700963,
      "grad_norm": 16.228504180908203,
      "learning_rate": 7.896660099126678e-06,
      "loss": 2.5524,
      "step": 178100
    },
    {
      "epoch": 1.8192212671253853,
      "grad_norm": 16.58936882019043,
      "learning_rate": 7.889842583565697e-06,
      "loss": 2.5141,
      "step": 178200
    },
    {
      "epoch": 1.8202421544806746,
      "grad_norm": 14.278627395629883,
      "learning_rate": 7.883025068004718e-06,
      "loss": 2.4733,
      "step": 178300
    },
    {
      "epoch": 1.8212630418359637,
      "grad_norm": 17.624792098999023,
      "learning_rate": 7.876207552443738e-06,
      "loss": 2.5138,
      "step": 178400
    },
    {
      "epoch": 1.822283929191253,
      "grad_norm": 13.814037322998047,
      "learning_rate": 7.869390036882761e-06,
      "loss": 2.4912,
      "step": 178500
    },
    {
      "epoch": 1.8233048165465422,
      "grad_norm": 15.667158126831055,
      "learning_rate": 7.86257252132178e-06,
      "loss": 2.5956,
      "step": 178600
    },
    {
      "epoch": 1.8243257039018315,
      "grad_norm": 19.55665397644043,
      "learning_rate": 7.855755005760802e-06,
      "loss": 2.5567,
      "step": 178700
    },
    {
      "epoch": 1.8253465912571207,
      "grad_norm": 20.47868537902832,
      "learning_rate": 7.848937490199822e-06,
      "loss": 2.5495,
      "step": 178800
    },
    {
      "epoch": 1.82636747861241,
      "grad_norm": 14.580953598022461,
      "learning_rate": 7.842119974638843e-06,
      "loss": 2.5535,
      "step": 178900
    },
    {
      "epoch": 1.8273883659676993,
      "grad_norm": 17.251781463623047,
      "learning_rate": 7.835302459077864e-06,
      "loss": 2.5014,
      "step": 179000
    },
    {
      "epoch": 1.8284092533229883,
      "grad_norm": 19.702762603759766,
      "learning_rate": 7.828484943516884e-06,
      "loss": 2.5389,
      "step": 179100
    },
    {
      "epoch": 1.8294301406782776,
      "grad_norm": 15.70628833770752,
      "learning_rate": 7.821667427955905e-06,
      "loss": 2.6333,
      "step": 179200
    },
    {
      "epoch": 1.8304510280335666,
      "grad_norm": 19.08727264404297,
      "learning_rate": 7.814849912394926e-06,
      "loss": 2.5498,
      "step": 179300
    },
    {
      "epoch": 1.831471915388856,
      "grad_norm": 14.818500518798828,
      "learning_rate": 7.808032396833946e-06,
      "loss": 2.5929,
      "step": 179400
    },
    {
      "epoch": 1.8324928027441452,
      "grad_norm": 14.560455322265625,
      "learning_rate": 7.801214881272967e-06,
      "loss": 2.5768,
      "step": 179500
    },
    {
      "epoch": 1.8335136900994344,
      "grad_norm": 13.185827255249023,
      "learning_rate": 7.794397365711987e-06,
      "loss": 2.6828,
      "step": 179600
    },
    {
      "epoch": 1.8345345774547237,
      "grad_norm": 14.827425003051758,
      "learning_rate": 7.787579850151008e-06,
      "loss": 2.4914,
      "step": 179700
    },
    {
      "epoch": 1.835555464810013,
      "grad_norm": 16.455623626708984,
      "learning_rate": 7.78076233459003e-06,
      "loss": 2.497,
      "step": 179800
    },
    {
      "epoch": 1.8365763521653022,
      "grad_norm": 14.49338150024414,
      "learning_rate": 7.77394481902905e-06,
      "loss": 2.5599,
      "step": 179900
    },
    {
      "epoch": 1.8375972395205913,
      "grad_norm": 20.488998413085938,
      "learning_rate": 7.76712730346807e-06,
      "loss": 2.5687,
      "step": 180000
    },
    {
      "epoch": 1.8386181268758806,
      "grad_norm": 17.181533813476562,
      "learning_rate": 7.760377963062702e-06,
      "loss": 2.5924,
      "step": 180100
    },
    {
      "epoch": 1.8396390142311696,
      "grad_norm": 17.928983688354492,
      "learning_rate": 7.753560447501723e-06,
      "loss": 2.5853,
      "step": 180200
    },
    {
      "epoch": 1.8406599015864589,
      "grad_norm": 19.7132511138916,
      "learning_rate": 7.746742931940743e-06,
      "loss": 2.5516,
      "step": 180300
    },
    {
      "epoch": 1.8416807889417481,
      "grad_norm": 17.164093017578125,
      "learning_rate": 7.739925416379764e-06,
      "loss": 2.5641,
      "step": 180400
    },
    {
      "epoch": 1.8427016762970374,
      "grad_norm": 15.29035472869873,
      "learning_rate": 7.733107900818784e-06,
      "loss": 2.4914,
      "step": 180500
    },
    {
      "epoch": 1.8437225636523267,
      "grad_norm": 14.49611759185791,
      "learning_rate": 7.726358560413415e-06,
      "loss": 2.5404,
      "step": 180600
    },
    {
      "epoch": 1.844743451007616,
      "grad_norm": 16.157407760620117,
      "learning_rate": 7.719541044852436e-06,
      "loss": 2.5377,
      "step": 180700
    },
    {
      "epoch": 1.845764338362905,
      "grad_norm": 15.133827209472656,
      "learning_rate": 7.712723529291456e-06,
      "loss": 2.5555,
      "step": 180800
    },
    {
      "epoch": 1.8467852257181943,
      "grad_norm": 14.774191856384277,
      "learning_rate": 7.705906013730477e-06,
      "loss": 2.5643,
      "step": 180900
    },
    {
      "epoch": 1.8478061130734835,
      "grad_norm": 20.912925720214844,
      "learning_rate": 7.699088498169498e-06,
      "loss": 2.552,
      "step": 181000
    },
    {
      "epoch": 1.8488270004287726,
      "grad_norm": 18.768245697021484,
      "learning_rate": 7.692270982608518e-06,
      "loss": 2.5916,
      "step": 181100
    },
    {
      "epoch": 1.8498478877840618,
      "grad_norm": 18.42960548400879,
      "learning_rate": 7.685453467047539e-06,
      "loss": 2.5126,
      "step": 181200
    },
    {
      "epoch": 1.850868775139351,
      "grad_norm": 16.436946868896484,
      "learning_rate": 7.678635951486559e-06,
      "loss": 2.6443,
      "step": 181300
    },
    {
      "epoch": 1.8518896624946404,
      "grad_norm": 14.37082290649414,
      "learning_rate": 7.671818435925582e-06,
      "loss": 2.555,
      "step": 181400
    },
    {
      "epoch": 1.8529105498499296,
      "grad_norm": 13.438319206237793,
      "learning_rate": 7.665000920364601e-06,
      "loss": 2.5963,
      "step": 181500
    },
    {
      "epoch": 1.853931437205219,
      "grad_norm": 14.71932315826416,
      "learning_rate": 7.658183404803623e-06,
      "loss": 2.4954,
      "step": 181600
    },
    {
      "epoch": 1.854952324560508,
      "grad_norm": 17.292905807495117,
      "learning_rate": 7.651365889242642e-06,
      "loss": 2.5295,
      "step": 181700
    },
    {
      "epoch": 1.8559732119157972,
      "grad_norm": 14.139132499694824,
      "learning_rate": 7.644548373681664e-06,
      "loss": 2.5378,
      "step": 181800
    },
    {
      "epoch": 1.8569940992710863,
      "grad_norm": 17.700382232666016,
      "learning_rate": 7.637730858120685e-06,
      "loss": 2.6277,
      "step": 181900
    },
    {
      "epoch": 1.8580149866263755,
      "grad_norm": 14.841828346252441,
      "learning_rate": 7.630913342559705e-06,
      "loss": 2.5532,
      "step": 182000
    },
    {
      "epoch": 1.8590358739816648,
      "grad_norm": 18.64600372314453,
      "learning_rate": 7.624095826998725e-06,
      "loss": 2.4829,
      "step": 182100
    },
    {
      "epoch": 1.860056761336954,
      "grad_norm": 15.122225761413574,
      "learning_rate": 7.617346486593357e-06,
      "loss": 2.5929,
      "step": 182200
    },
    {
      "epoch": 1.8610776486922433,
      "grad_norm": 21.985675811767578,
      "learning_rate": 7.610528971032377e-06,
      "loss": 2.4894,
      "step": 182300
    },
    {
      "epoch": 1.8620985360475326,
      "grad_norm": 13.85528564453125,
      "learning_rate": 7.603711455471398e-06,
      "loss": 2.5719,
      "step": 182400
    },
    {
      "epoch": 1.8631194234028219,
      "grad_norm": 19.229228973388672,
      "learning_rate": 7.596893939910418e-06,
      "loss": 2.5703,
      "step": 182500
    },
    {
      "epoch": 1.864140310758111,
      "grad_norm": 16.515277862548828,
      "learning_rate": 7.59007642434944e-06,
      "loss": 2.5589,
      "step": 182600
    },
    {
      "epoch": 1.8651611981134002,
      "grad_norm": 14.565777778625488,
      "learning_rate": 7.58325890878846e-06,
      "loss": 2.5109,
      "step": 182700
    },
    {
      "epoch": 1.8661820854686892,
      "grad_norm": 15.864530563354492,
      "learning_rate": 7.5764413932274806e-06,
      "loss": 2.5705,
      "step": 182800
    },
    {
      "epoch": 1.8672029728239785,
      "grad_norm": 14.840770721435547,
      "learning_rate": 7.569623877666501e-06,
      "loss": 2.6218,
      "step": 182900
    },
    {
      "epoch": 1.8682238601792678,
      "grad_norm": 17.885103225708008,
      "learning_rate": 7.562806362105522e-06,
      "loss": 2.5727,
      "step": 183000
    },
    {
      "epoch": 1.869244747534557,
      "grad_norm": 12.686087608337402,
      "learning_rate": 7.555988846544543e-06,
      "loss": 2.6311,
      "step": 183100
    },
    {
      "epoch": 1.8702656348898463,
      "grad_norm": 18.423828125,
      "learning_rate": 7.549171330983563e-06,
      "loss": 2.4893,
      "step": 183200
    },
    {
      "epoch": 1.8712865222451356,
      "grad_norm": 13.68307113647461,
      "learning_rate": 7.542353815422584e-06,
      "loss": 2.5874,
      "step": 183300
    },
    {
      "epoch": 1.8723074096004249,
      "grad_norm": 13.62741756439209,
      "learning_rate": 7.535536299861606e-06,
      "loss": 2.5439,
      "step": 183400
    },
    {
      "epoch": 1.873328296955714,
      "grad_norm": 12.078619956970215,
      "learning_rate": 7.528718784300626e-06,
      "loss": 2.5889,
      "step": 183500
    },
    {
      "epoch": 1.8743491843110032,
      "grad_norm": 16.05615234375,
      "learning_rate": 7.521901268739647e-06,
      "loss": 2.5412,
      "step": 183600
    },
    {
      "epoch": 1.8753700716662922,
      "grad_norm": 20.994733810424805,
      "learning_rate": 7.515083753178667e-06,
      "loss": 2.5231,
      "step": 183700
    },
    {
      "epoch": 1.8763909590215815,
      "grad_norm": 15.862170219421387,
      "learning_rate": 7.508266237617689e-06,
      "loss": 2.6329,
      "step": 183800
    },
    {
      "epoch": 1.8774118463768708,
      "grad_norm": 13.984801292419434,
      "learning_rate": 7.501448722056709e-06,
      "loss": 2.4901,
      "step": 183900
    },
    {
      "epoch": 1.87843273373216,
      "grad_norm": 15.980700492858887,
      "learning_rate": 7.4946312064957295e-06,
      "loss": 2.494,
      "step": 184000
    },
    {
      "epoch": 1.8794536210874493,
      "grad_norm": 18.59994888305664,
      "learning_rate": 7.48781369093475e-06,
      "loss": 2.5078,
      "step": 184100
    },
    {
      "epoch": 1.8804745084427386,
      "grad_norm": 20.351947784423828,
      "learning_rate": 7.480996175373771e-06,
      "loss": 2.5697,
      "step": 184200
    },
    {
      "epoch": 1.8814953957980276,
      "grad_norm": 20.029434204101562,
      "learning_rate": 7.474178659812792e-06,
      "loss": 2.5036,
      "step": 184300
    },
    {
      "epoch": 1.8825162831533169,
      "grad_norm": 16.020797729492188,
      "learning_rate": 7.467361144251812e-06,
      "loss": 2.5145,
      "step": 184400
    },
    {
      "epoch": 1.883537170508606,
      "grad_norm": 16.96306037902832,
      "learning_rate": 7.460543628690833e-06,
      "loss": 2.5684,
      "step": 184500
    },
    {
      "epoch": 1.8845580578638952,
      "grad_norm": 16.73936653137207,
      "learning_rate": 7.453726113129853e-06,
      "loss": 2.5322,
      "step": 184600
    },
    {
      "epoch": 1.8855789452191845,
      "grad_norm": 12.639314651489258,
      "learning_rate": 7.4469085975688745e-06,
      "loss": 2.5931,
      "step": 184700
    },
    {
      "epoch": 1.8865998325744737,
      "grad_norm": 16.245655059814453,
      "learning_rate": 7.440091082007895e-06,
      "loss": 2.588,
      "step": 184800
    },
    {
      "epoch": 1.887620719929763,
      "grad_norm": 12.90404987335205,
      "learning_rate": 7.433273566446915e-06,
      "loss": 2.5748,
      "step": 184900
    },
    {
      "epoch": 1.8886416072850523,
      "grad_norm": 16.74128532409668,
      "learning_rate": 7.426456050885936e-06,
      "loss": 2.5321,
      "step": 185000
    },
    {
      "epoch": 1.8896624946403415,
      "grad_norm": 14.969952583312988,
      "learning_rate": 7.419638535324958e-06,
      "loss": 2.5162,
      "step": 185100
    },
    {
      "epoch": 1.8906833819956306,
      "grad_norm": 15.067776679992676,
      "learning_rate": 7.4128210197639785e-06,
      "loss": 2.5014,
      "step": 185200
    },
    {
      "epoch": 1.8917042693509198,
      "grad_norm": 13.749592781066895,
      "learning_rate": 7.406003504202999e-06,
      "loss": 2.6341,
      "step": 185300
    },
    {
      "epoch": 1.8927251567062089,
      "grad_norm": 16.88998031616211,
      "learning_rate": 7.3991859886420194e-06,
      "loss": 2.5405,
      "step": 185400
    },
    {
      "epoch": 1.8937460440614982,
      "grad_norm": 20.94685935974121,
      "learning_rate": 7.392368473081041e-06,
      "loss": 2.5051,
      "step": 185500
    },
    {
      "epoch": 1.8947669314167874,
      "grad_norm": 16.457469940185547,
      "learning_rate": 7.385550957520061e-06,
      "loss": 2.5269,
      "step": 185600
    },
    {
      "epoch": 1.8957878187720767,
      "grad_norm": 15.924805641174316,
      "learning_rate": 7.378733441959082e-06,
      "loss": 2.612,
      "step": 185700
    },
    {
      "epoch": 1.896808706127366,
      "grad_norm": 21.190948486328125,
      "learning_rate": 7.371915926398102e-06,
      "loss": 2.6361,
      "step": 185800
    },
    {
      "epoch": 1.8978295934826552,
      "grad_norm": 19.65740203857422,
      "learning_rate": 7.3650984108371235e-06,
      "loss": 2.5285,
      "step": 185900
    },
    {
      "epoch": 1.8988504808379445,
      "grad_norm": 17.380231857299805,
      "learning_rate": 7.358280895276144e-06,
      "loss": 2.5963,
      "step": 186000
    },
    {
      "epoch": 1.8998713681932335,
      "grad_norm": 16.009275436401367,
      "learning_rate": 7.351463379715164e-06,
      "loss": 2.5166,
      "step": 186100
    },
    {
      "epoch": 1.9008922555485228,
      "grad_norm": 21.68360137939453,
      "learning_rate": 7.344645864154185e-06,
      "loss": 2.592,
      "step": 186200
    },
    {
      "epoch": 1.9019131429038119,
      "grad_norm": 13.209153175354004,
      "learning_rate": 7.337828348593207e-06,
      "loss": 2.63,
      "step": 186300
    },
    {
      "epoch": 1.9029340302591011,
      "grad_norm": 12.036482810974121,
      "learning_rate": 7.3310108330322275e-06,
      "loss": 2.4736,
      "step": 186400
    },
    {
      "epoch": 1.9039549176143904,
      "grad_norm": 16.471712112426758,
      "learning_rate": 7.324193317471248e-06,
      "loss": 2.5077,
      "step": 186500
    },
    {
      "epoch": 1.9049758049696797,
      "grad_norm": 13.815495491027832,
      "learning_rate": 7.317375801910268e-06,
      "loss": 2.6185,
      "step": 186600
    },
    {
      "epoch": 1.905996692324969,
      "grad_norm": 12.814621925354004,
      "learning_rate": 7.31055828634929e-06,
      "loss": 2.5475,
      "step": 186700
    },
    {
      "epoch": 1.9070175796802582,
      "grad_norm": 17.77927017211914,
      "learning_rate": 7.30374077078831e-06,
      "loss": 2.4828,
      "step": 186800
    },
    {
      "epoch": 1.9080384670355472,
      "grad_norm": 13.612772941589355,
      "learning_rate": 7.296923255227331e-06,
      "loss": 2.5187,
      "step": 186900
    },
    {
      "epoch": 1.9090593543908365,
      "grad_norm": 19.767423629760742,
      "learning_rate": 7.290105739666351e-06,
      "loss": 2.4834,
      "step": 187000
    },
    {
      "epoch": 1.9100802417461258,
      "grad_norm": 14.33957290649414,
      "learning_rate": 7.2832882241053724e-06,
      "loss": 2.5229,
      "step": 187100
    },
    {
      "epoch": 1.9111011291014148,
      "grad_norm": 17.508102416992188,
      "learning_rate": 7.276470708544393e-06,
      "loss": 2.5256,
      "step": 187200
    },
    {
      "epoch": 1.912122016456704,
      "grad_norm": 23.772579193115234,
      "learning_rate": 7.269653192983413e-06,
      "loss": 2.5267,
      "step": 187300
    },
    {
      "epoch": 1.9131429038119934,
      "grad_norm": 13.337556838989258,
      "learning_rate": 7.262835677422434e-06,
      "loss": 2.5515,
      "step": 187400
    },
    {
      "epoch": 1.9141637911672826,
      "grad_norm": 16.345043182373047,
      "learning_rate": 7.256018161861456e-06,
      "loss": 2.4702,
      "step": 187500
    },
    {
      "epoch": 1.915184678522572,
      "grad_norm": 32.532752990722656,
      "learning_rate": 7.249268821456085e-06,
      "loss": 2.5316,
      "step": 187600
    },
    {
      "epoch": 1.9162055658778612,
      "grad_norm": 15.010828971862793,
      "learning_rate": 7.242451305895106e-06,
      "loss": 2.5495,
      "step": 187700
    },
    {
      "epoch": 1.9172264532331502,
      "grad_norm": 20.142120361328125,
      "learning_rate": 7.235633790334126e-06,
      "loss": 2.522,
      "step": 187800
    },
    {
      "epoch": 1.9182473405884395,
      "grad_norm": 17.047943115234375,
      "learning_rate": 7.2288162747731485e-06,
      "loss": 2.6075,
      "step": 187900
    },
    {
      "epoch": 1.9192682279437285,
      "grad_norm": 14.244081497192383,
      "learning_rate": 7.221998759212169e-06,
      "loss": 2.4771,
      "step": 188000
    },
    {
      "epoch": 1.9202891152990178,
      "grad_norm": 13.215232849121094,
      "learning_rate": 7.215181243651189e-06,
      "loss": 2.5016,
      "step": 188100
    },
    {
      "epoch": 1.921310002654307,
      "grad_norm": 16.886234283447266,
      "learning_rate": 7.20836372809021e-06,
      "loss": 2.5345,
      "step": 188200
    },
    {
      "epoch": 1.9223308900095963,
      "grad_norm": 16.23383903503418,
      "learning_rate": 7.20154621252923e-06,
      "loss": 2.5587,
      "step": 188300
    },
    {
      "epoch": 1.9233517773648856,
      "grad_norm": 15.38980484008789,
      "learning_rate": 7.194728696968252e-06,
      "loss": 2.6305,
      "step": 188400
    },
    {
      "epoch": 1.9243726647201749,
      "grad_norm": 13.555225372314453,
      "learning_rate": 7.187911181407272e-06,
      "loss": 2.5441,
      "step": 188500
    },
    {
      "epoch": 1.9253935520754641,
      "grad_norm": 16.18878936767578,
      "learning_rate": 7.181093665846293e-06,
      "loss": 2.4798,
      "step": 188600
    },
    {
      "epoch": 1.9264144394307532,
      "grad_norm": 19.4832763671875,
      "learning_rate": 7.174276150285313e-06,
      "loss": 2.5586,
      "step": 188700
    },
    {
      "epoch": 1.9274353267860425,
      "grad_norm": 19.336565017700195,
      "learning_rate": 7.167458634724334e-06,
      "loss": 2.5424,
      "step": 188800
    },
    {
      "epoch": 1.9284562141413315,
      "grad_norm": 16.083349227905273,
      "learning_rate": 7.160641119163355e-06,
      "loss": 2.5314,
      "step": 188900
    },
    {
      "epoch": 1.9294771014966208,
      "grad_norm": 18.300161361694336,
      "learning_rate": 7.153823603602375e-06,
      "loss": 2.5456,
      "step": 189000
    },
    {
      "epoch": 1.93049798885191,
      "grad_norm": 15.306392669677734,
      "learning_rate": 7.147006088041396e-06,
      "loss": 2.5107,
      "step": 189100
    },
    {
      "epoch": 1.9315188762071993,
      "grad_norm": 13.58676528930664,
      "learning_rate": 7.140188572480418e-06,
      "loss": 2.5893,
      "step": 189200
    },
    {
      "epoch": 1.9325397635624886,
      "grad_norm": 18.54159164428711,
      "learning_rate": 7.133371056919438e-06,
      "loss": 2.619,
      "step": 189300
    },
    {
      "epoch": 1.9335606509177778,
      "grad_norm": 15.026662826538086,
      "learning_rate": 7.126553541358459e-06,
      "loss": 2.5577,
      "step": 189400
    },
    {
      "epoch": 1.9345815382730671,
      "grad_norm": 18.430225372314453,
      "learning_rate": 7.119736025797479e-06,
      "loss": 2.5417,
      "step": 189500
    },
    {
      "epoch": 1.9356024256283562,
      "grad_norm": 18.501359939575195,
      "learning_rate": 7.112918510236501e-06,
      "loss": 2.5425,
      "step": 189600
    },
    {
      "epoch": 1.9366233129836454,
      "grad_norm": 17.351186752319336,
      "learning_rate": 7.106100994675521e-06,
      "loss": 2.5164,
      "step": 189700
    },
    {
      "epoch": 1.9376442003389345,
      "grad_norm": 15.272762298583984,
      "learning_rate": 7.0992834791145416e-06,
      "loss": 2.5403,
      "step": 189800
    },
    {
      "epoch": 1.9386650876942237,
      "grad_norm": 16.952390670776367,
      "learning_rate": 7.092465963553562e-06,
      "loss": 2.5515,
      "step": 189900
    },
    {
      "epoch": 1.939685975049513,
      "grad_norm": 13.66716194152832,
      "learning_rate": 7.085648447992583e-06,
      "loss": 2.5325,
      "step": 190000
    },
    {
      "epoch": 1.9407068624048023,
      "grad_norm": 15.86540699005127,
      "learning_rate": 7.078830932431604e-06,
      "loss": 2.5797,
      "step": 190100
    },
    {
      "epoch": 1.9417277497600915,
      "grad_norm": 17.615488052368164,
      "learning_rate": 7.072013416870624e-06,
      "loss": 2.4726,
      "step": 190200
    },
    {
      "epoch": 1.9427486371153808,
      "grad_norm": 12.909910202026367,
      "learning_rate": 7.065195901309645e-06,
      "loss": 2.6725,
      "step": 190300
    },
    {
      "epoch": 1.9437695244706699,
      "grad_norm": 17.342863082885742,
      "learning_rate": 7.058378385748667e-06,
      "loss": 2.5705,
      "step": 190400
    },
    {
      "epoch": 1.9447904118259591,
      "grad_norm": 16.394264221191406,
      "learning_rate": 7.051560870187687e-06,
      "loss": 2.5543,
      "step": 190500
    },
    {
      "epoch": 1.9458112991812482,
      "grad_norm": 16.25718116760254,
      "learning_rate": 7.044743354626708e-06,
      "loss": 2.5436,
      "step": 190600
    },
    {
      "epoch": 1.9468321865365374,
      "grad_norm": 13.64887809753418,
      "learning_rate": 7.037925839065728e-06,
      "loss": 2.5815,
      "step": 190700
    },
    {
      "epoch": 1.9478530738918267,
      "grad_norm": 18.463985443115234,
      "learning_rate": 7.03110832350475e-06,
      "loss": 2.4684,
      "step": 190800
    },
    {
      "epoch": 1.948873961247116,
      "grad_norm": 26.683914184570312,
      "learning_rate": 7.02429080794377e-06,
      "loss": 2.4849,
      "step": 190900
    },
    {
      "epoch": 1.9498948486024053,
      "grad_norm": 16.587390899658203,
      "learning_rate": 7.0174732923827905e-06,
      "loss": 2.4926,
      "step": 191000
    },
    {
      "epoch": 1.9509157359576945,
      "grad_norm": 14.835627555847168,
      "learning_rate": 7.010655776821811e-06,
      "loss": 2.5197,
      "step": 191100
    },
    {
      "epoch": 1.9519366233129838,
      "grad_norm": 17.23017120361328,
      "learning_rate": 7.003838261260832e-06,
      "loss": 2.5761,
      "step": 191200
    },
    {
      "epoch": 1.9529575106682728,
      "grad_norm": 18.017982482910156,
      "learning_rate": 6.997020745699853e-06,
      "loss": 2.4934,
      "step": 191300
    },
    {
      "epoch": 1.953978398023562,
      "grad_norm": 15.834219932556152,
      "learning_rate": 6.990203230138873e-06,
      "loss": 2.5404,
      "step": 191400
    },
    {
      "epoch": 1.9549992853788511,
      "grad_norm": 16.961036682128906,
      "learning_rate": 6.983385714577894e-06,
      "loss": 2.5912,
      "step": 191500
    },
    {
      "epoch": 1.9560201727341404,
      "grad_norm": 20.52972412109375,
      "learning_rate": 6.976568199016914e-06,
      "loss": 2.5505,
      "step": 191600
    },
    {
      "epoch": 1.9570410600894297,
      "grad_norm": 21.298498153686523,
      "learning_rate": 6.9697506834559355e-06,
      "loss": 2.5908,
      "step": 191700
    },
    {
      "epoch": 1.958061947444719,
      "grad_norm": 14.814704895019531,
      "learning_rate": 6.962933167894956e-06,
      "loss": 2.5212,
      "step": 191800
    },
    {
      "epoch": 1.9590828348000082,
      "grad_norm": 15.678630828857422,
      "learning_rate": 6.956115652333976e-06,
      "loss": 2.5097,
      "step": 191900
    },
    {
      "epoch": 1.9601037221552975,
      "grad_norm": 19.633724212646484,
      "learning_rate": 6.949298136772997e-06,
      "loss": 2.5601,
      "step": 192000
    },
    {
      "epoch": 1.9611246095105868,
      "grad_norm": 18.339061737060547,
      "learning_rate": 6.942548796367629e-06,
      "loss": 2.4887,
      "step": 192100
    },
    {
      "epoch": 1.9621454968658758,
      "grad_norm": 19.93499755859375,
      "learning_rate": 6.935731280806649e-06,
      "loss": 2.4681,
      "step": 192200
    },
    {
      "epoch": 1.963166384221165,
      "grad_norm": 15.607712745666504,
      "learning_rate": 6.92891376524567e-06,
      "loss": 2.5238,
      "step": 192300
    },
    {
      "epoch": 1.9641872715764541,
      "grad_norm": 13.65249252319336,
      "learning_rate": 6.92209624968469e-06,
      "loss": 2.6144,
      "step": 192400
    },
    {
      "epoch": 1.9652081589317434,
      "grad_norm": 21.896347045898438,
      "learning_rate": 6.9152787341237115e-06,
      "loss": 2.5057,
      "step": 192500
    },
    {
      "epoch": 1.9662290462870327,
      "grad_norm": 15.482840538024902,
      "learning_rate": 6.908461218562732e-06,
      "loss": 2.5427,
      "step": 192600
    },
    {
      "epoch": 1.967249933642322,
      "grad_norm": 14.38451099395752,
      "learning_rate": 6.9016437030017525e-06,
      "loss": 2.4121,
      "step": 192700
    },
    {
      "epoch": 1.9682708209976112,
      "grad_norm": 20.787097930908203,
      "learning_rate": 6.894826187440773e-06,
      "loss": 2.6048,
      "step": 192800
    },
    {
      "epoch": 1.9692917083529005,
      "grad_norm": 15.57509994506836,
      "learning_rate": 6.888008671879794e-06,
      "loss": 2.4724,
      "step": 192900
    },
    {
      "epoch": 1.9703125957081895,
      "grad_norm": 15.197792053222656,
      "learning_rate": 6.881191156318815e-06,
      "loss": 2.4468,
      "step": 193000
    },
    {
      "epoch": 1.9713334830634788,
      "grad_norm": 16.582651138305664,
      "learning_rate": 6.874373640757835e-06,
      "loss": 2.5603,
      "step": 193100
    },
    {
      "epoch": 1.972354370418768,
      "grad_norm": 14.705167770385742,
      "learning_rate": 6.867556125196856e-06,
      "loss": 2.542,
      "step": 193200
    },
    {
      "epoch": 1.973375257774057,
      "grad_norm": 18.20194435119629,
      "learning_rate": 6.860738609635878e-06,
      "loss": 2.5072,
      "step": 193300
    },
    {
      "epoch": 1.9743961451293464,
      "grad_norm": 15.25793170928955,
      "learning_rate": 6.853921094074898e-06,
      "loss": 2.4304,
      "step": 193400
    },
    {
      "epoch": 1.9754170324846356,
      "grad_norm": 20.000160217285156,
      "learning_rate": 6.847103578513919e-06,
      "loss": 2.6012,
      "step": 193500
    },
    {
      "epoch": 1.976437919839925,
      "grad_norm": 15.817843437194824,
      "learning_rate": 6.840286062952939e-06,
      "loss": 2.3983,
      "step": 193600
    },
    {
      "epoch": 1.9774588071952142,
      "grad_norm": 12.20382308959961,
      "learning_rate": 6.8334685473919605e-06,
      "loss": 2.5835,
      "step": 193700
    },
    {
      "epoch": 1.9784796945505034,
      "grad_norm": 15.780710220336914,
      "learning_rate": 6.826651031830981e-06,
      "loss": 2.5656,
      "step": 193800
    },
    {
      "epoch": 1.9795005819057925,
      "grad_norm": 17.1900691986084,
      "learning_rate": 6.819901691425611e-06,
      "loss": 2.5784,
      "step": 193900
    },
    {
      "epoch": 1.9805214692610817,
      "grad_norm": 13.853535652160645,
      "learning_rate": 6.813084175864632e-06,
      "loss": 2.5365,
      "step": 194000
    },
    {
      "epoch": 1.9815423566163708,
      "grad_norm": 21.390384674072266,
      "learning_rate": 6.806266660303653e-06,
      "loss": 2.5039,
      "step": 194100
    },
    {
      "epoch": 1.98256324397166,
      "grad_norm": 19.278972625732422,
      "learning_rate": 6.7994491447426734e-06,
      "loss": 2.5827,
      "step": 194200
    },
    {
      "epoch": 1.9835841313269493,
      "grad_norm": 14.800477981567383,
      "learning_rate": 6.792631629181694e-06,
      "loss": 2.5239,
      "step": 194300
    },
    {
      "epoch": 1.9846050186822386,
      "grad_norm": 18.634071350097656,
      "learning_rate": 6.785814113620714e-06,
      "loss": 2.5907,
      "step": 194400
    },
    {
      "epoch": 1.9856259060375279,
      "grad_norm": 17.26498031616211,
      "learning_rate": 6.778996598059736e-06,
      "loss": 2.6297,
      "step": 194500
    },
    {
      "epoch": 1.9866467933928171,
      "grad_norm": 19.197662353515625,
      "learning_rate": 6.772179082498756e-06,
      "loss": 2.563,
      "step": 194600
    },
    {
      "epoch": 1.9876676807481064,
      "grad_norm": 19.710912704467773,
      "learning_rate": 6.765361566937777e-06,
      "loss": 2.5555,
      "step": 194700
    },
    {
      "epoch": 1.9886885681033954,
      "grad_norm": 15.7547025680542,
      "learning_rate": 6.758544051376797e-06,
      "loss": 2.5068,
      "step": 194800
    },
    {
      "epoch": 1.9897094554586847,
      "grad_norm": 18.36589241027832,
      "learning_rate": 6.751726535815819e-06,
      "loss": 2.5337,
      "step": 194900
    },
    {
      "epoch": 1.9907303428139738,
      "grad_norm": 16.992738723754883,
      "learning_rate": 6.74490902025484e-06,
      "loss": 2.5101,
      "step": 195000
    },
    {
      "epoch": 1.991751230169263,
      "grad_norm": 15.513450622558594,
      "learning_rate": 6.73809150469386e-06,
      "loss": 2.4708,
      "step": 195100
    },
    {
      "epoch": 1.9927721175245523,
      "grad_norm": 13.687627792358398,
      "learning_rate": 6.731273989132881e-06,
      "loss": 2.4775,
      "step": 195200
    },
    {
      "epoch": 1.9937930048798416,
      "grad_norm": 17.43164825439453,
      "learning_rate": 6.724456473571902e-06,
      "loss": 2.5095,
      "step": 195300
    },
    {
      "epoch": 1.9948138922351308,
      "grad_norm": 18.271015167236328,
      "learning_rate": 6.717638958010922e-06,
      "loss": 2.5389,
      "step": 195400
    },
    {
      "epoch": 1.99583477959042,
      "grad_norm": 17.26091766357422,
      "learning_rate": 6.710821442449943e-06,
      "loss": 2.5008,
      "step": 195500
    },
    {
      "epoch": 1.9968556669457094,
      "grad_norm": 15.849424362182617,
      "learning_rate": 6.704003926888963e-06,
      "loss": 2.5055,
      "step": 195600
    },
    {
      "epoch": 1.9978765543009984,
      "grad_norm": 16.35654067993164,
      "learning_rate": 6.697186411327984e-06,
      "loss": 2.5372,
      "step": 195700
    },
    {
      "epoch": 1.9988974416562877,
      "grad_norm": 16.80613899230957,
      "learning_rate": 6.690368895767005e-06,
      "loss": 2.5139,
      "step": 195800
    },
    {
      "epoch": 1.9999183290115767,
      "grad_norm": 16.60234832763672,
      "learning_rate": 6.683551380206026e-06,
      "loss": 2.4792,
      "step": 195900
    },
    {
      "epoch": 2.000939216366866,
      "grad_norm": 18.68174171447754,
      "learning_rate": 6.676733864645046e-06,
      "loss": 2.4989,
      "step": 196000
    },
    {
      "epoch": 2.0019601037221553,
      "grad_norm": 17.405136108398438,
      "learning_rate": 6.6699163490840665e-06,
      "loss": 2.4536,
      "step": 196100
    },
    {
      "epoch": 2.0029809910774445,
      "grad_norm": 14.861824989318848,
      "learning_rate": 6.663098833523089e-06,
      "loss": 2.5393,
      "step": 196200
    },
    {
      "epoch": 2.004001878432734,
      "grad_norm": 20.63228416442871,
      "learning_rate": 6.656281317962109e-06,
      "loss": 2.5362,
      "step": 196300
    },
    {
      "epoch": 2.005022765788023,
      "grad_norm": 16.51541519165039,
      "learning_rate": 6.64946380240113e-06,
      "loss": 2.5048,
      "step": 196400
    },
    {
      "epoch": 2.0060436531433123,
      "grad_norm": 14.552027702331543,
      "learning_rate": 6.64264628684015e-06,
      "loss": 2.4856,
      "step": 196500
    },
    {
      "epoch": 2.007064540498601,
      "grad_norm": 13.597309112548828,
      "learning_rate": 6.635828771279171e-06,
      "loss": 2.435,
      "step": 196600
    },
    {
      "epoch": 2.0080854278538904,
      "grad_norm": 17.23441505432129,
      "learning_rate": 6.629011255718192e-06,
      "loss": 2.5012,
      "step": 196700
    },
    {
      "epoch": 2.0091063152091797,
      "grad_norm": 16.942546844482422,
      "learning_rate": 6.622193740157212e-06,
      "loss": 2.5217,
      "step": 196800
    },
    {
      "epoch": 2.010127202564469,
      "grad_norm": 17.25127410888672,
      "learning_rate": 6.615376224596233e-06,
      "loss": 2.5171,
      "step": 196900
    },
    {
      "epoch": 2.0111480899197582,
      "grad_norm": 14.075287818908691,
      "learning_rate": 6.608558709035254e-06,
      "loss": 2.5792,
      "step": 197000
    },
    {
      "epoch": 2.0121689772750475,
      "grad_norm": 11.557575225830078,
      "learning_rate": 6.6017411934742746e-06,
      "loss": 2.5345,
      "step": 197100
    },
    {
      "epoch": 2.013189864630337,
      "grad_norm": 13.236161231994629,
      "learning_rate": 6.594923677913295e-06,
      "loss": 2.5411,
      "step": 197200
    },
    {
      "epoch": 2.014210751985626,
      "grad_norm": 15.633476257324219,
      "learning_rate": 6.5881061623523155e-06,
      "loss": 2.5325,
      "step": 197300
    },
    {
      "epoch": 2.0152316393409153,
      "grad_norm": 13.614191055297852,
      "learning_rate": 6.581288646791338e-06,
      "loss": 2.5723,
      "step": 197400
    },
    {
      "epoch": 2.016252526696204,
      "grad_norm": 15.631356239318848,
      "learning_rate": 6.574471131230358e-06,
      "loss": 2.456,
      "step": 197500
    },
    {
      "epoch": 2.0172734140514934,
      "grad_norm": 14.538830757141113,
      "learning_rate": 6.567653615669379e-06,
      "loss": 2.5539,
      "step": 197600
    },
    {
      "epoch": 2.0182943014067827,
      "grad_norm": 21.56698989868164,
      "learning_rate": 6.560836100108399e-06,
      "loss": 2.4722,
      "step": 197700
    },
    {
      "epoch": 2.019315188762072,
      "grad_norm": 16.761734008789062,
      "learning_rate": 6.55401858454742e-06,
      "loss": 2.4931,
      "step": 197800
    },
    {
      "epoch": 2.020336076117361,
      "grad_norm": 20.08936882019043,
      "learning_rate": 6.547201068986441e-06,
      "loss": 2.6193,
      "step": 197900
    },
    {
      "epoch": 2.0213569634726505,
      "grad_norm": 20.701332092285156,
      "learning_rate": 6.540383553425461e-06,
      "loss": 2.5504,
      "step": 198000
    },
    {
      "epoch": 2.0223778508279397,
      "grad_norm": 15.669644355773926,
      "learning_rate": 6.533566037864482e-06,
      "loss": 2.59,
      "step": 198100
    },
    {
      "epoch": 2.023398738183229,
      "grad_norm": 15.315141677856445,
      "learning_rate": 6.526748522303503e-06,
      "loss": 2.4584,
      "step": 198200
    },
    {
      "epoch": 2.0244196255385183,
      "grad_norm": 13.761092185974121,
      "learning_rate": 6.5199310067425235e-06,
      "loss": 2.5583,
      "step": 198300
    },
    {
      "epoch": 2.025440512893807,
      "grad_norm": 14.225837707519531,
      "learning_rate": 6.513113491181544e-06,
      "loss": 2.5823,
      "step": 198400
    },
    {
      "epoch": 2.0264614002490964,
      "grad_norm": 15.501608848571777,
      "learning_rate": 6.5062959756205645e-06,
      "loss": 2.5092,
      "step": 198500
    },
    {
      "epoch": 2.0274822876043856,
      "grad_norm": 20.206905364990234,
      "learning_rate": 6.499478460059586e-06,
      "loss": 2.5081,
      "step": 198600
    },
    {
      "epoch": 2.028503174959675,
      "grad_norm": 17.31333351135254,
      "learning_rate": 6.492660944498606e-06,
      "loss": 2.5303,
      "step": 198700
    },
    {
      "epoch": 2.029524062314964,
      "grad_norm": 16.80562973022461,
      "learning_rate": 6.485843428937627e-06,
      "loss": 2.5448,
      "step": 198800
    },
    {
      "epoch": 2.0305449496702535,
      "grad_norm": 17.737625122070312,
      "learning_rate": 6.479025913376647e-06,
      "loss": 2.5101,
      "step": 198900
    },
    {
      "epoch": 2.0315658370255427,
      "grad_norm": 15.264314651489258,
      "learning_rate": 6.472208397815668e-06,
      "loss": 2.4722,
      "step": 199000
    },
    {
      "epoch": 2.032586724380832,
      "grad_norm": 16.137413024902344,
      "learning_rate": 6.46539088225469e-06,
      "loss": 2.5119,
      "step": 199100
    },
    {
      "epoch": 2.0336076117361213,
      "grad_norm": 13.902619361877441,
      "learning_rate": 6.45857336669371e-06,
      "loss": 2.5086,
      "step": 199200
    },
    {
      "epoch": 2.03462849909141,
      "grad_norm": 13.67958927154541,
      "learning_rate": 6.451755851132731e-06,
      "loss": 2.5449,
      "step": 199300
    },
    {
      "epoch": 2.0356493864466993,
      "grad_norm": 16.45733070373535,
      "learning_rate": 6.444938335571751e-06,
      "loss": 2.4446,
      "step": 199400
    },
    {
      "epoch": 2.0366702738019886,
      "grad_norm": 14.293648719787598,
      "learning_rate": 6.4381208200107725e-06,
      "loss": 2.5317,
      "step": 199500
    },
    {
      "epoch": 2.037691161157278,
      "grad_norm": 14.113121032714844,
      "learning_rate": 6.431303304449793e-06,
      "loss": 2.4973,
      "step": 199600
    },
    {
      "epoch": 2.038712048512567,
      "grad_norm": 17.110990524291992,
      "learning_rate": 6.4244857888888135e-06,
      "loss": 2.4523,
      "step": 199700
    },
    {
      "epoch": 2.0397329358678564,
      "grad_norm": 16.125423431396484,
      "learning_rate": 6.417668273327834e-06,
      "loss": 2.4961,
      "step": 199800
    },
    {
      "epoch": 2.0407538232231457,
      "grad_norm": 15.377408027648926,
      "learning_rate": 6.410918932922465e-06,
      "loss": 2.4833,
      "step": 199900
    },
    {
      "epoch": 2.041774710578435,
      "grad_norm": 14.43859577178955,
      "learning_rate": 6.4041014173614855e-06,
      "loss": 2.4783,
      "step": 200000
    },
    {
      "epoch": 2.042795597933724,
      "grad_norm": 12.975882530212402,
      "learning_rate": 6.397283901800506e-06,
      "loss": 2.5052,
      "step": 200100
    },
    {
      "epoch": 2.043816485289013,
      "grad_norm": 12.987286567687988,
      "learning_rate": 6.390466386239526e-06,
      "loss": 2.4567,
      "step": 200200
    },
    {
      "epoch": 2.0448373726443023,
      "grad_norm": 19.242116928100586,
      "learning_rate": 6.3836488706785486e-06,
      "loss": 2.5855,
      "step": 200300
    },
    {
      "epoch": 2.0458582599995916,
      "grad_norm": 12.504589080810547,
      "learning_rate": 6.376831355117569e-06,
      "loss": 2.512,
      "step": 200400
    },
    {
      "epoch": 2.046879147354881,
      "grad_norm": 18.61959457397461,
      "learning_rate": 6.370082014712199e-06,
      "loss": 2.5475,
      "step": 200500
    },
    {
      "epoch": 2.04790003471017,
      "grad_norm": 14.269651412963867,
      "learning_rate": 6.36326449915122e-06,
      "loss": 2.5329,
      "step": 200600
    },
    {
      "epoch": 2.0489209220654594,
      "grad_norm": 14.718923568725586,
      "learning_rate": 6.356446983590241e-06,
      "loss": 2.4861,
      "step": 200700
    },
    {
      "epoch": 2.0499418094207487,
      "grad_norm": 18.178905487060547,
      "learning_rate": 6.3496294680292615e-06,
      "loss": 2.5261,
      "step": 200800
    },
    {
      "epoch": 2.050962696776038,
      "grad_norm": 18.056238174438477,
      "learning_rate": 6.342811952468282e-06,
      "loss": 2.4591,
      "step": 200900
    },
    {
      "epoch": 2.0519835841313268,
      "grad_norm": 15.077943801879883,
      "learning_rate": 6.336062612062912e-06,
      "loss": 2.5322,
      "step": 201000
    },
    {
      "epoch": 2.053004471486616,
      "grad_norm": 15.994932174682617,
      "learning_rate": 6.3292450965019335e-06,
      "loss": 2.584,
      "step": 201100
    },
    {
      "epoch": 2.0540253588419053,
      "grad_norm": 14.27541446685791,
      "learning_rate": 6.322427580940954e-06,
      "loss": 2.5188,
      "step": 201200
    },
    {
      "epoch": 2.0550462461971946,
      "grad_norm": 16.964818954467773,
      "learning_rate": 6.3156100653799744e-06,
      "loss": 2.6296,
      "step": 201300
    },
    {
      "epoch": 2.056067133552484,
      "grad_norm": 15.603289604187012,
      "learning_rate": 6.308792549818995e-06,
      "loss": 2.5016,
      "step": 201400
    },
    {
      "epoch": 2.057088020907773,
      "grad_norm": 18.745309829711914,
      "learning_rate": 6.301975034258016e-06,
      "loss": 2.5716,
      "step": 201500
    },
    {
      "epoch": 2.0581089082630624,
      "grad_norm": 24.14266014099121,
      "learning_rate": 6.295157518697037e-06,
      "loss": 2.4824,
      "step": 201600
    },
    {
      "epoch": 2.0591297956183516,
      "grad_norm": 15.707295417785645,
      "learning_rate": 6.288340003136057e-06,
      "loss": 2.3999,
      "step": 201700
    },
    {
      "epoch": 2.060150682973641,
      "grad_norm": 18.712890625,
      "learning_rate": 6.281590662730687e-06,
      "loss": 2.5234,
      "step": 201800
    },
    {
      "epoch": 2.0611715703289297,
      "grad_norm": 15.952702522277832,
      "learning_rate": 6.2747731471697096e-06,
      "loss": 2.4759,
      "step": 201900
    },
    {
      "epoch": 2.062192457684219,
      "grad_norm": 16.565942764282227,
      "learning_rate": 6.26795563160873e-06,
      "loss": 2.4988,
      "step": 202000
    },
    {
      "epoch": 2.0632133450395083,
      "grad_norm": 16.396772384643555,
      "learning_rate": 6.2611381160477505e-06,
      "loss": 2.5136,
      "step": 202100
    },
    {
      "epoch": 2.0642342323947975,
      "grad_norm": 16.271209716796875,
      "learning_rate": 6.254320600486771e-06,
      "loss": 2.566,
      "step": 202200
    },
    {
      "epoch": 2.065255119750087,
      "grad_norm": 16.595535278320312,
      "learning_rate": 6.247503084925792e-06,
      "loss": 2.3923,
      "step": 202300
    },
    {
      "epoch": 2.066276007105376,
      "grad_norm": 14.458846092224121,
      "learning_rate": 6.240685569364813e-06,
      "loss": 2.5279,
      "step": 202400
    },
    {
      "epoch": 2.0672968944606653,
      "grad_norm": 21.015756607055664,
      "learning_rate": 6.233868053803833e-06,
      "loss": 2.3891,
      "step": 202500
    },
    {
      "epoch": 2.0683177818159546,
      "grad_norm": 14.782758712768555,
      "learning_rate": 6.227050538242854e-06,
      "loss": 2.5865,
      "step": 202600
    },
    {
      "epoch": 2.069338669171244,
      "grad_norm": 16.944969177246094,
      "learning_rate": 6.220233022681875e-06,
      "loss": 2.5425,
      "step": 202700
    },
    {
      "epoch": 2.0703595565265327,
      "grad_norm": 16.867040634155273,
      "learning_rate": 6.2134155071208954e-06,
      "loss": 2.5436,
      "step": 202800
    },
    {
      "epoch": 2.071380443881822,
      "grad_norm": 14.951821327209473,
      "learning_rate": 6.206597991559916e-06,
      "loss": 2.5114,
      "step": 202900
    },
    {
      "epoch": 2.0724013312371112,
      "grad_norm": 14.68949031829834,
      "learning_rate": 6.199780475998936e-06,
      "loss": 2.4686,
      "step": 203000
    },
    {
      "epoch": 2.0734222185924005,
      "grad_norm": 15.443853378295898,
      "learning_rate": 6.1929629604379585e-06,
      "loss": 2.4923,
      "step": 203100
    },
    {
      "epoch": 2.0744431059476898,
      "grad_norm": 18.204349517822266,
      "learning_rate": 6.186145444876979e-06,
      "loss": 2.4917,
      "step": 203200
    },
    {
      "epoch": 2.075463993302979,
      "grad_norm": 14.323716163635254,
      "learning_rate": 6.1793279293159995e-06,
      "loss": 2.4744,
      "step": 203300
    },
    {
      "epoch": 2.0764848806582683,
      "grad_norm": 13.1257905960083,
      "learning_rate": 6.17251041375502e-06,
      "loss": 2.4792,
      "step": 203400
    },
    {
      "epoch": 2.0775057680135576,
      "grad_norm": 12.944128036499023,
      "learning_rate": 6.165692898194041e-06,
      "loss": 2.5286,
      "step": 203500
    },
    {
      "epoch": 2.0785266553688464,
      "grad_norm": 17.002065658569336,
      "learning_rate": 6.158875382633062e-06,
      "loss": 2.533,
      "step": 203600
    },
    {
      "epoch": 2.0795475427241357,
      "grad_norm": 18.695880889892578,
      "learning_rate": 6.152057867072082e-06,
      "loss": 2.5278,
      "step": 203700
    },
    {
      "epoch": 2.080568430079425,
      "grad_norm": 11.293279647827148,
      "learning_rate": 6.145240351511103e-06,
      "loss": 2.5179,
      "step": 203800
    },
    {
      "epoch": 2.081589317434714,
      "grad_norm": 14.543193817138672,
      "learning_rate": 6.138422835950123e-06,
      "loss": 2.5071,
      "step": 203900
    },
    {
      "epoch": 2.0826102047900035,
      "grad_norm": 16.217241287231445,
      "learning_rate": 6.131605320389144e-06,
      "loss": 2.5129,
      "step": 204000
    },
    {
      "epoch": 2.0836310921452927,
      "grad_norm": 14.34235668182373,
      "learning_rate": 6.124787804828165e-06,
      "loss": 2.4537,
      "step": 204100
    },
    {
      "epoch": 2.084651979500582,
      "grad_norm": 19.46574592590332,
      "learning_rate": 6.117970289267185e-06,
      "loss": 2.6053,
      "step": 204200
    },
    {
      "epoch": 2.0856728668558713,
      "grad_norm": 14.684361457824707,
      "learning_rate": 6.111152773706206e-06,
      "loss": 2.4886,
      "step": 204300
    },
    {
      "epoch": 2.0866937542111605,
      "grad_norm": 15.270774841308594,
      "learning_rate": 6.104335258145227e-06,
      "loss": 2.5345,
      "step": 204400
    },
    {
      "epoch": 2.0877146415664494,
      "grad_norm": 19.5907039642334,
      "learning_rate": 6.097517742584248e-06,
      "loss": 2.5407,
      "step": 204500
    },
    {
      "epoch": 2.0887355289217386,
      "grad_norm": 17.316356658935547,
      "learning_rate": 6.090700227023268e-06,
      "loss": 2.511,
      "step": 204600
    },
    {
      "epoch": 2.089756416277028,
      "grad_norm": 21.023822784423828,
      "learning_rate": 6.0838827114622885e-06,
      "loss": 2.5234,
      "step": 204700
    },
    {
      "epoch": 2.090777303632317,
      "grad_norm": 14.364425659179688,
      "learning_rate": 6.077065195901311e-06,
      "loss": 2.5078,
      "step": 204800
    },
    {
      "epoch": 2.0917981909876064,
      "grad_norm": 14.208032608032227,
      "learning_rate": 6.070247680340331e-06,
      "loss": 2.5323,
      "step": 204900
    },
    {
      "epoch": 2.0928190783428957,
      "grad_norm": 16.286251068115234,
      "learning_rate": 6.063430164779352e-06,
      "loss": 2.4617,
      "step": 205000
    },
    {
      "epoch": 2.093839965698185,
      "grad_norm": 15.675570487976074,
      "learning_rate": 6.056612649218372e-06,
      "loss": 2.5183,
      "step": 205100
    },
    {
      "epoch": 2.0948608530534742,
      "grad_norm": 17.69516944885254,
      "learning_rate": 6.049795133657393e-06,
      "loss": 2.5176,
      "step": 205200
    },
    {
      "epoch": 2.095881740408763,
      "grad_norm": 14.056341171264648,
      "learning_rate": 6.042977618096414e-06,
      "loss": 2.4867,
      "step": 205300
    },
    {
      "epoch": 2.0969026277640523,
      "grad_norm": 20.619863510131836,
      "learning_rate": 6.036160102535434e-06,
      "loss": 2.4923,
      "step": 205400
    },
    {
      "epoch": 2.0979235151193416,
      "grad_norm": 13.151215553283691,
      "learning_rate": 6.029342586974455e-06,
      "loss": 2.471,
      "step": 205500
    },
    {
      "epoch": 2.098944402474631,
      "grad_norm": 15.985443115234375,
      "learning_rate": 6.022525071413476e-06,
      "loss": 2.5291,
      "step": 205600
    },
    {
      "epoch": 2.09996528982992,
      "grad_norm": 16.42943000793457,
      "learning_rate": 6.0157075558524966e-06,
      "loss": 2.5031,
      "step": 205700
    },
    {
      "epoch": 2.1009861771852094,
      "grad_norm": 14.283811569213867,
      "learning_rate": 6.008890040291517e-06,
      "loss": 2.5451,
      "step": 205800
    },
    {
      "epoch": 2.1020070645404987,
      "grad_norm": 14.759937286376953,
      "learning_rate": 6.0020725247305375e-06,
      "loss": 2.4086,
      "step": 205900
    },
    {
      "epoch": 2.103027951895788,
      "grad_norm": 16.577198028564453,
      "learning_rate": 5.99525500916956e-06,
      "loss": 2.4199,
      "step": 206000
    },
    {
      "epoch": 2.104048839251077,
      "grad_norm": 14.475464820861816,
      "learning_rate": 5.98843749360858e-06,
      "loss": 2.5226,
      "step": 206100
    },
    {
      "epoch": 2.105069726606366,
      "grad_norm": 17.506860733032227,
      "learning_rate": 5.981619978047601e-06,
      "loss": 2.5133,
      "step": 206200
    },
    {
      "epoch": 2.1060906139616553,
      "grad_norm": 14.584193229675293,
      "learning_rate": 5.974802462486621e-06,
      "loss": 2.4934,
      "step": 206300
    },
    {
      "epoch": 2.1071115013169446,
      "grad_norm": 13.09087085723877,
      "learning_rate": 5.967984946925642e-06,
      "loss": 2.4851,
      "step": 206400
    },
    {
      "epoch": 2.108132388672234,
      "grad_norm": 16.098413467407227,
      "learning_rate": 5.961167431364663e-06,
      "loss": 2.4552,
      "step": 206500
    },
    {
      "epoch": 2.109153276027523,
      "grad_norm": 15.125092506408691,
      "learning_rate": 5.954349915803683e-06,
      "loss": 2.3979,
      "step": 206600
    },
    {
      "epoch": 2.1101741633828124,
      "grad_norm": 12.869117736816406,
      "learning_rate": 5.947532400242704e-06,
      "loss": 2.5045,
      "step": 206700
    },
    {
      "epoch": 2.1111950507381017,
      "grad_norm": 13.928237915039062,
      "learning_rate": 5.940714884681725e-06,
      "loss": 2.4617,
      "step": 206800
    },
    {
      "epoch": 2.112215938093391,
      "grad_norm": 13.224834442138672,
      "learning_rate": 5.9338973691207455e-06,
      "loss": 2.5282,
      "step": 206900
    },
    {
      "epoch": 2.11323682544868,
      "grad_norm": 18.007652282714844,
      "learning_rate": 5.927148028715376e-06,
      "loss": 2.5111,
      "step": 207000
    },
    {
      "epoch": 2.114257712803969,
      "grad_norm": 14.76274299621582,
      "learning_rate": 5.920330513154396e-06,
      "loss": 2.4767,
      "step": 207100
    },
    {
      "epoch": 2.1152786001592583,
      "grad_norm": 15.640830039978027,
      "learning_rate": 5.913512997593418e-06,
      "loss": 2.5396,
      "step": 207200
    },
    {
      "epoch": 2.1162994875145476,
      "grad_norm": 19.197450637817383,
      "learning_rate": 5.906695482032439e-06,
      "loss": 2.5592,
      "step": 207300
    },
    {
      "epoch": 2.117320374869837,
      "grad_norm": 16.46554946899414,
      "learning_rate": 5.899877966471459e-06,
      "loss": 2.4705,
      "step": 207400
    },
    {
      "epoch": 2.118341262225126,
      "grad_norm": 14.04850959777832,
      "learning_rate": 5.89306045091048e-06,
      "loss": 2.5268,
      "step": 207500
    },
    {
      "epoch": 2.1193621495804154,
      "grad_norm": 21.136003494262695,
      "learning_rate": 5.8862429353495e-06,
      "loss": 2.5666,
      "step": 207600
    },
    {
      "epoch": 2.1203830369357046,
      "grad_norm": 16.8293399810791,
      "learning_rate": 5.879425419788522e-06,
      "loss": 2.474,
      "step": 207700
    },
    {
      "epoch": 2.121403924290994,
      "grad_norm": 19.309293746948242,
      "learning_rate": 5.872607904227542e-06,
      "loss": 2.4541,
      "step": 207800
    },
    {
      "epoch": 2.122424811646283,
      "grad_norm": 17.89139747619629,
      "learning_rate": 5.8657903886665625e-06,
      "loss": 2.397,
      "step": 207900
    },
    {
      "epoch": 2.123445699001572,
      "grad_norm": 14.438584327697754,
      "learning_rate": 5.858972873105583e-06,
      "loss": 2.4817,
      "step": 208000
    },
    {
      "epoch": 2.1244665863568613,
      "grad_norm": 22.09333610534668,
      "learning_rate": 5.852155357544604e-06,
      "loss": 2.5413,
      "step": 208100
    },
    {
      "epoch": 2.1254874737121505,
      "grad_norm": 15.763721466064453,
      "learning_rate": 5.845337841983625e-06,
      "loss": 2.5502,
      "step": 208200
    },
    {
      "epoch": 2.12650836106744,
      "grad_norm": 16.769641876220703,
      "learning_rate": 5.838520326422645e-06,
      "loss": 2.551,
      "step": 208300
    },
    {
      "epoch": 2.127529248422729,
      "grad_norm": 13.797613143920898,
      "learning_rate": 5.831702810861666e-06,
      "loss": 2.4997,
      "step": 208400
    },
    {
      "epoch": 2.1285501357780183,
      "grad_norm": 15.216472625732422,
      "learning_rate": 5.824885295300687e-06,
      "loss": 2.4793,
      "step": 208500
    },
    {
      "epoch": 2.1295710231333076,
      "grad_norm": 16.655406951904297,
      "learning_rate": 5.8180677797397075e-06,
      "loss": 2.499,
      "step": 208600
    },
    {
      "epoch": 2.130591910488597,
      "grad_norm": 14.36929988861084,
      "learning_rate": 5.811250264178728e-06,
      "loss": 2.4762,
      "step": 208700
    },
    {
      "epoch": 2.1316127978438857,
      "grad_norm": 12.183028221130371,
      "learning_rate": 5.804432748617748e-06,
      "loss": 2.5056,
      "step": 208800
    },
    {
      "epoch": 2.132633685199175,
      "grad_norm": 14.645825386047363,
      "learning_rate": 5.7976152330567706e-06,
      "loss": 2.5012,
      "step": 208900
    },
    {
      "epoch": 2.1336545725544642,
      "grad_norm": 15.40334701538086,
      "learning_rate": 5.790797717495791e-06,
      "loss": 2.538,
      "step": 209000
    },
    {
      "epoch": 2.1346754599097535,
      "grad_norm": 17.5982608795166,
      "learning_rate": 5.7839802019348115e-06,
      "loss": 2.4349,
      "step": 209100
    },
    {
      "epoch": 2.1356963472650428,
      "grad_norm": 13.28230094909668,
      "learning_rate": 5.777162686373832e-06,
      "loss": 2.534,
      "step": 209200
    },
    {
      "epoch": 2.136717234620332,
      "grad_norm": 17.319408416748047,
      "learning_rate": 5.770345170812853e-06,
      "loss": 2.4747,
      "step": 209300
    },
    {
      "epoch": 2.1377381219756213,
      "grad_norm": 17.688034057617188,
      "learning_rate": 5.763527655251874e-06,
      "loss": 2.3976,
      "step": 209400
    },
    {
      "epoch": 2.1387590093309106,
      "grad_norm": 15.365653991699219,
      "learning_rate": 5.756710139690894e-06,
      "loss": 2.5412,
      "step": 209500
    },
    {
      "epoch": 2.1397798966862,
      "grad_norm": 26.400869369506836,
      "learning_rate": 5.749892624129915e-06,
      "loss": 2.4316,
      "step": 209600
    },
    {
      "epoch": 2.1408007840414887,
      "grad_norm": 15.765982627868652,
      "learning_rate": 5.743075108568936e-06,
      "loss": 2.5242,
      "step": 209700
    },
    {
      "epoch": 2.141821671396778,
      "grad_norm": 18.97137451171875,
      "learning_rate": 5.7362575930079564e-06,
      "loss": 2.4457,
      "step": 209800
    },
    {
      "epoch": 2.142842558752067,
      "grad_norm": 15.652060508728027,
      "learning_rate": 5.729440077446977e-06,
      "loss": 2.4124,
      "step": 209900
    },
    {
      "epoch": 2.1438634461073565,
      "grad_norm": 15.154318809509277,
      "learning_rate": 5.722622561885997e-06,
      "loss": 2.4283,
      "step": 210000
    },
    {
      "epoch": 2.1448843334626457,
      "grad_norm": 18.75728416442871,
      "learning_rate": 5.7158050463250195e-06,
      "loss": 2.4717,
      "step": 210100
    },
    {
      "epoch": 2.145905220817935,
      "grad_norm": 14.658828735351562,
      "learning_rate": 5.70898753076404e-06,
      "loss": 2.5033,
      "step": 210200
    },
    {
      "epoch": 2.1469261081732243,
      "grad_norm": 15.574875831604004,
      "learning_rate": 5.7021700152030605e-06,
      "loss": 2.4527,
      "step": 210300
    },
    {
      "epoch": 2.1479469955285135,
      "grad_norm": 13.855047225952148,
      "learning_rate": 5.695352499642081e-06,
      "loss": 2.4406,
      "step": 210400
    },
    {
      "epoch": 2.148967882883803,
      "grad_norm": 20.46540069580078,
      "learning_rate": 5.688534984081102e-06,
      "loss": 2.5125,
      "step": 210500
    },
    {
      "epoch": 2.1499887702390916,
      "grad_norm": 19.708227157592773,
      "learning_rate": 5.681717468520123e-06,
      "loss": 2.5545,
      "step": 210600
    },
    {
      "epoch": 2.151009657594381,
      "grad_norm": 15.47181224822998,
      "learning_rate": 5.674899952959143e-06,
      "loss": 2.5215,
      "step": 210700
    },
    {
      "epoch": 2.15203054494967,
      "grad_norm": 14.773242950439453,
      "learning_rate": 5.668082437398164e-06,
      "loss": 2.5945,
      "step": 210800
    },
    {
      "epoch": 2.1530514323049594,
      "grad_norm": 16.031545639038086,
      "learning_rate": 5.661264921837184e-06,
      "loss": 2.4515,
      "step": 210900
    },
    {
      "epoch": 2.1540723196602487,
      "grad_norm": 15.752330780029297,
      "learning_rate": 5.654447406276205e-06,
      "loss": 2.4829,
      "step": 211000
    },
    {
      "epoch": 2.155093207015538,
      "grad_norm": 16.501073837280273,
      "learning_rate": 5.647629890715226e-06,
      "loss": 2.5412,
      "step": 211100
    },
    {
      "epoch": 2.1561140943708272,
      "grad_norm": 16.897573471069336,
      "learning_rate": 5.640812375154246e-06,
      "loss": 2.487,
      "step": 211200
    },
    {
      "epoch": 2.1571349817261165,
      "grad_norm": 13.929792404174805,
      "learning_rate": 5.633994859593267e-06,
      "loss": 2.5066,
      "step": 211300
    },
    {
      "epoch": 2.1581558690814058,
      "grad_norm": 14.909573554992676,
      "learning_rate": 5.627177344032289e-06,
      "loss": 2.6189,
      "step": 211400
    },
    {
      "epoch": 2.1591767564366946,
      "grad_norm": 18.089929580688477,
      "learning_rate": 5.620359828471309e-06,
      "loss": 2.5569,
      "step": 211500
    },
    {
      "epoch": 2.160197643791984,
      "grad_norm": 13.50747299194336,
      "learning_rate": 5.613542312910329e-06,
      "loss": 2.5184,
      "step": 211600
    },
    {
      "epoch": 2.161218531147273,
      "grad_norm": 17.59784698486328,
      "learning_rate": 5.6067247973493495e-06,
      "loss": 2.5487,
      "step": 211700
    },
    {
      "epoch": 2.1622394185025624,
      "grad_norm": 15.632375717163086,
      "learning_rate": 5.5999754569439814e-06,
      "loss": 2.6008,
      "step": 211800
    },
    {
      "epoch": 2.1632603058578517,
      "grad_norm": 13.25400161743164,
      "learning_rate": 5.593157941383002e-06,
      "loss": 2.5756,
      "step": 211900
    },
    {
      "epoch": 2.164281193213141,
      "grad_norm": 17.9804630279541,
      "learning_rate": 5.586340425822022e-06,
      "loss": 2.5323,
      "step": 212000
    },
    {
      "epoch": 2.16530208056843,
      "grad_norm": 16.154451370239258,
      "learning_rate": 5.579522910261043e-06,
      "loss": 2.4887,
      "step": 212100
    },
    {
      "epoch": 2.1663229679237195,
      "grad_norm": 16.9695987701416,
      "learning_rate": 5.572705394700064e-06,
      "loss": 2.5324,
      "step": 212200
    },
    {
      "epoch": 2.1673438552790083,
      "grad_norm": 17.81056785583496,
      "learning_rate": 5.565887879139085e-06,
      "loss": 2.4873,
      "step": 212300
    },
    {
      "epoch": 2.1683647426342976,
      "grad_norm": 21.99461555480957,
      "learning_rate": 5.559070363578105e-06,
      "loss": 2.5005,
      "step": 212400
    },
    {
      "epoch": 2.169385629989587,
      "grad_norm": 18.356061935424805,
      "learning_rate": 5.5522528480171256e-06,
      "loss": 2.4276,
      "step": 212500
    },
    {
      "epoch": 2.170406517344876,
      "grad_norm": 20.630390167236328,
      "learning_rate": 5.545435332456147e-06,
      "loss": 2.4959,
      "step": 212600
    },
    {
      "epoch": 2.1714274047001654,
      "grad_norm": 17.255416870117188,
      "learning_rate": 5.538617816895167e-06,
      "loss": 2.5432,
      "step": 212700
    },
    {
      "epoch": 2.1724482920554546,
      "grad_norm": 13.258979797363281,
      "learning_rate": 5.531800301334188e-06,
      "loss": 2.4788,
      "step": 212800
    },
    {
      "epoch": 2.173469179410744,
      "grad_norm": 17.524494171142578,
      "learning_rate": 5.524982785773208e-06,
      "loss": 2.4709,
      "step": 212900
    },
    {
      "epoch": 2.174490066766033,
      "grad_norm": 15.062793731689453,
      "learning_rate": 5.5181652702122304e-06,
      "loss": 2.4911,
      "step": 213000
    },
    {
      "epoch": 2.1755109541213224,
      "grad_norm": 14.690547943115234,
      "learning_rate": 5.511347754651251e-06,
      "loss": 2.5228,
      "step": 213100
    },
    {
      "epoch": 2.1765318414766113,
      "grad_norm": 29.747026443481445,
      "learning_rate": 5.504530239090271e-06,
      "loss": 2.5627,
      "step": 213200
    },
    {
      "epoch": 2.1775527288319005,
      "grad_norm": 21.169185638427734,
      "learning_rate": 5.497712723529292e-06,
      "loss": 2.5332,
      "step": 213300
    },
    {
      "epoch": 2.17857361618719,
      "grad_norm": 13.48088550567627,
      "learning_rate": 5.490895207968313e-06,
      "loss": 2.4558,
      "step": 213400
    },
    {
      "epoch": 2.179594503542479,
      "grad_norm": 16.307157516479492,
      "learning_rate": 5.484145867562943e-06,
      "loss": 2.4445,
      "step": 213500
    },
    {
      "epoch": 2.1806153908977683,
      "grad_norm": 15.548273086547852,
      "learning_rate": 5.477328352001964e-06,
      "loss": 2.4621,
      "step": 213600
    },
    {
      "epoch": 2.1816362782530576,
      "grad_norm": 17.605243682861328,
      "learning_rate": 5.470510836440984e-06,
      "loss": 2.4405,
      "step": 213700
    },
    {
      "epoch": 2.182657165608347,
      "grad_norm": 15.300552368164062,
      "learning_rate": 5.463693320880006e-06,
      "loss": 2.4583,
      "step": 213800
    },
    {
      "epoch": 2.183678052963636,
      "grad_norm": 17.140016555786133,
      "learning_rate": 5.456875805319026e-06,
      "loss": 2.4611,
      "step": 213900
    },
    {
      "epoch": 2.184698940318925,
      "grad_norm": 18.220870971679688,
      "learning_rate": 5.4500582897580465e-06,
      "loss": 2.5106,
      "step": 214000
    },
    {
      "epoch": 2.1857198276742142,
      "grad_norm": 13.573358535766602,
      "learning_rate": 5.443240774197067e-06,
      "loss": 2.4979,
      "step": 214100
    },
    {
      "epoch": 2.1867407150295035,
      "grad_norm": 14.39554214477539,
      "learning_rate": 5.436423258636089e-06,
      "loss": 2.4656,
      "step": 214200
    },
    {
      "epoch": 2.187761602384793,
      "grad_norm": 20.343183517456055,
      "learning_rate": 5.42960574307511e-06,
      "loss": 2.4762,
      "step": 214300
    },
    {
      "epoch": 2.188782489740082,
      "grad_norm": 17.202749252319336,
      "learning_rate": 5.42278822751413e-06,
      "loss": 2.4667,
      "step": 214400
    },
    {
      "epoch": 2.1898033770953713,
      "grad_norm": 17.51992416381836,
      "learning_rate": 5.4159707119531506e-06,
      "loss": 2.5323,
      "step": 214500
    },
    {
      "epoch": 2.1908242644506606,
      "grad_norm": 14.52529525756836,
      "learning_rate": 5.409153196392172e-06,
      "loss": 2.5319,
      "step": 214600
    },
    {
      "epoch": 2.19184515180595,
      "grad_norm": 16.04582977294922,
      "learning_rate": 5.402335680831192e-06,
      "loss": 2.4451,
      "step": 214700
    },
    {
      "epoch": 2.192866039161239,
      "grad_norm": 19.2779598236084,
      "learning_rate": 5.395518165270213e-06,
      "loss": 2.4134,
      "step": 214800
    },
    {
      "epoch": 2.1938869265165284,
      "grad_norm": 16.576631546020508,
      "learning_rate": 5.388700649709233e-06,
      "loss": 2.5229,
      "step": 214900
    },
    {
      "epoch": 2.194907813871817,
      "grad_norm": 18.882740020751953,
      "learning_rate": 5.381883134148254e-06,
      "loss": 2.5019,
      "step": 215000
    },
    {
      "epoch": 2.1959287012271065,
      "grad_norm": 16.908098220825195,
      "learning_rate": 5.375065618587275e-06,
      "loss": 2.4962,
      "step": 215100
    },
    {
      "epoch": 2.1969495885823958,
      "grad_norm": 16.826459884643555,
      "learning_rate": 5.3682481030262955e-06,
      "loss": 2.4301,
      "step": 215200
    },
    {
      "epoch": 2.197970475937685,
      "grad_norm": 19.710275650024414,
      "learning_rate": 5.361430587465316e-06,
      "loss": 2.5436,
      "step": 215300
    },
    {
      "epoch": 2.1989913632929743,
      "grad_norm": 17.645322799682617,
      "learning_rate": 5.3546130719043365e-06,
      "loss": 2.4504,
      "step": 215400
    },
    {
      "epoch": 2.2000122506482636,
      "grad_norm": 16.072492599487305,
      "learning_rate": 5.347795556343358e-06,
      "loss": 2.5961,
      "step": 215500
    },
    {
      "epoch": 2.201033138003553,
      "grad_norm": 13.65532398223877,
      "learning_rate": 5.340978040782378e-06,
      "loss": 2.5168,
      "step": 215600
    },
    {
      "epoch": 2.202054025358842,
      "grad_norm": 16.89379119873047,
      "learning_rate": 5.334160525221399e-06,
      "loss": 2.5317,
      "step": 215700
    },
    {
      "epoch": 2.203074912714131,
      "grad_norm": 12.281950950622559,
      "learning_rate": 5.327343009660419e-06,
      "loss": 2.5063,
      "step": 215800
    },
    {
      "epoch": 2.20409580006942,
      "grad_norm": 18.070552825927734,
      "learning_rate": 5.320525494099441e-06,
      "loss": 2.5545,
      "step": 215900
    },
    {
      "epoch": 2.2051166874247095,
      "grad_norm": 16.726219177246094,
      "learning_rate": 5.3137761536940716e-06,
      "loss": 2.4913,
      "step": 216000
    },
    {
      "epoch": 2.2061375747799987,
      "grad_norm": 17.319110870361328,
      "learning_rate": 5.306958638133092e-06,
      "loss": 2.498,
      "step": 216100
    },
    {
      "epoch": 2.207158462135288,
      "grad_norm": 15.514450073242188,
      "learning_rate": 5.3001411225721125e-06,
      "loss": 2.4023,
      "step": 216200
    },
    {
      "epoch": 2.2081793494905773,
      "grad_norm": 15.729303359985352,
      "learning_rate": 5.293323607011134e-06,
      "loss": 2.4827,
      "step": 216300
    },
    {
      "epoch": 2.2092002368458665,
      "grad_norm": 13.851116180419922,
      "learning_rate": 5.286506091450154e-06,
      "loss": 2.3941,
      "step": 216400
    },
    {
      "epoch": 2.210221124201156,
      "grad_norm": 17.580371856689453,
      "learning_rate": 5.279688575889175e-06,
      "loss": 2.484,
      "step": 216500
    },
    {
      "epoch": 2.211242011556445,
      "grad_norm": 17.95086669921875,
      "learning_rate": 5.272871060328195e-06,
      "loss": 2.4864,
      "step": 216600
    },
    {
      "epoch": 2.212262898911734,
      "grad_norm": 15.73630142211914,
      "learning_rate": 5.2660535447672165e-06,
      "loss": 2.4016,
      "step": 216700
    },
    {
      "epoch": 2.213283786267023,
      "grad_norm": 21.188478469848633,
      "learning_rate": 5.259236029206237e-06,
      "loss": 2.4295,
      "step": 216800
    },
    {
      "epoch": 2.2143046736223124,
      "grad_norm": 18.519197463989258,
      "learning_rate": 5.2524185136452574e-06,
      "loss": 2.563,
      "step": 216900
    },
    {
      "epoch": 2.2153255609776017,
      "grad_norm": 11.920157432556152,
      "learning_rate": 5.245600998084278e-06,
      "loss": 2.5062,
      "step": 217000
    },
    {
      "epoch": 2.216346448332891,
      "grad_norm": 12.865155220031738,
      "learning_rate": 5.2387834825233e-06,
      "loss": 2.4693,
      "step": 217100
    },
    {
      "epoch": 2.2173673356881802,
      "grad_norm": 14.507832527160645,
      "learning_rate": 5.2319659669623205e-06,
      "loss": 2.4658,
      "step": 217200
    },
    {
      "epoch": 2.2183882230434695,
      "grad_norm": 18.155960083007812,
      "learning_rate": 5.225148451401341e-06,
      "loss": 2.4703,
      "step": 217300
    },
    {
      "epoch": 2.2194091103987588,
      "grad_norm": 16.687442779541016,
      "learning_rate": 5.2183309358403615e-06,
      "loss": 2.3973,
      "step": 217400
    },
    {
      "epoch": 2.2204299977540476,
      "grad_norm": 18.370576858520508,
      "learning_rate": 5.211513420279383e-06,
      "loss": 2.4989,
      "step": 217500
    },
    {
      "epoch": 2.221450885109337,
      "grad_norm": 14.792513847351074,
      "learning_rate": 5.204695904718403e-06,
      "loss": 2.5075,
      "step": 217600
    },
    {
      "epoch": 2.222471772464626,
      "grad_norm": 15.28012466430664,
      "learning_rate": 5.197878389157424e-06,
      "loss": 2.4987,
      "step": 217700
    },
    {
      "epoch": 2.2234926598199154,
      "grad_norm": 14.417951583862305,
      "learning_rate": 5.191060873596444e-06,
      "loss": 2.4479,
      "step": 217800
    },
    {
      "epoch": 2.2245135471752047,
      "grad_norm": 19.105892181396484,
      "learning_rate": 5.1842433580354655e-06,
      "loss": 2.4597,
      "step": 217900
    },
    {
      "epoch": 2.225534434530494,
      "grad_norm": 17.49308204650879,
      "learning_rate": 5.177425842474486e-06,
      "loss": 2.4334,
      "step": 218000
    },
    {
      "epoch": 2.226555321885783,
      "grad_norm": 20.713550567626953,
      "learning_rate": 5.170608326913506e-06,
      "loss": 2.4651,
      "step": 218100
    },
    {
      "epoch": 2.2275762092410725,
      "grad_norm": 20.269460678100586,
      "learning_rate": 5.163790811352527e-06,
      "loss": 2.4202,
      "step": 218200
    },
    {
      "epoch": 2.2285970965963617,
      "grad_norm": 18.547393798828125,
      "learning_rate": 5.156973295791548e-06,
      "loss": 2.439,
      "step": 218300
    },
    {
      "epoch": 2.229617983951651,
      "grad_norm": 13.509264945983887,
      "learning_rate": 5.150155780230569e-06,
      "loss": 2.4733,
      "step": 218400
    },
    {
      "epoch": 2.23063887130694,
      "grad_norm": 13.130724906921387,
      "learning_rate": 5.143338264669589e-06,
      "loss": 2.4358,
      "step": 218500
    },
    {
      "epoch": 2.231659758662229,
      "grad_norm": 18.686262130737305,
      "learning_rate": 5.13652074910861e-06,
      "loss": 2.5201,
      "step": 218600
    },
    {
      "epoch": 2.2326806460175184,
      "grad_norm": 16.0650634765625,
      "learning_rate": 5.12970323354763e-06,
      "loss": 2.4334,
      "step": 218700
    },
    {
      "epoch": 2.2337015333728076,
      "grad_norm": 14.15012264251709,
      "learning_rate": 5.122885717986652e-06,
      "loss": 2.5663,
      "step": 218800
    },
    {
      "epoch": 2.234722420728097,
      "grad_norm": 18.195960998535156,
      "learning_rate": 5.116068202425673e-06,
      "loss": 2.531,
      "step": 218900
    },
    {
      "epoch": 2.235743308083386,
      "grad_norm": 16.163799285888672,
      "learning_rate": 5.109250686864693e-06,
      "loss": 2.5024,
      "step": 219000
    },
    {
      "epoch": 2.2367641954386754,
      "grad_norm": 21.02232551574707,
      "learning_rate": 5.102433171303714e-06,
      "loss": 2.5298,
      "step": 219100
    },
    {
      "epoch": 2.2377850827939647,
      "grad_norm": 13.755509376525879,
      "learning_rate": 5.095615655742735e-06,
      "loss": 2.5127,
      "step": 219200
    },
    {
      "epoch": 2.2388059701492535,
      "grad_norm": 13.376769065856934,
      "learning_rate": 5.088798140181755e-06,
      "loss": 2.4612,
      "step": 219300
    },
    {
      "epoch": 2.239826857504543,
      "grad_norm": 22.551807403564453,
      "learning_rate": 5.081980624620776e-06,
      "loss": 2.5432,
      "step": 219400
    },
    {
      "epoch": 2.240847744859832,
      "grad_norm": 17.005796432495117,
      "learning_rate": 5.075163109059796e-06,
      "loss": 2.4819,
      "step": 219500
    },
    {
      "epoch": 2.2418686322151213,
      "grad_norm": 15.04010009765625,
      "learning_rate": 5.068345593498818e-06,
      "loss": 2.5186,
      "step": 219600
    },
    {
      "epoch": 2.2428895195704106,
      "grad_norm": 13.063486099243164,
      "learning_rate": 5.061528077937838e-06,
      "loss": 2.4496,
      "step": 219700
    },
    {
      "epoch": 2.2439104069257,
      "grad_norm": 16.540103912353516,
      "learning_rate": 5.0547105623768586e-06,
      "loss": 2.5421,
      "step": 219800
    },
    {
      "epoch": 2.244931294280989,
      "grad_norm": 18.448633193969727,
      "learning_rate": 5.047893046815879e-06,
      "loss": 2.606,
      "step": 219900
    },
    {
      "epoch": 2.2459521816362784,
      "grad_norm": 16.27255630493164,
      "learning_rate": 5.041075531254901e-06,
      "loss": 2.5401,
      "step": 220000
    },
    {
      "epoch": 2.2469730689915677,
      "grad_norm": 22.30028533935547,
      "learning_rate": 5.034258015693922e-06,
      "loss": 2.486,
      "step": 220100
    },
    {
      "epoch": 2.2479939563468565,
      "grad_norm": 19.098012924194336,
      "learning_rate": 5.027508675288552e-06,
      "loss": 2.4429,
      "step": 220200
    },
    {
      "epoch": 2.2490148437021458,
      "grad_norm": 16.27407455444336,
      "learning_rate": 5.020691159727572e-06,
      "loss": 2.5282,
      "step": 220300
    },
    {
      "epoch": 2.250035731057435,
      "grad_norm": 18.677146911621094,
      "learning_rate": 5.013873644166594e-06,
      "loss": 2.4532,
      "step": 220400
    },
    {
      "epoch": 2.2510566184127243,
      "grad_norm": 16.094337463378906,
      "learning_rate": 5.007056128605614e-06,
      "loss": 2.507,
      "step": 220500
    },
    {
      "epoch": 2.2520775057680136,
      "grad_norm": 16.001976013183594,
      "learning_rate": 5.000238613044635e-06,
      "loss": 2.4783,
      "step": 220600
    },
    {
      "epoch": 2.253098393123303,
      "grad_norm": 13.147266387939453,
      "learning_rate": 4.993421097483656e-06,
      "loss": 2.5609,
      "step": 220700
    },
    {
      "epoch": 2.254119280478592,
      "grad_norm": 13.667777061462402,
      "learning_rate": 4.986603581922676e-06,
      "loss": 2.5145,
      "step": 220800
    },
    {
      "epoch": 2.2551401678338814,
      "grad_norm": 16.30178451538086,
      "learning_rate": 4.979786066361697e-06,
      "loss": 2.5434,
      "step": 220900
    },
    {
      "epoch": 2.25616105518917,
      "grad_norm": 17.343433380126953,
      "learning_rate": 4.972968550800717e-06,
      "loss": 2.4918,
      "step": 221000
    },
    {
      "epoch": 2.2571819425444595,
      "grad_norm": 15.319948196411133,
      "learning_rate": 4.966151035239739e-06,
      "loss": 2.5005,
      "step": 221100
    },
    {
      "epoch": 2.2582028298997487,
      "grad_norm": 17.52442169189453,
      "learning_rate": 4.959333519678759e-06,
      "loss": 2.4318,
      "step": 221200
    },
    {
      "epoch": 2.259223717255038,
      "grad_norm": 13.787431716918945,
      "learning_rate": 4.95251600411778e-06,
      "loss": 2.4989,
      "step": 221300
    },
    {
      "epoch": 2.2602446046103273,
      "grad_norm": 15.436279296875,
      "learning_rate": 4.945698488556801e-06,
      "loss": 2.4867,
      "step": 221400
    },
    {
      "epoch": 2.2612654919656165,
      "grad_norm": 17.809255599975586,
      "learning_rate": 4.938880972995821e-06,
      "loss": 2.5664,
      "step": 221500
    },
    {
      "epoch": 2.262286379320906,
      "grad_norm": 15.173337936401367,
      "learning_rate": 4.932063457434842e-06,
      "loss": 2.4393,
      "step": 221600
    },
    {
      "epoch": 2.263307266676195,
      "grad_norm": 15.765691757202148,
      "learning_rate": 4.925245941873863e-06,
      "loss": 2.4343,
      "step": 221700
    },
    {
      "epoch": 2.2643281540314844,
      "grad_norm": 16.6348876953125,
      "learning_rate": 4.918428426312884e-06,
      "loss": 2.4163,
      "step": 221800
    },
    {
      "epoch": 2.2653490413867736,
      "grad_norm": 15.0311861038208,
      "learning_rate": 4.911610910751904e-06,
      "loss": 2.5477,
      "step": 221900
    },
    {
      "epoch": 2.2663699287420624,
      "grad_norm": 13.967707633972168,
      "learning_rate": 4.9047933951909245e-06,
      "loss": 2.5219,
      "step": 222000
    },
    {
      "epoch": 2.2673908160973517,
      "grad_norm": 18.088603973388672,
      "learning_rate": 4.897975879629946e-06,
      "loss": 2.5065,
      "step": 222100
    },
    {
      "epoch": 2.268411703452641,
      "grad_norm": 16.982152938842773,
      "learning_rate": 4.891158364068966e-06,
      "loss": 2.5652,
      "step": 222200
    },
    {
      "epoch": 2.2694325908079303,
      "grad_norm": 15.357585906982422,
      "learning_rate": 4.884340848507987e-06,
      "loss": 2.5309,
      "step": 222300
    },
    {
      "epoch": 2.2704534781632195,
      "grad_norm": 19.480918884277344,
      "learning_rate": 4.877523332947008e-06,
      "loss": 2.4647,
      "step": 222400
    },
    {
      "epoch": 2.271474365518509,
      "grad_norm": 17.088674545288086,
      "learning_rate": 4.8707058173860285e-06,
      "loss": 2.4742,
      "step": 222500
    },
    {
      "epoch": 2.272495252873798,
      "grad_norm": 16.471145629882812,
      "learning_rate": 4.863888301825049e-06,
      "loss": 2.4579,
      "step": 222600
    },
    {
      "epoch": 2.273516140229087,
      "grad_norm": 17.24479103088379,
      "learning_rate": 4.8570707862640695e-06,
      "loss": 2.5024,
      "step": 222700
    },
    {
      "epoch": 2.274537027584376,
      "grad_norm": 17.515518188476562,
      "learning_rate": 4.850253270703091e-06,
      "loss": 2.4771,
      "step": 222800
    },
    {
      "epoch": 2.2755579149396654,
      "grad_norm": 17.889148712158203,
      "learning_rate": 4.843435755142111e-06,
      "loss": 2.4371,
      "step": 222900
    },
    {
      "epoch": 2.2765788022949547,
      "grad_norm": 15.298223495483398,
      "learning_rate": 4.8366182395811326e-06,
      "loss": 2.4397,
      "step": 223000
    },
    {
      "epoch": 2.277599689650244,
      "grad_norm": 16.521162033081055,
      "learning_rate": 4.829800724020153e-06,
      "loss": 2.4693,
      "step": 223100
    },
    {
      "epoch": 2.278620577005533,
      "grad_norm": 15.777270317077637,
      "learning_rate": 4.8229832084591735e-06,
      "loss": 2.5847,
      "step": 223200
    },
    {
      "epoch": 2.2796414643608225,
      "grad_norm": 15.839730262756348,
      "learning_rate": 4.816233868053804e-06,
      "loss": 2.5236,
      "step": 223300
    },
    {
      "epoch": 2.2806623517161118,
      "grad_norm": 17.50175666809082,
      "learning_rate": 4.809416352492825e-06,
      "loss": 2.5753,
      "step": 223400
    },
    {
      "epoch": 2.281683239071401,
      "grad_norm": 17.63091468811035,
      "learning_rate": 4.8025988369318455e-06,
      "loss": 2.4653,
      "step": 223500
    },
    {
      "epoch": 2.2827041264266903,
      "grad_norm": 13.231945037841797,
      "learning_rate": 4.795849496526477e-06,
      "loss": 2.5985,
      "step": 223600
    },
    {
      "epoch": 2.283725013781979,
      "grad_norm": 14.438380241394043,
      "learning_rate": 4.789100156121107e-06,
      "loss": 2.5448,
      "step": 223700
    },
    {
      "epoch": 2.2847459011372684,
      "grad_norm": 17.208736419677734,
      "learning_rate": 4.782282640560127e-06,
      "loss": 2.5502,
      "step": 223800
    },
    {
      "epoch": 2.2857667884925577,
      "grad_norm": 14.881519317626953,
      "learning_rate": 4.775465124999148e-06,
      "loss": 2.489,
      "step": 223900
    },
    {
      "epoch": 2.286787675847847,
      "grad_norm": 15.9794921875,
      "learning_rate": 4.768647609438169e-06,
      "loss": 2.5293,
      "step": 224000
    },
    {
      "epoch": 2.287808563203136,
      "grad_norm": 15.032246589660645,
      "learning_rate": 4.7618300938771895e-06,
      "loss": 2.554,
      "step": 224100
    },
    {
      "epoch": 2.2888294505584255,
      "grad_norm": 20.796743392944336,
      "learning_rate": 4.755012578316211e-06,
      "loss": 2.5135,
      "step": 224200
    },
    {
      "epoch": 2.2898503379137147,
      "grad_norm": 25.006366729736328,
      "learning_rate": 4.748195062755231e-06,
      "loss": 2.4523,
      "step": 224300
    },
    {
      "epoch": 2.290871225269004,
      "grad_norm": 17.469039916992188,
      "learning_rate": 4.741377547194252e-06,
      "loss": 2.4769,
      "step": 224400
    },
    {
      "epoch": 2.291892112624293,
      "grad_norm": 15.803725242614746,
      "learning_rate": 4.734560031633272e-06,
      "loss": 2.4833,
      "step": 224500
    },
    {
      "epoch": 2.292912999979582,
      "grad_norm": 17.100337982177734,
      "learning_rate": 4.7277425160722935e-06,
      "loss": 2.3849,
      "step": 224600
    },
    {
      "epoch": 2.2939338873348714,
      "grad_norm": 19.06302833557129,
      "learning_rate": 4.720925000511314e-06,
      "loss": 2.4842,
      "step": 224700
    },
    {
      "epoch": 2.2949547746901606,
      "grad_norm": 17.165441513061523,
      "learning_rate": 4.7141074849503345e-06,
      "loss": 2.4755,
      "step": 224800
    },
    {
      "epoch": 2.29597566204545,
      "grad_norm": 14.089667320251465,
      "learning_rate": 4.707289969389355e-06,
      "loss": 2.5316,
      "step": 224900
    },
    {
      "epoch": 2.296996549400739,
      "grad_norm": 14.169533729553223,
      "learning_rate": 4.700472453828376e-06,
      "loss": 2.5637,
      "step": 225000
    },
    {
      "epoch": 2.2980174367560284,
      "grad_norm": 15.540886878967285,
      "learning_rate": 4.693654938267397e-06,
      "loss": 2.4337,
      "step": 225100
    },
    {
      "epoch": 2.2990383241113177,
      "grad_norm": 18.977249145507812,
      "learning_rate": 4.686837422706418e-06,
      "loss": 2.5446,
      "step": 225200
    },
    {
      "epoch": 2.300059211466607,
      "grad_norm": 18.293954849243164,
      "learning_rate": 4.6800199071454385e-06,
      "loss": 2.4186,
      "step": 225300
    },
    {
      "epoch": 2.3010800988218962,
      "grad_norm": 15.952140808105469,
      "learning_rate": 4.673202391584459e-06,
      "loss": 2.4707,
      "step": 225400
    },
    {
      "epoch": 2.302100986177185,
      "grad_norm": 17.759096145629883,
      "learning_rate": 4.6663848760234794e-06,
      "loss": 2.4449,
      "step": 225500
    },
    {
      "epoch": 2.3031218735324743,
      "grad_norm": 20.83900260925293,
      "learning_rate": 4.659567360462501e-06,
      "loss": 2.4341,
      "step": 225600
    },
    {
      "epoch": 2.3041427608877636,
      "grad_norm": 14.979985237121582,
      "learning_rate": 4.652749844901521e-06,
      "loss": 2.5546,
      "step": 225700
    },
    {
      "epoch": 2.305163648243053,
      "grad_norm": 16.229061126708984,
      "learning_rate": 4.6459323293405425e-06,
      "loss": 2.4597,
      "step": 225800
    },
    {
      "epoch": 2.306184535598342,
      "grad_norm": 20.173898696899414,
      "learning_rate": 4.639114813779563e-06,
      "loss": 2.5098,
      "step": 225900
    },
    {
      "epoch": 2.3072054229536314,
      "grad_norm": 17.26975440979004,
      "learning_rate": 4.6322972982185835e-06,
      "loss": 2.5231,
      "step": 226000
    },
    {
      "epoch": 2.3082263103089207,
      "grad_norm": 21.879188537597656,
      "learning_rate": 4.625479782657604e-06,
      "loss": 2.4744,
      "step": 226100
    },
    {
      "epoch": 2.3092471976642095,
      "grad_norm": 17.15676498413086,
      "learning_rate": 4.618662267096625e-06,
      "loss": 2.5011,
      "step": 226200
    },
    {
      "epoch": 2.3102680850194988,
      "grad_norm": 16.926956176757812,
      "learning_rate": 4.611844751535646e-06,
      "loss": 2.471,
      "step": 226300
    },
    {
      "epoch": 2.311288972374788,
      "grad_norm": 17.679563522338867,
      "learning_rate": 4.605027235974667e-06,
      "loss": 2.5112,
      "step": 226400
    },
    {
      "epoch": 2.3123098597300773,
      "grad_norm": 13.854089736938477,
      "learning_rate": 4.5982097204136875e-06,
      "loss": 2.4806,
      "step": 226500
    },
    {
      "epoch": 2.3133307470853666,
      "grad_norm": 15.086844444274902,
      "learning_rate": 4.591392204852708e-06,
      "loss": 2.4626,
      "step": 226600
    },
    {
      "epoch": 2.314351634440656,
      "grad_norm": 17.07346534729004,
      "learning_rate": 4.584574689291728e-06,
      "loss": 2.4978,
      "step": 226700
    },
    {
      "epoch": 2.315372521795945,
      "grad_norm": 16.999347686767578,
      "learning_rate": 4.57775717373075e-06,
      "loss": 2.4651,
      "step": 226800
    },
    {
      "epoch": 2.3163934091512344,
      "grad_norm": 15.699782371520996,
      "learning_rate": 4.57093965816977e-06,
      "loss": 2.5182,
      "step": 226900
    },
    {
      "epoch": 2.3174142965065236,
      "grad_norm": 14.385303497314453,
      "learning_rate": 4.564122142608791e-06,
      "loss": 2.4776,
      "step": 227000
    },
    {
      "epoch": 2.318435183861813,
      "grad_norm": 15.242522239685059,
      "learning_rate": 4.557304627047812e-06,
      "loss": 2.4814,
      "step": 227100
    },
    {
      "epoch": 2.3194560712171017,
      "grad_norm": 12.381078720092773,
      "learning_rate": 4.5504871114868324e-06,
      "loss": 2.4413,
      "step": 227200
    },
    {
      "epoch": 2.320476958572391,
      "grad_norm": 15.893172264099121,
      "learning_rate": 4.543669595925853e-06,
      "loss": 2.3833,
      "step": 227300
    },
    {
      "epoch": 2.3214978459276803,
      "grad_norm": 16.197282791137695,
      "learning_rate": 4.536852080364873e-06,
      "loss": 2.44,
      "step": 227400
    },
    {
      "epoch": 2.3225187332829695,
      "grad_norm": 11.192431449890137,
      "learning_rate": 4.530034564803895e-06,
      "loss": 2.529,
      "step": 227500
    },
    {
      "epoch": 2.323539620638259,
      "grad_norm": 12.85191535949707,
      "learning_rate": 4.523217049242915e-06,
      "loss": 2.5192,
      "step": 227600
    },
    {
      "epoch": 2.324560507993548,
      "grad_norm": 17.346662521362305,
      "learning_rate": 4.5163995336819365e-06,
      "loss": 2.5114,
      "step": 227700
    },
    {
      "epoch": 2.3255813953488373,
      "grad_norm": 18.167972564697266,
      "learning_rate": 4.509582018120957e-06,
      "loss": 2.5546,
      "step": 227800
    },
    {
      "epoch": 2.3266022827041266,
      "grad_norm": 18.551774978637695,
      "learning_rate": 4.502764502559977e-06,
      "loss": 2.4853,
      "step": 227900
    },
    {
      "epoch": 2.3276231700594154,
      "grad_norm": 18.795188903808594,
      "learning_rate": 4.495946986998998e-06,
      "loss": 2.4864,
      "step": 228000
    },
    {
      "epoch": 2.3286440574147047,
      "grad_norm": 16.518253326416016,
      "learning_rate": 4.489129471438019e-06,
      "loss": 2.4478,
      "step": 228100
    },
    {
      "epoch": 2.329664944769994,
      "grad_norm": 14.879751205444336,
      "learning_rate": 4.48231195587704e-06,
      "loss": 2.56,
      "step": 228200
    },
    {
      "epoch": 2.3306858321252832,
      "grad_norm": 19.328964233398438,
      "learning_rate": 4.47549444031606e-06,
      "loss": 2.477,
      "step": 228300
    },
    {
      "epoch": 2.3317067194805725,
      "grad_norm": 15.364694595336914,
      "learning_rate": 4.4686769247550806e-06,
      "loss": 2.4152,
      "step": 228400
    },
    {
      "epoch": 2.3327276068358618,
      "grad_norm": 17.125394821166992,
      "learning_rate": 4.461859409194102e-06,
      "loss": 2.4653,
      "step": 228500
    },
    {
      "epoch": 2.333748494191151,
      "grad_norm": 12.227882385253906,
      "learning_rate": 4.455041893633122e-06,
      "loss": 2.4756,
      "step": 228600
    },
    {
      "epoch": 2.3347693815464403,
      "grad_norm": 15.2926025390625,
      "learning_rate": 4.448224378072144e-06,
      "loss": 2.4619,
      "step": 228700
    },
    {
      "epoch": 2.3357902689017296,
      "grad_norm": 19.012653350830078,
      "learning_rate": 4.441406862511164e-06,
      "loss": 2.4728,
      "step": 228800
    },
    {
      "epoch": 2.3368111562570184,
      "grad_norm": 14.909732818603516,
      "learning_rate": 4.434589346950185e-06,
      "loss": 2.4571,
      "step": 228900
    },
    {
      "epoch": 2.3378320436123077,
      "grad_norm": 16.3620662689209,
      "learning_rate": 4.427771831389205e-06,
      "loss": 2.4499,
      "step": 229000
    },
    {
      "epoch": 2.338852930967597,
      "grad_norm": 17.358308792114258,
      "learning_rate": 4.420954315828226e-06,
      "loss": 2.4398,
      "step": 229100
    },
    {
      "epoch": 2.339873818322886,
      "grad_norm": 15.36608600616455,
      "learning_rate": 4.414136800267247e-06,
      "loss": 2.4499,
      "step": 229200
    },
    {
      "epoch": 2.3408947056781755,
      "grad_norm": 17.562423706054688,
      "learning_rate": 4.407319284706268e-06,
      "loss": 2.5211,
      "step": 229300
    },
    {
      "epoch": 2.3419155930334647,
      "grad_norm": 20.087249755859375,
      "learning_rate": 4.400501769145289e-06,
      "loss": 2.4413,
      "step": 229400
    },
    {
      "epoch": 2.342936480388754,
      "grad_norm": 11.473799705505371,
      "learning_rate": 4.393684253584309e-06,
      "loss": 2.5134,
      "step": 229500
    },
    {
      "epoch": 2.3439573677440433,
      "grad_norm": 17.562841415405273,
      "learning_rate": 4.3868667380233295e-06,
      "loss": 2.5345,
      "step": 229600
    },
    {
      "epoch": 2.344978255099332,
      "grad_norm": 13.689496994018555,
      "learning_rate": 4.380049222462351e-06,
      "loss": 2.4517,
      "step": 229700
    },
    {
      "epoch": 2.3459991424546214,
      "grad_norm": 14.141313552856445,
      "learning_rate": 4.373231706901371e-06,
      "loss": 2.5133,
      "step": 229800
    },
    {
      "epoch": 2.3470200298099106,
      "grad_norm": 20.202308654785156,
      "learning_rate": 4.366414191340393e-06,
      "loss": 2.4307,
      "step": 229900
    },
    {
      "epoch": 2.3480409171652,
      "grad_norm": 14.7958402633667,
      "learning_rate": 4.359596675779413e-06,
      "loss": 2.536,
      "step": 230000
    },
    {
      "epoch": 2.349061804520489,
      "grad_norm": 13.653510093688965,
      "learning_rate": 4.3527791602184336e-06,
      "loss": 2.5083,
      "step": 230100
    },
    {
      "epoch": 2.3500826918757785,
      "grad_norm": 15.592963218688965,
      "learning_rate": 4.345961644657454e-06,
      "loss": 2.5252,
      "step": 230200
    },
    {
      "epoch": 2.3511035792310677,
      "grad_norm": 14.64931869506836,
      "learning_rate": 4.3391441290964745e-06,
      "loss": 2.4182,
      "step": 230300
    },
    {
      "epoch": 2.352124466586357,
      "grad_norm": 13.659625053405762,
      "learning_rate": 4.3323947886911056e-06,
      "loss": 2.4183,
      "step": 230400
    },
    {
      "epoch": 2.3531453539416463,
      "grad_norm": 18.38736915588379,
      "learning_rate": 4.325577273130127e-06,
      "loss": 2.4438,
      "step": 230500
    },
    {
      "epoch": 2.3541662412969355,
      "grad_norm": 17.548664093017578,
      "learning_rate": 4.318759757569147e-06,
      "loss": 2.4496,
      "step": 230600
    },
    {
      "epoch": 2.3551871286522243,
      "grad_norm": 20.2454776763916,
      "learning_rate": 4.311942242008168e-06,
      "loss": 2.4762,
      "step": 230700
    },
    {
      "epoch": 2.3562080160075136,
      "grad_norm": 14.21785831451416,
      "learning_rate": 4.305124726447188e-06,
      "loss": 2.5429,
      "step": 230800
    },
    {
      "epoch": 2.357228903362803,
      "grad_norm": 16.988588333129883,
      "learning_rate": 4.298307210886209e-06,
      "loss": 2.4472,
      "step": 230900
    },
    {
      "epoch": 2.358249790718092,
      "grad_norm": 14.815147399902344,
      "learning_rate": 4.29148969532523e-06,
      "loss": 2.4935,
      "step": 231000
    },
    {
      "epoch": 2.3592706780733814,
      "grad_norm": 18.061838150024414,
      "learning_rate": 4.2846721797642505e-06,
      "loss": 2.5499,
      "step": 231100
    },
    {
      "epoch": 2.3602915654286707,
      "grad_norm": 16.927480697631836,
      "learning_rate": 4.277854664203272e-06,
      "loss": 2.5159,
      "step": 231200
    },
    {
      "epoch": 2.36131245278396,
      "grad_norm": 18.944278717041016,
      "learning_rate": 4.271037148642292e-06,
      "loss": 2.4566,
      "step": 231300
    },
    {
      "epoch": 2.3623333401392492,
      "grad_norm": 18.779001235961914,
      "learning_rate": 4.264219633081313e-06,
      "loss": 2.4764,
      "step": 231400
    },
    {
      "epoch": 2.363354227494538,
      "grad_norm": 15.702012062072754,
      "learning_rate": 4.257470292675943e-06,
      "loss": 2.46,
      "step": 231500
    },
    {
      "epoch": 2.3643751148498273,
      "grad_norm": 18.502849578857422,
      "learning_rate": 4.250652777114964e-06,
      "loss": 2.4571,
      "step": 231600
    },
    {
      "epoch": 2.3653960022051166,
      "grad_norm": 16.042949676513672,
      "learning_rate": 4.243835261553985e-06,
      "loss": 2.5155,
      "step": 231700
    },
    {
      "epoch": 2.366416889560406,
      "grad_norm": 18.345136642456055,
      "learning_rate": 4.237017745993005e-06,
      "loss": 2.5312,
      "step": 231800
    },
    {
      "epoch": 2.367437776915695,
      "grad_norm": 16.69707679748535,
      "learning_rate": 4.230200230432026e-06,
      "loss": 2.4147,
      "step": 231900
    },
    {
      "epoch": 2.3684586642709844,
      "grad_norm": 18.64362907409668,
      "learning_rate": 4.223382714871047e-06,
      "loss": 2.5363,
      "step": 232000
    },
    {
      "epoch": 2.3694795516262737,
      "grad_norm": 17.854175567626953,
      "learning_rate": 4.2165651993100675e-06,
      "loss": 2.4787,
      "step": 232100
    },
    {
      "epoch": 2.370500438981563,
      "grad_norm": 17.067447662353516,
      "learning_rate": 4.209747683749089e-06,
      "loss": 2.4901,
      "step": 232200
    },
    {
      "epoch": 2.371521326336852,
      "grad_norm": 18.908218383789062,
      "learning_rate": 4.202930168188109e-06,
      "loss": 2.5071,
      "step": 232300
    },
    {
      "epoch": 2.372542213692141,
      "grad_norm": 15.088629722595215,
      "learning_rate": 4.19611265262713e-06,
      "loss": 2.5252,
      "step": 232400
    },
    {
      "epoch": 2.3735631010474303,
      "grad_norm": 17.200754165649414,
      "learning_rate": 4.18929513706615e-06,
      "loss": 2.4479,
      "step": 232500
    },
    {
      "epoch": 2.3745839884027196,
      "grad_norm": 22.313213348388672,
      "learning_rate": 4.1824776215051715e-06,
      "loss": 2.4599,
      "step": 232600
    },
    {
      "epoch": 2.375604875758009,
      "grad_norm": 14.848472595214844,
      "learning_rate": 4.175660105944192e-06,
      "loss": 2.4046,
      "step": 232700
    },
    {
      "epoch": 2.376625763113298,
      "grad_norm": 15.75937557220459,
      "learning_rate": 4.168842590383213e-06,
      "loss": 2.5336,
      "step": 232800
    },
    {
      "epoch": 2.3776466504685874,
      "grad_norm": 15.63990306854248,
      "learning_rate": 4.162025074822234e-06,
      "loss": 2.4873,
      "step": 232900
    },
    {
      "epoch": 2.3786675378238766,
      "grad_norm": 16.090076446533203,
      "learning_rate": 4.155207559261254e-06,
      "loss": 2.4098,
      "step": 233000
    },
    {
      "epoch": 2.379688425179166,
      "grad_norm": 14.734309196472168,
      "learning_rate": 4.148390043700275e-06,
      "loss": 2.4457,
      "step": 233100
    },
    {
      "epoch": 2.3807093125344547,
      "grad_norm": 12.092403411865234,
      "learning_rate": 4.141572528139296e-06,
      "loss": 2.5404,
      "step": 233200
    },
    {
      "epoch": 2.381730199889744,
      "grad_norm": 16.12270164489746,
      "learning_rate": 4.1347550125783165e-06,
      "loss": 2.4805,
      "step": 233300
    },
    {
      "epoch": 2.3827510872450333,
      "grad_norm": 14.724405288696289,
      "learning_rate": 4.127937497017338e-06,
      "loss": 2.5094,
      "step": 233400
    },
    {
      "epoch": 2.3837719746003225,
      "grad_norm": 19.954750061035156,
      "learning_rate": 4.121119981456358e-06,
      "loss": 2.4864,
      "step": 233500
    },
    {
      "epoch": 2.384792861955612,
      "grad_norm": 12.65524959564209,
      "learning_rate": 4.114302465895379e-06,
      "loss": 2.5569,
      "step": 233600
    },
    {
      "epoch": 2.385813749310901,
      "grad_norm": 16.859375,
      "learning_rate": 4.107484950334399e-06,
      "loss": 2.4159,
      "step": 233700
    },
    {
      "epoch": 2.3868346366661903,
      "grad_norm": 16.22818374633789,
      "learning_rate": 4.1006674347734205e-06,
      "loss": 2.5106,
      "step": 233800
    },
    {
      "epoch": 2.3878555240214796,
      "grad_norm": 19.70389175415039,
      "learning_rate": 4.093849919212441e-06,
      "loss": 2.4636,
      "step": 233900
    },
    {
      "epoch": 2.388876411376769,
      "grad_norm": 14.96565055847168,
      "learning_rate": 4.087032403651462e-06,
      "loss": 2.425,
      "step": 234000
    },
    {
      "epoch": 2.389897298732058,
      "grad_norm": 17.697973251342773,
      "learning_rate": 4.080214888090483e-06,
      "loss": 2.4251,
      "step": 234100
    },
    {
      "epoch": 2.390918186087347,
      "grad_norm": 18.27754020690918,
      "learning_rate": 4.073397372529503e-06,
      "loss": 2.442,
      "step": 234200
    },
    {
      "epoch": 2.3919390734426362,
      "grad_norm": 21.891592025756836,
      "learning_rate": 4.066579856968524e-06,
      "loss": 2.4864,
      "step": 234300
    },
    {
      "epoch": 2.3929599607979255,
      "grad_norm": 16.47702407836914,
      "learning_rate": 4.059762341407544e-06,
      "loss": 2.568,
      "step": 234400
    },
    {
      "epoch": 2.3939808481532148,
      "grad_norm": 13.348169326782227,
      "learning_rate": 4.0529448258465654e-06,
      "loss": 2.5066,
      "step": 234500
    },
    {
      "epoch": 2.395001735508504,
      "grad_norm": 17.777544021606445,
      "learning_rate": 4.046127310285586e-06,
      "loss": 2.4829,
      "step": 234600
    },
    {
      "epoch": 2.3960226228637933,
      "grad_norm": 15.9660005569458,
      "learning_rate": 4.039309794724607e-06,
      "loss": 2.5224,
      "step": 234700
    },
    {
      "epoch": 2.3970435102190826,
      "grad_norm": 17.175626754760742,
      "learning_rate": 4.032492279163628e-06,
      "loss": 2.4095,
      "step": 234800
    },
    {
      "epoch": 2.3980643975743714,
      "grad_norm": 13.159035682678223,
      "learning_rate": 4.025674763602648e-06,
      "loss": 2.5029,
      "step": 234900
    },
    {
      "epoch": 2.3990852849296607,
      "grad_norm": 20.90946388244629,
      "learning_rate": 4.018857248041669e-06,
      "loss": 2.4058,
      "step": 235000
    },
    {
      "epoch": 2.40010617228495,
      "grad_norm": 22.2458553314209,
      "learning_rate": 4.01203973248069e-06,
      "loss": 2.4131,
      "step": 235100
    },
    {
      "epoch": 2.401127059640239,
      "grad_norm": 18.47629737854004,
      "learning_rate": 4.00522221691971e-06,
      "loss": 2.5417,
      "step": 235200
    },
    {
      "epoch": 2.4021479469955285,
      "grad_norm": 17.384313583374023,
      "learning_rate": 3.998404701358731e-06,
      "loss": 2.4555,
      "step": 235300
    },
    {
      "epoch": 2.4031688343508177,
      "grad_norm": 15.504772186279297,
      "learning_rate": 3.991587185797751e-06,
      "loss": 2.5211,
      "step": 235400
    },
    {
      "epoch": 2.404189721706107,
      "grad_norm": 18.680696487426758,
      "learning_rate": 3.984769670236773e-06,
      "loss": 2.5172,
      "step": 235500
    },
    {
      "epoch": 2.4052106090613963,
      "grad_norm": 24.01415252685547,
      "learning_rate": 3.977952154675793e-06,
      "loss": 2.4989,
      "step": 235600
    },
    {
      "epoch": 2.4062314964166855,
      "grad_norm": 13.521302223205566,
      "learning_rate": 3.971134639114814e-06,
      "loss": 2.4917,
      "step": 235700
    },
    {
      "epoch": 2.407252383771975,
      "grad_norm": 13.651540756225586,
      "learning_rate": 3.964317123553835e-06,
      "loss": 2.4521,
      "step": 235800
    },
    {
      "epoch": 2.4082732711272636,
      "grad_norm": 17.982187271118164,
      "learning_rate": 3.957499607992855e-06,
      "loss": 2.4884,
      "step": 235900
    },
    {
      "epoch": 2.409294158482553,
      "grad_norm": 14.058511734008789,
      "learning_rate": 3.950682092431876e-06,
      "loss": 2.5438,
      "step": 236000
    },
    {
      "epoch": 2.410315045837842,
      "grad_norm": 21.247150421142578,
      "learning_rate": 3.943864576870897e-06,
      "loss": 2.4526,
      "step": 236100
    },
    {
      "epoch": 2.4113359331931314,
      "grad_norm": 17.99908447265625,
      "learning_rate": 3.937047061309918e-06,
      "loss": 2.3679,
      "step": 236200
    },
    {
      "epoch": 2.4123568205484207,
      "grad_norm": 17.414039611816406,
      "learning_rate": 3.930229545748939e-06,
      "loss": 2.4349,
      "step": 236300
    },
    {
      "epoch": 2.41337770790371,
      "grad_norm": 14.958518981933594,
      "learning_rate": 3.923412030187959e-06,
      "loss": 2.4352,
      "step": 236400
    },
    {
      "epoch": 2.4143985952589992,
      "grad_norm": 22.743732452392578,
      "learning_rate": 3.91659451462698e-06,
      "loss": 2.4771,
      "step": 236500
    },
    {
      "epoch": 2.4154194826142885,
      "grad_norm": 16.700462341308594,
      "learning_rate": 3.909776999066e-06,
      "loss": 2.4685,
      "step": 236600
    },
    {
      "epoch": 2.4164403699695773,
      "grad_norm": 13.53549861907959,
      "learning_rate": 3.902959483505022e-06,
      "loss": 2.3787,
      "step": 236700
    },
    {
      "epoch": 2.4174612573248666,
      "grad_norm": 15.077692031860352,
      "learning_rate": 3.896141967944042e-06,
      "loss": 2.4675,
      "step": 236800
    },
    {
      "epoch": 2.418482144680156,
      "grad_norm": 17.030017852783203,
      "learning_rate": 3.889324452383063e-06,
      "loss": 2.3789,
      "step": 236900
    },
    {
      "epoch": 2.419503032035445,
      "grad_norm": 16.02330207824707,
      "learning_rate": 3.882506936822084e-06,
      "loss": 2.4192,
      "step": 237000
    },
    {
      "epoch": 2.4205239193907344,
      "grad_norm": 14.70767879486084,
      "learning_rate": 3.875689421261104e-06,
      "loss": 2.4277,
      "step": 237100
    },
    {
      "epoch": 2.4215448067460237,
      "grad_norm": 16.291154861450195,
      "learning_rate": 3.868871905700125e-06,
      "loss": 2.4831,
      "step": 237200
    },
    {
      "epoch": 2.422565694101313,
      "grad_norm": 14.136693000793457,
      "learning_rate": 3.862054390139146e-06,
      "loss": 2.4346,
      "step": 237300
    },
    {
      "epoch": 2.423586581456602,
      "grad_norm": 19.832685470581055,
      "learning_rate": 3.8552368745781666e-06,
      "loss": 2.3765,
      "step": 237400
    },
    {
      "epoch": 2.4246074688118915,
      "grad_norm": 19.71554183959961,
      "learning_rate": 3.848487534172798e-06,
      "loss": 2.4435,
      "step": 237500
    },
    {
      "epoch": 2.4256283561671808,
      "grad_norm": 15.219382286071777,
      "learning_rate": 3.841670018611818e-06,
      "loss": 2.4714,
      "step": 237600
    },
    {
      "epoch": 2.4266492435224696,
      "grad_norm": 15.523924827575684,
      "learning_rate": 3.834852503050839e-06,
      "loss": 2.4353,
      "step": 237700
    },
    {
      "epoch": 2.427670130877759,
      "grad_norm": 18.18520164489746,
      "learning_rate": 3.828034987489859e-06,
      "loss": 2.4645,
      "step": 237800
    },
    {
      "epoch": 2.428691018233048,
      "grad_norm": 17.941394805908203,
      "learning_rate": 3.82121747192888e-06,
      "loss": 2.4957,
      "step": 237900
    },
    {
      "epoch": 2.4297119055883374,
      "grad_norm": 18.843473434448242,
      "learning_rate": 3.814399956367901e-06,
      "loss": 2.5595,
      "step": 238000
    },
    {
      "epoch": 2.4307327929436267,
      "grad_norm": 16.54530906677246,
      "learning_rate": 3.8075824408069213e-06,
      "loss": 2.3938,
      "step": 238100
    },
    {
      "epoch": 2.431753680298916,
      "grad_norm": 20.763490676879883,
      "learning_rate": 3.800764925245942e-06,
      "loss": 2.5095,
      "step": 238200
    },
    {
      "epoch": 2.432774567654205,
      "grad_norm": 16.040287017822266,
      "learning_rate": 3.7939474096849626e-06,
      "loss": 2.459,
      "step": 238300
    },
    {
      "epoch": 2.433795455009494,
      "grad_norm": 14.608609199523926,
      "learning_rate": 3.7871298941239835e-06,
      "loss": 2.4513,
      "step": 238400
    },
    {
      "epoch": 2.4348163423647833,
      "grad_norm": 16.14383888244629,
      "learning_rate": 3.780312378563004e-06,
      "loss": 2.4236,
      "step": 238500
    },
    {
      "epoch": 2.4358372297200725,
      "grad_norm": 14.23701286315918,
      "learning_rate": 3.7734948630020253e-06,
      "loss": 2.4575,
      "step": 238600
    },
    {
      "epoch": 2.436858117075362,
      "grad_norm": 13.89569091796875,
      "learning_rate": 3.7666773474410458e-06,
      "loss": 2.49,
      "step": 238700
    },
    {
      "epoch": 2.437879004430651,
      "grad_norm": 15.57324504852295,
      "learning_rate": 3.7598598318800667e-06,
      "loss": 2.5121,
      "step": 238800
    },
    {
      "epoch": 2.4388998917859404,
      "grad_norm": 14.14759349822998,
      "learning_rate": 3.753042316319087e-06,
      "loss": 2.5285,
      "step": 238900
    },
    {
      "epoch": 2.4399207791412296,
      "grad_norm": 20.583017349243164,
      "learning_rate": 3.746224800758108e-06,
      "loss": 2.4328,
      "step": 239000
    },
    {
      "epoch": 2.440941666496519,
      "grad_norm": 18.538949966430664,
      "learning_rate": 3.7394072851971285e-06,
      "loss": 2.4385,
      "step": 239100
    },
    {
      "epoch": 2.441962553851808,
      "grad_norm": 14.107574462890625,
      "learning_rate": 3.73258976963615e-06,
      "loss": 2.5431,
      "step": 239200
    },
    {
      "epoch": 2.4429834412070974,
      "grad_norm": 20.921974182128906,
      "learning_rate": 3.7257722540751703e-06,
      "loss": 2.4293,
      "step": 239300
    },
    {
      "epoch": 2.4440043285623863,
      "grad_norm": 13.80403995513916,
      "learning_rate": 3.718954738514191e-06,
      "loss": 2.4276,
      "step": 239400
    },
    {
      "epoch": 2.4450252159176755,
      "grad_norm": 18.544208526611328,
      "learning_rate": 3.7121372229532116e-06,
      "loss": 2.465,
      "step": 239500
    },
    {
      "epoch": 2.446046103272965,
      "grad_norm": 13.283319473266602,
      "learning_rate": 3.7053878825478423e-06,
      "loss": 2.4445,
      "step": 239600
    },
    {
      "epoch": 2.447066990628254,
      "grad_norm": 15.992746353149414,
      "learning_rate": 3.6985703669868627e-06,
      "loss": 2.4743,
      "step": 239700
    },
    {
      "epoch": 2.4480878779835433,
      "grad_norm": 22.560359954833984,
      "learning_rate": 3.6917528514258836e-06,
      "loss": 2.4496,
      "step": 239800
    },
    {
      "epoch": 2.4491087653388326,
      "grad_norm": 16.176881790161133,
      "learning_rate": 3.684935335864904e-06,
      "loss": 2.4007,
      "step": 239900
    },
    {
      "epoch": 2.450129652694122,
      "grad_norm": 12.298942565917969,
      "learning_rate": 3.6781178203039254e-06,
      "loss": 2.4323,
      "step": 240000
    },
    {
      "epoch": 2.451150540049411,
      "grad_norm": 20.74795913696289,
      "learning_rate": 3.671300304742946e-06,
      "loss": 2.4631,
      "step": 240100
    },
    {
      "epoch": 2.4521714274047,
      "grad_norm": 19.37584114074707,
      "learning_rate": 3.6644827891819668e-06,
      "loss": 2.4204,
      "step": 240200
    },
    {
      "epoch": 2.4531923147599892,
      "grad_norm": 19.05898666381836,
      "learning_rate": 3.6576652736209872e-06,
      "loss": 2.4539,
      "step": 240300
    },
    {
      "epoch": 2.4542132021152785,
      "grad_norm": 17.901151657104492,
      "learning_rate": 3.650847758060008e-06,
      "loss": 2.4282,
      "step": 240400
    },
    {
      "epoch": 2.4552340894705678,
      "grad_norm": 14.259075164794922,
      "learning_rate": 3.6440302424990286e-06,
      "loss": 2.5318,
      "step": 240500
    },
    {
      "epoch": 2.456254976825857,
      "grad_norm": 17.299894332885742,
      "learning_rate": 3.63721272693805e-06,
      "loss": 2.4863,
      "step": 240600
    },
    {
      "epoch": 2.4572758641811463,
      "grad_norm": 20.406734466552734,
      "learning_rate": 3.6303952113770704e-06,
      "loss": 2.4394,
      "step": 240700
    },
    {
      "epoch": 2.4582967515364356,
      "grad_norm": 20.675888061523438,
      "learning_rate": 3.6235776958160913e-06,
      "loss": 2.5034,
      "step": 240800
    },
    {
      "epoch": 2.459317638891725,
      "grad_norm": 18.513608932495117,
      "learning_rate": 3.6167601802551117e-06,
      "loss": 2.3827,
      "step": 240900
    },
    {
      "epoch": 2.460338526247014,
      "grad_norm": 16.26384162902832,
      "learning_rate": 3.6099426646941326e-06,
      "loss": 2.479,
      "step": 241000
    },
    {
      "epoch": 2.4613594136023034,
      "grad_norm": 12.035516738891602,
      "learning_rate": 3.603125149133153e-06,
      "loss": 2.4316,
      "step": 241100
    },
    {
      "epoch": 2.462380300957592,
      "grad_norm": 17.297733306884766,
      "learning_rate": 3.596307633572174e-06,
      "loss": 2.5619,
      "step": 241200
    },
    {
      "epoch": 2.4634011883128815,
      "grad_norm": 15.405481338500977,
      "learning_rate": 3.5894901180111944e-06,
      "loss": 2.5529,
      "step": 241300
    },
    {
      "epoch": 2.4644220756681707,
      "grad_norm": 20.537935256958008,
      "learning_rate": 3.5827407776058255e-06,
      "loss": 2.4503,
      "step": 241400
    },
    {
      "epoch": 2.46544296302346,
      "grad_norm": 20.764877319335938,
      "learning_rate": 3.575923262044846e-06,
      "loss": 2.4375,
      "step": 241500
    },
    {
      "epoch": 2.4664638503787493,
      "grad_norm": 16.846540451049805,
      "learning_rate": 3.569105746483867e-06,
      "loss": 2.5374,
      "step": 241600
    },
    {
      "epoch": 2.4674847377340385,
      "grad_norm": 16.507871627807617,
      "learning_rate": 3.5622882309228873e-06,
      "loss": 2.4917,
      "step": 241700
    },
    {
      "epoch": 2.468505625089328,
      "grad_norm": 16.653966903686523,
      "learning_rate": 3.5554707153619082e-06,
      "loss": 2.3969,
      "step": 241800
    },
    {
      "epoch": 2.4695265124446166,
      "grad_norm": 13.066001892089844,
      "learning_rate": 3.5486531998009287e-06,
      "loss": 2.3748,
      "step": 241900
    },
    {
      "epoch": 2.470547399799906,
      "grad_norm": 16.234228134155273,
      "learning_rate": 3.54183568423995e-06,
      "loss": 2.4478,
      "step": 242000
    },
    {
      "epoch": 2.471568287155195,
      "grad_norm": 19.39727783203125,
      "learning_rate": 3.5350181686789705e-06,
      "loss": 2.4705,
      "step": 242100
    },
    {
      "epoch": 2.4725891745104844,
      "grad_norm": 22.225479125976562,
      "learning_rate": 3.528200653117991e-06,
      "loss": 2.4102,
      "step": 242200
    },
    {
      "epoch": 2.4736100618657737,
      "grad_norm": 17.767127990722656,
      "learning_rate": 3.521383137557012e-06,
      "loss": 2.4268,
      "step": 242300
    },
    {
      "epoch": 2.474630949221063,
      "grad_norm": 15.932116508483887,
      "learning_rate": 3.5145656219960323e-06,
      "loss": 2.4409,
      "step": 242400
    },
    {
      "epoch": 2.4756518365763522,
      "grad_norm": 17.042375564575195,
      "learning_rate": 3.507748106435053e-06,
      "loss": 2.4855,
      "step": 242500
    },
    {
      "epoch": 2.4766727239316415,
      "grad_norm": 17.438440322875977,
      "learning_rate": 3.5009305908740736e-06,
      "loss": 2.4715,
      "step": 242600
    },
    {
      "epoch": 2.4776936112869308,
      "grad_norm": 14.758759498596191,
      "learning_rate": 3.4941130753130945e-06,
      "loss": 2.4754,
      "step": 242700
    },
    {
      "epoch": 2.47871449864222,
      "grad_norm": 20.039731979370117,
      "learning_rate": 3.487295559752115e-06,
      "loss": 2.4363,
      "step": 242800
    },
    {
      "epoch": 2.479735385997509,
      "grad_norm": 16.835176467895508,
      "learning_rate": 3.4804780441911363e-06,
      "loss": 2.4949,
      "step": 242900
    },
    {
      "epoch": 2.480756273352798,
      "grad_norm": 13.112823486328125,
      "learning_rate": 3.4736605286301568e-06,
      "loss": 2.4448,
      "step": 243000
    },
    {
      "epoch": 2.4817771607080874,
      "grad_norm": 17.255151748657227,
      "learning_rate": 3.4668430130691777e-06,
      "loss": 2.3364,
      "step": 243100
    },
    {
      "epoch": 2.4827980480633767,
      "grad_norm": 18.250791549682617,
      "learning_rate": 3.460025497508198e-06,
      "loss": 2.443,
      "step": 243200
    },
    {
      "epoch": 2.483818935418666,
      "grad_norm": 14.190982818603516,
      "learning_rate": 3.453207981947219e-06,
      "loss": 2.3786,
      "step": 243300
    },
    {
      "epoch": 2.484839822773955,
      "grad_norm": 17.160873413085938,
      "learning_rate": 3.4463904663862395e-06,
      "loss": 2.4225,
      "step": 243400
    },
    {
      "epoch": 2.4858607101292445,
      "grad_norm": 16.560726165771484,
      "learning_rate": 3.439572950825261e-06,
      "loss": 2.4632,
      "step": 243500
    },
    {
      "epoch": 2.4868815974845337,
      "grad_norm": 15.61562442779541,
      "learning_rate": 3.4327554352642813e-06,
      "loss": 2.5164,
      "step": 243600
    },
    {
      "epoch": 2.4879024848398226,
      "grad_norm": 19.910470962524414,
      "learning_rate": 3.425937919703302e-06,
      "loss": 2.3617,
      "step": 243700
    },
    {
      "epoch": 2.488923372195112,
      "grad_norm": 17.12962532043457,
      "learning_rate": 3.4191204041423226e-06,
      "loss": 2.4092,
      "step": 243800
    },
    {
      "epoch": 2.489944259550401,
      "grad_norm": 15.843524932861328,
      "learning_rate": 3.4123028885813435e-06,
      "loss": 2.453,
      "step": 243900
    },
    {
      "epoch": 2.4909651469056904,
      "grad_norm": 15.286022186279297,
      "learning_rate": 3.405485373020364e-06,
      "loss": 2.4319,
      "step": 244000
    },
    {
      "epoch": 2.4919860342609796,
      "grad_norm": 19.508277893066406,
      "learning_rate": 3.3986678574593853e-06,
      "loss": 2.3843,
      "step": 244100
    },
    {
      "epoch": 2.493006921616269,
      "grad_norm": 18.839841842651367,
      "learning_rate": 3.3918503418984058e-06,
      "loss": 2.3906,
      "step": 244200
    },
    {
      "epoch": 2.494027808971558,
      "grad_norm": 15.307440757751465,
      "learning_rate": 3.3850328263374266e-06,
      "loss": 2.5075,
      "step": 244300
    },
    {
      "epoch": 2.4950486963268474,
      "grad_norm": 19.26678466796875,
      "learning_rate": 3.378215310776447e-06,
      "loss": 2.4199,
      "step": 244400
    },
    {
      "epoch": 2.4960695836821367,
      "grad_norm": 16.502891540527344,
      "learning_rate": 3.371397795215468e-06,
      "loss": 2.4947,
      "step": 244500
    },
    {
      "epoch": 2.4970904710374255,
      "grad_norm": 18.692811965942383,
      "learning_rate": 3.3646484548100982e-06,
      "loss": 2.5011,
      "step": 244600
    },
    {
      "epoch": 2.498111358392715,
      "grad_norm": 18.033620834350586,
      "learning_rate": 3.357830939249119e-06,
      "loss": 2.4591,
      "step": 244700
    },
    {
      "epoch": 2.499132245748004,
      "grad_norm": 19.98988151550293,
      "learning_rate": 3.3510134236881396e-06,
      "loss": 2.5027,
      "step": 244800
    },
    {
      "epoch": 2.5001531331032933,
      "grad_norm": 21.18260955810547,
      "learning_rate": 3.344195908127161e-06,
      "loss": 2.4251,
      "step": 244900
    },
    {
      "epoch": 2.5011740204585826,
      "grad_norm": 16.318944931030273,
      "learning_rate": 3.3373783925661814e-06,
      "loss": 2.4279,
      "step": 245000
    },
    {
      "epoch": 2.502194907813872,
      "grad_norm": 13.284533500671387,
      "learning_rate": 3.3305608770052023e-06,
      "loss": 2.4682,
      "step": 245100
    },
    {
      "epoch": 2.503215795169161,
      "grad_norm": 14.497879028320312,
      "learning_rate": 3.3237433614442227e-06,
      "loss": 2.4285,
      "step": 245200
    },
    {
      "epoch": 2.50423668252445,
      "grad_norm": 16.342504501342773,
      "learning_rate": 3.3169258458832436e-06,
      "loss": 2.4779,
      "step": 245300
    },
    {
      "epoch": 2.5052575698797392,
      "grad_norm": 17.81178855895996,
      "learning_rate": 3.310108330322264e-06,
      "loss": 2.4454,
      "step": 245400
    },
    {
      "epoch": 2.5062784572350285,
      "grad_norm": 15.560016632080078,
      "learning_rate": 3.3032908147612854e-06,
      "loss": 2.4885,
      "step": 245500
    },
    {
      "epoch": 2.507299344590318,
      "grad_norm": 15.436796188354492,
      "learning_rate": 3.296473299200306e-06,
      "loss": 2.4552,
      "step": 245600
    },
    {
      "epoch": 2.508320231945607,
      "grad_norm": 16.50242805480957,
      "learning_rate": 3.2896557836393267e-06,
      "loss": 2.4312,
      "step": 245700
    },
    {
      "epoch": 2.5093411193008963,
      "grad_norm": 17.523502349853516,
      "learning_rate": 3.282838268078347e-06,
      "loss": 2.489,
      "step": 245800
    },
    {
      "epoch": 2.5103620066561856,
      "grad_norm": 16.723581314086914,
      "learning_rate": 3.2760207525173677e-06,
      "loss": 2.3933,
      "step": 245900
    },
    {
      "epoch": 2.511382894011475,
      "grad_norm": 24.419532775878906,
      "learning_rate": 3.2692032369563886e-06,
      "loss": 2.484,
      "step": 246000
    },
    {
      "epoch": 2.512403781366764,
      "grad_norm": 12.822798728942871,
      "learning_rate": 3.262385721395409e-06,
      "loss": 2.5538,
      "step": 246100
    },
    {
      "epoch": 2.5134246687220534,
      "grad_norm": 14.752836227416992,
      "learning_rate": 3.25556820583443e-06,
      "loss": 2.4174,
      "step": 246200
    },
    {
      "epoch": 2.5144455560773427,
      "grad_norm": 20.838298797607422,
      "learning_rate": 3.2487506902734504e-06,
      "loss": 2.4123,
      "step": 246300
    },
    {
      "epoch": 2.5154664434326315,
      "grad_norm": 18.96034812927246,
      "learning_rate": 3.2419331747124717e-06,
      "loss": 2.4413,
      "step": 246400
    },
    {
      "epoch": 2.5164873307879208,
      "grad_norm": 15.009047508239746,
      "learning_rate": 3.235115659151492e-06,
      "loss": 2.3824,
      "step": 246500
    },
    {
      "epoch": 2.51750821814321,
      "grad_norm": 15.599255561828613,
      "learning_rate": 3.228298143590513e-06,
      "loss": 2.5085,
      "step": 246600
    },
    {
      "epoch": 2.5185291054984993,
      "grad_norm": 18.31998062133789,
      "learning_rate": 3.2214806280295335e-06,
      "loss": 2.4887,
      "step": 246700
    },
    {
      "epoch": 2.5195499928537886,
      "grad_norm": 18.21572494506836,
      "learning_rate": 3.2146631124685544e-06,
      "loss": 2.4755,
      "step": 246800
    },
    {
      "epoch": 2.520570880209078,
      "grad_norm": 14.930322647094727,
      "learning_rate": 3.207845596907575e-06,
      "loss": 2.4161,
      "step": 246900
    },
    {
      "epoch": 2.521591767564367,
      "grad_norm": 15.49997329711914,
      "learning_rate": 3.201028081346596e-06,
      "loss": 2.3716,
      "step": 247000
    },
    {
      "epoch": 2.522612654919656,
      "grad_norm": 15.321514129638672,
      "learning_rate": 3.1942105657856167e-06,
      "loss": 2.4232,
      "step": 247100
    },
    {
      "epoch": 2.523633542274945,
      "grad_norm": 16.7601261138916,
      "learning_rate": 3.1873930502246375e-06,
      "loss": 2.4405,
      "step": 247200
    },
    {
      "epoch": 2.5246544296302345,
      "grad_norm": 17.41434097290039,
      "learning_rate": 3.180575534663658e-06,
      "loss": 2.4564,
      "step": 247300
    },
    {
      "epoch": 2.5256753169855237,
      "grad_norm": 14.566454887390137,
      "learning_rate": 3.173758019102679e-06,
      "loss": 2.5558,
      "step": 247400
    },
    {
      "epoch": 2.526696204340813,
      "grad_norm": 16.428585052490234,
      "learning_rate": 3.1669405035416994e-06,
      "loss": 2.425,
      "step": 247500
    },
    {
      "epoch": 2.5277170916961023,
      "grad_norm": 23.117918014526367,
      "learning_rate": 3.16019116313633e-06,
      "loss": 2.4558,
      "step": 247600
    },
    {
      "epoch": 2.5287379790513915,
      "grad_norm": 15.80005168914795,
      "learning_rate": 3.1533736475753505e-06,
      "loss": 2.4748,
      "step": 247700
    },
    {
      "epoch": 2.529758866406681,
      "grad_norm": 14.675796508789062,
      "learning_rate": 3.146556132014372e-06,
      "loss": 2.3965,
      "step": 247800
    },
    {
      "epoch": 2.53077975376197,
      "grad_norm": 22.365285873413086,
      "learning_rate": 3.1397386164533923e-06,
      "loss": 2.4472,
      "step": 247900
    },
    {
      "epoch": 2.5318006411172593,
      "grad_norm": 17.010713577270508,
      "learning_rate": 3.132921100892413e-06,
      "loss": 2.5176,
      "step": 248000
    },
    {
      "epoch": 2.5328215284725486,
      "grad_norm": 17.837739944458008,
      "learning_rate": 3.1261035853314336e-06,
      "loss": 2.5115,
      "step": 248100
    },
    {
      "epoch": 2.5338424158278374,
      "grad_norm": 11.716019630432129,
      "learning_rate": 3.1192860697704545e-06,
      "loss": 2.4961,
      "step": 248200
    },
    {
      "epoch": 2.5348633031831267,
      "grad_norm": 15.672430038452148,
      "learning_rate": 3.112468554209475e-06,
      "loss": 2.4512,
      "step": 248300
    },
    {
      "epoch": 2.535884190538416,
      "grad_norm": 15.329938888549805,
      "learning_rate": 3.1056510386484963e-06,
      "loss": 2.4127,
      "step": 248400
    },
    {
      "epoch": 2.5369050778937052,
      "grad_norm": 16.867382049560547,
      "learning_rate": 3.0988335230875168e-06,
      "loss": 2.4316,
      "step": 248500
    },
    {
      "epoch": 2.5379259652489945,
      "grad_norm": 16.64664649963379,
      "learning_rate": 3.0920160075265376e-06,
      "loss": 2.4662,
      "step": 248600
    },
    {
      "epoch": 2.5389468526042838,
      "grad_norm": 21.399545669555664,
      "learning_rate": 3.085198491965558e-06,
      "loss": 2.5754,
      "step": 248700
    },
    {
      "epoch": 2.5399677399595726,
      "grad_norm": 17.460886001586914,
      "learning_rate": 3.078380976404579e-06,
      "loss": 2.4868,
      "step": 248800
    },
    {
      "epoch": 2.540988627314862,
      "grad_norm": 15.963179588317871,
      "learning_rate": 3.0715634608435995e-06,
      "loss": 2.3698,
      "step": 248900
    },
    {
      "epoch": 2.542009514670151,
      "grad_norm": 15.119538307189941,
      "learning_rate": 3.0647459452826208e-06,
      "loss": 2.4562,
      "step": 249000
    },
    {
      "epoch": 2.5430304020254404,
      "grad_norm": 19.171707153320312,
      "learning_rate": 3.0579284297216412e-06,
      "loss": 2.513,
      "step": 249100
    },
    {
      "epoch": 2.5440512893807297,
      "grad_norm": 21.341402053833008,
      "learning_rate": 3.051110914160662e-06,
      "loss": 2.4516,
      "step": 249200
    },
    {
      "epoch": 2.545072176736019,
      "grad_norm": 13.485434532165527,
      "learning_rate": 3.0442933985996826e-06,
      "loss": 2.3995,
      "step": 249300
    },
    {
      "epoch": 2.546093064091308,
      "grad_norm": 16.54074478149414,
      "learning_rate": 3.0374758830387035e-06,
      "loss": 2.4837,
      "step": 249400
    },
    {
      "epoch": 2.5471139514465975,
      "grad_norm": 22.15399169921875,
      "learning_rate": 3.030658367477724e-06,
      "loss": 2.4775,
      "step": 249500
    },
    {
      "epoch": 2.5481348388018867,
      "grad_norm": 15.216079711914062,
      "learning_rate": 3.0238408519167444e-06,
      "loss": 2.4922,
      "step": 249600
    },
    {
      "epoch": 2.549155726157176,
      "grad_norm": 21.148305892944336,
      "learning_rate": 3.0170233363557653e-06,
      "loss": 2.5088,
      "step": 249700
    },
    {
      "epoch": 2.5501766135124653,
      "grad_norm": 17.50261878967285,
      "learning_rate": 3.0102058207947858e-06,
      "loss": 2.4694,
      "step": 249800
    },
    {
      "epoch": 2.551197500867754,
      "grad_norm": 16.783493041992188,
      "learning_rate": 3.003388305233807e-06,
      "loss": 2.4135,
      "step": 249900
    },
    {
      "epoch": 2.5522183882230434,
      "grad_norm": 14.581963539123535,
      "learning_rate": 2.9965707896728275e-06,
      "loss": 2.506,
      "step": 250000
    },
    {
      "epoch": 2.5532392755783326,
      "grad_norm": 17.038761138916016,
      "learning_rate": 2.9897532741118484e-06,
      "loss": 2.4621,
      "step": 250100
    },
    {
      "epoch": 2.554260162933622,
      "grad_norm": 17.12959098815918,
      "learning_rate": 2.982935758550869e-06,
      "loss": 2.4459,
      "step": 250200
    },
    {
      "epoch": 2.555281050288911,
      "grad_norm": 25.01656723022461,
      "learning_rate": 2.97611824298989e-06,
      "loss": 2.3685,
      "step": 250300
    },
    {
      "epoch": 2.5563019376442004,
      "grad_norm": 15.846064567565918,
      "learning_rate": 2.9693007274289103e-06,
      "loss": 2.3964,
      "step": 250400
    },
    {
      "epoch": 2.5573228249994897,
      "grad_norm": 16.995338439941406,
      "learning_rate": 2.9624832118679316e-06,
      "loss": 2.4565,
      "step": 250500
    },
    {
      "epoch": 2.5583437123547785,
      "grad_norm": 15.986784934997559,
      "learning_rate": 2.955665696306952e-06,
      "loss": 2.4599,
      "step": 250600
    },
    {
      "epoch": 2.559364599710068,
      "grad_norm": 13.03303337097168,
      "learning_rate": 2.948848180745973e-06,
      "loss": 2.473,
      "step": 250700
    },
    {
      "epoch": 2.560385487065357,
      "grad_norm": 13.505849838256836,
      "learning_rate": 2.9420306651849934e-06,
      "loss": 2.4567,
      "step": 250800
    },
    {
      "epoch": 2.5614063744206463,
      "grad_norm": 14.275596618652344,
      "learning_rate": 2.9352131496240143e-06,
      "loss": 2.3858,
      "step": 250900
    },
    {
      "epoch": 2.5624272617759356,
      "grad_norm": 18.973114013671875,
      "learning_rate": 2.9283956340630347e-06,
      "loss": 2.4936,
      "step": 251000
    },
    {
      "epoch": 2.563448149131225,
      "grad_norm": 16.326107025146484,
      "learning_rate": 2.921578118502056e-06,
      "loss": 2.3857,
      "step": 251100
    },
    {
      "epoch": 2.564469036486514,
      "grad_norm": 16.42168426513672,
      "learning_rate": 2.914828778096686e-06,
      "loss": 2.5146,
      "step": 251200
    },
    {
      "epoch": 2.5654899238418034,
      "grad_norm": 16.647994995117188,
      "learning_rate": 2.908011262535707e-06,
      "loss": 2.4111,
      "step": 251300
    },
    {
      "epoch": 2.5665108111970927,
      "grad_norm": 17.642642974853516,
      "learning_rate": 2.9011937469747276e-06,
      "loss": 2.5289,
      "step": 251400
    },
    {
      "epoch": 2.567531698552382,
      "grad_norm": 19.876087188720703,
      "learning_rate": 2.8943762314137485e-06,
      "loss": 2.4813,
      "step": 251500
    },
    {
      "epoch": 2.568552585907671,
      "grad_norm": 19.468154907226562,
      "learning_rate": 2.887558715852769e-06,
      "loss": 2.4457,
      "step": 251600
    },
    {
      "epoch": 2.56957347326296,
      "grad_norm": 18.20332908630371,
      "learning_rate": 2.88074120029179e-06,
      "loss": 2.5025,
      "step": 251700
    },
    {
      "epoch": 2.5705943606182493,
      "grad_norm": 13.639196395874023,
      "learning_rate": 2.8739236847308104e-06,
      "loss": 2.4488,
      "step": 251800
    },
    {
      "epoch": 2.5716152479735386,
      "grad_norm": 19.00288200378418,
      "learning_rate": 2.8671061691698317e-06,
      "loss": 2.4663,
      "step": 251900
    },
    {
      "epoch": 2.572636135328828,
      "grad_norm": 14.566380500793457,
      "learning_rate": 2.860288653608852e-06,
      "loss": 2.4962,
      "step": 252000
    },
    {
      "epoch": 2.573657022684117,
      "grad_norm": 12.876618385314941,
      "learning_rate": 2.853471138047873e-06,
      "loss": 2.452,
      "step": 252100
    },
    {
      "epoch": 2.5746779100394064,
      "grad_norm": 15.849169731140137,
      "learning_rate": 2.8466536224868935e-06,
      "loss": 2.5089,
      "step": 252200
    },
    {
      "epoch": 2.575698797394695,
      "grad_norm": 13.660112380981445,
      "learning_rate": 2.8398361069259144e-06,
      "loss": 2.4856,
      "step": 252300
    },
    {
      "epoch": 2.5767196847499845,
      "grad_norm": 15.40715503692627,
      "learning_rate": 2.833018591364935e-06,
      "loss": 2.5198,
      "step": 252400
    },
    {
      "epoch": 2.5777405721052737,
      "grad_norm": 21.515342712402344,
      "learning_rate": 2.826201075803956e-06,
      "loss": 2.4461,
      "step": 252500
    },
    {
      "epoch": 2.578761459460563,
      "grad_norm": 15.18094539642334,
      "learning_rate": 2.8193835602429766e-06,
      "loss": 2.4774,
      "step": 252600
    },
    {
      "epoch": 2.5797823468158523,
      "grad_norm": 15.257437705993652,
      "learning_rate": 2.8125660446819975e-06,
      "loss": 2.5004,
      "step": 252700
    },
    {
      "epoch": 2.5808032341711415,
      "grad_norm": 18.577423095703125,
      "learning_rate": 2.805748529121018e-06,
      "loss": 2.5316,
      "step": 252800
    },
    {
      "epoch": 2.581824121526431,
      "grad_norm": 15.525350570678711,
      "learning_rate": 2.798931013560039e-06,
      "loss": 2.49,
      "step": 252900
    },
    {
      "epoch": 2.58284500888172,
      "grad_norm": 17.386789321899414,
      "learning_rate": 2.7921134979990593e-06,
      "loss": 2.4634,
      "step": 253000
    },
    {
      "epoch": 2.5838658962370094,
      "grad_norm": 15.044392585754395,
      "learning_rate": 2.7852959824380802e-06,
      "loss": 2.4272,
      "step": 253100
    },
    {
      "epoch": 2.5848867835922986,
      "grad_norm": 14.28875732421875,
      "learning_rate": 2.7784784668771007e-06,
      "loss": 2.4738,
      "step": 253200
    },
    {
      "epoch": 2.585907670947588,
      "grad_norm": 16.0881290435791,
      "learning_rate": 2.771660951316121e-06,
      "loss": 2.4727,
      "step": 253300
    },
    {
      "epoch": 2.5869285583028767,
      "grad_norm": 14.799158096313477,
      "learning_rate": 2.7648434357551425e-06,
      "loss": 2.4313,
      "step": 253400
    },
    {
      "epoch": 2.587949445658166,
      "grad_norm": 17.156946182250977,
      "learning_rate": 2.758025920194163e-06,
      "loss": 2.4153,
      "step": 253500
    },
    {
      "epoch": 2.5889703330134552,
      "grad_norm": 13.927801132202148,
      "learning_rate": 2.751208404633184e-06,
      "loss": 2.4213,
      "step": 253600
    },
    {
      "epoch": 2.5899912203687445,
      "grad_norm": 17.397369384765625,
      "learning_rate": 2.7443908890722043e-06,
      "loss": 2.4773,
      "step": 253700
    },
    {
      "epoch": 2.591012107724034,
      "grad_norm": 16.555057525634766,
      "learning_rate": 2.737641548666835e-06,
      "loss": 2.426,
      "step": 253800
    },
    {
      "epoch": 2.592032995079323,
      "grad_norm": 22.060508728027344,
      "learning_rate": 2.7308240331058554e-06,
      "loss": 2.38,
      "step": 253900
    },
    {
      "epoch": 2.5930538824346123,
      "grad_norm": 18.063594818115234,
      "learning_rate": 2.7240065175448767e-06,
      "loss": 2.4325,
      "step": 254000
    },
    {
      "epoch": 2.594074769789901,
      "grad_norm": 20.199682235717773,
      "learning_rate": 2.717189001983897e-06,
      "loss": 2.3647,
      "step": 254100
    },
    {
      "epoch": 2.5950956571451904,
      "grad_norm": 14.554524421691895,
      "learning_rate": 2.710371486422918e-06,
      "loss": 2.418,
      "step": 254200
    },
    {
      "epoch": 2.5961165445004797,
      "grad_norm": 13.137178421020508,
      "learning_rate": 2.7035539708619385e-06,
      "loss": 2.423,
      "step": 254300
    },
    {
      "epoch": 2.597137431855769,
      "grad_norm": 21.60310173034668,
      "learning_rate": 2.6967364553009594e-06,
      "loss": 2.4518,
      "step": 254400
    },
    {
      "epoch": 2.598158319211058,
      "grad_norm": 18.18025779724121,
      "learning_rate": 2.68991893973998e-06,
      "loss": 2.3983,
      "step": 254500
    },
    {
      "epoch": 2.5991792065663475,
      "grad_norm": 17.026063919067383,
      "learning_rate": 2.6831014241790008e-06,
      "loss": 2.4255,
      "step": 254600
    },
    {
      "epoch": 2.6002000939216368,
      "grad_norm": 12.075904846191406,
      "learning_rate": 2.6762839086180213e-06,
      "loss": 2.3596,
      "step": 254700
    },
    {
      "epoch": 2.601220981276926,
      "grad_norm": 15.060318946838379,
      "learning_rate": 2.6694663930570426e-06,
      "loss": 2.4496,
      "step": 254800
    },
    {
      "epoch": 2.6022418686322153,
      "grad_norm": 17.147493362426758,
      "learning_rate": 2.662648877496063e-06,
      "loss": 2.4866,
      "step": 254900
    },
    {
      "epoch": 2.6032627559875046,
      "grad_norm": 15.337367057800293,
      "learning_rate": 2.655831361935084e-06,
      "loss": 2.4013,
      "step": 255000
    },
    {
      "epoch": 2.604283643342794,
      "grad_norm": 12.933860778808594,
      "learning_rate": 2.6490138463741044e-06,
      "loss": 2.4544,
      "step": 255100
    },
    {
      "epoch": 2.6053045306980827,
      "grad_norm": 21.52128028869629,
      "learning_rate": 2.6421963308131253e-06,
      "loss": 2.4191,
      "step": 255200
    },
    {
      "epoch": 2.606325418053372,
      "grad_norm": 18.384599685668945,
      "learning_rate": 2.6353788152521457e-06,
      "loss": 2.493,
      "step": 255300
    },
    {
      "epoch": 2.607346305408661,
      "grad_norm": 15.419081687927246,
      "learning_rate": 2.628561299691167e-06,
      "loss": 2.4037,
      "step": 255400
    },
    {
      "epoch": 2.6083671927639505,
      "grad_norm": 15.486193656921387,
      "learning_rate": 2.6217437841301875e-06,
      "loss": 2.3839,
      "step": 255500
    },
    {
      "epoch": 2.6093880801192397,
      "grad_norm": 16.52037811279297,
      "learning_rate": 2.6149262685692084e-06,
      "loss": 2.3862,
      "step": 255600
    },
    {
      "epoch": 2.610408967474529,
      "grad_norm": 14.20993709564209,
      "learning_rate": 2.608108753008229e-06,
      "loss": 2.3949,
      "step": 255700
    },
    {
      "epoch": 2.611429854829818,
      "grad_norm": 14.260527610778809,
      "learning_rate": 2.6012912374472498e-06,
      "loss": 2.4305,
      "step": 255800
    },
    {
      "epoch": 2.612450742185107,
      "grad_norm": 16.331233978271484,
      "learning_rate": 2.5944737218862702e-06,
      "loss": 2.4626,
      "step": 255900
    },
    {
      "epoch": 2.6134716295403964,
      "grad_norm": 19.14990234375,
      "learning_rate": 2.5876562063252915e-06,
      "loss": 2.4563,
      "step": 256000
    },
    {
      "epoch": 2.6144925168956856,
      "grad_norm": 17.548831939697266,
      "learning_rate": 2.580838690764312e-06,
      "loss": 2.5026,
      "step": 256100
    },
    {
      "epoch": 2.615513404250975,
      "grad_norm": 20.0415096282959,
      "learning_rate": 2.574021175203333e-06,
      "loss": 2.4257,
      "step": 256200
    },
    {
      "epoch": 2.616534291606264,
      "grad_norm": 16.76551628112793,
      "learning_rate": 2.5672036596423534e-06,
      "loss": 2.4393,
      "step": 256300
    },
    {
      "epoch": 2.6175551789615534,
      "grad_norm": 14.35548210144043,
      "learning_rate": 2.5603861440813743e-06,
      "loss": 2.4719,
      "step": 256400
    },
    {
      "epoch": 2.6185760663168427,
      "grad_norm": 16.326013565063477,
      "learning_rate": 2.5535686285203947e-06,
      "loss": 2.4473,
      "step": 256500
    },
    {
      "epoch": 2.619596953672132,
      "grad_norm": 13.092453956604004,
      "learning_rate": 2.5467511129594156e-06,
      "loss": 2.4923,
      "step": 256600
    },
    {
      "epoch": 2.6206178410274212,
      "grad_norm": 17.28598403930664,
      "learning_rate": 2.539933597398436e-06,
      "loss": 2.5161,
      "step": 256700
    },
    {
      "epoch": 2.6216387283827105,
      "grad_norm": 20.526199340820312,
      "learning_rate": 2.5331160818374574e-06,
      "loss": 2.4908,
      "step": 256800
    },
    {
      "epoch": 2.6226596157379993,
      "grad_norm": 22.17719841003418,
      "learning_rate": 2.526298566276478e-06,
      "loss": 2.4079,
      "step": 256900
    },
    {
      "epoch": 2.6236805030932886,
      "grad_norm": 15.856280326843262,
      "learning_rate": 2.5194810507154983e-06,
      "loss": 2.4797,
      "step": 257000
    },
    {
      "epoch": 2.624701390448578,
      "grad_norm": 20.35711669921875,
      "learning_rate": 2.512663535154519e-06,
      "loss": 2.4293,
      "step": 257100
    },
    {
      "epoch": 2.625722277803867,
      "grad_norm": 15.651074409484863,
      "learning_rate": 2.5058460195935397e-06,
      "loss": 2.4642,
      "step": 257200
    },
    {
      "epoch": 2.6267431651591564,
      "grad_norm": 18.068233489990234,
      "learning_rate": 2.4990285040325606e-06,
      "loss": 2.5433,
      "step": 257300
    },
    {
      "epoch": 2.6277640525144457,
      "grad_norm": 16.85140609741211,
      "learning_rate": 2.4922109884715814e-06,
      "loss": 2.4748,
      "step": 257400
    },
    {
      "epoch": 2.628784939869735,
      "grad_norm": 15.560555458068848,
      "learning_rate": 2.4853934729106023e-06,
      "loss": 2.3706,
      "step": 257500
    },
    {
      "epoch": 2.6298058272250238,
      "grad_norm": 20.938562393188477,
      "learning_rate": 2.478575957349623e-06,
      "loss": 2.4633,
      "step": 257600
    },
    {
      "epoch": 2.630826714580313,
      "grad_norm": 15.677844047546387,
      "learning_rate": 2.4717584417886437e-06,
      "loss": 2.4511,
      "step": 257700
    },
    {
      "epoch": 2.6318476019356023,
      "grad_norm": 15.908628463745117,
      "learning_rate": 2.4649409262276646e-06,
      "loss": 2.4235,
      "step": 257800
    },
    {
      "epoch": 2.6328684892908916,
      "grad_norm": 14.895479202270508,
      "learning_rate": 2.458191585822295e-06,
      "loss": 2.4536,
      "step": 257900
    },
    {
      "epoch": 2.633889376646181,
      "grad_norm": 14.383353233337402,
      "learning_rate": 2.4513740702613157e-06,
      "loss": 2.4238,
      "step": 258000
    },
    {
      "epoch": 2.63491026400147,
      "grad_norm": 17.538536071777344,
      "learning_rate": 2.444556554700336e-06,
      "loss": 2.4298,
      "step": 258100
    },
    {
      "epoch": 2.6359311513567594,
      "grad_norm": 17.311077117919922,
      "learning_rate": 2.437739039139357e-06,
      "loss": 2.5314,
      "step": 258200
    },
    {
      "epoch": 2.6369520387120486,
      "grad_norm": 15.453913688659668,
      "learning_rate": 2.430921523578378e-06,
      "loss": 2.4623,
      "step": 258300
    },
    {
      "epoch": 2.637972926067338,
      "grad_norm": 18.40250587463379,
      "learning_rate": 2.4241040080173984e-06,
      "loss": 2.3873,
      "step": 258400
    },
    {
      "epoch": 2.638993813422627,
      "grad_norm": 18.989784240722656,
      "learning_rate": 2.4172864924564193e-06,
      "loss": 2.44,
      "step": 258500
    },
    {
      "epoch": 2.640014700777916,
      "grad_norm": 18.13758087158203,
      "learning_rate": 2.41046897689544e-06,
      "loss": 2.5005,
      "step": 258600
    },
    {
      "epoch": 2.6410355881332053,
      "grad_norm": 16.724246978759766,
      "learning_rate": 2.4036514613344607e-06,
      "loss": 2.4168,
      "step": 258700
    },
    {
      "epoch": 2.6420564754884945,
      "grad_norm": 23.492347717285156,
      "learning_rate": 2.3968339457734815e-06,
      "loss": 2.449,
      "step": 258800
    },
    {
      "epoch": 2.643077362843784,
      "grad_norm": 15.806678771972656,
      "learning_rate": 2.3900164302125024e-06,
      "loss": 2.5226,
      "step": 258900
    },
    {
      "epoch": 2.644098250199073,
      "grad_norm": 18.372554779052734,
      "learning_rate": 2.3832670898071327e-06,
      "loss": 2.4421,
      "step": 259000
    },
    {
      "epoch": 2.6451191375543623,
      "grad_norm": 19.303640365600586,
      "learning_rate": 2.3764495742461536e-06,
      "loss": 2.4437,
      "step": 259100
    },
    {
      "epoch": 2.6461400249096516,
      "grad_norm": 16.929412841796875,
      "learning_rate": 2.3696320586851745e-06,
      "loss": 2.4844,
      "step": 259200
    },
    {
      "epoch": 2.6471609122649404,
      "grad_norm": 20.02363395690918,
      "learning_rate": 2.362814543124195e-06,
      "loss": 2.5462,
      "step": 259300
    },
    {
      "epoch": 2.6481817996202297,
      "grad_norm": 20.891979217529297,
      "learning_rate": 2.355997027563216e-06,
      "loss": 2.4152,
      "step": 259400
    },
    {
      "epoch": 2.649202686975519,
      "grad_norm": 16.052453994750977,
      "learning_rate": 2.3491795120022363e-06,
      "loss": 2.4122,
      "step": 259500
    },
    {
      "epoch": 2.6502235743308082,
      "grad_norm": 17.673505783081055,
      "learning_rate": 2.3423619964412567e-06,
      "loss": 2.4811,
      "step": 259600
    },
    {
      "epoch": 2.6512444616860975,
      "grad_norm": 16.787206649780273,
      "learning_rate": 2.3355444808802776e-06,
      "loss": 2.4308,
      "step": 259700
    },
    {
      "epoch": 2.6522653490413868,
      "grad_norm": 16.56566047668457,
      "learning_rate": 2.3287269653192985e-06,
      "loss": 2.4326,
      "step": 259800
    },
    {
      "epoch": 2.653286236396676,
      "grad_norm": 16.855133056640625,
      "learning_rate": 2.321909449758319e-06,
      "loss": 2.4028,
      "step": 259900
    },
    {
      "epoch": 2.6543071237519653,
      "grad_norm": 18.038305282592773,
      "learning_rate": 2.31509193419734e-06,
      "loss": 2.467,
      "step": 260000
    },
    {
      "epoch": 2.6553280111072546,
      "grad_norm": 16.099205017089844,
      "learning_rate": 2.3082744186363608e-06,
      "loss": 2.3979,
      "step": 260100
    },
    {
      "epoch": 2.656348898462544,
      "grad_norm": 16.263044357299805,
      "learning_rate": 2.3014569030753812e-06,
      "loss": 2.4849,
      "step": 260200
    },
    {
      "epoch": 2.657369785817833,
      "grad_norm": 18.594144821166992,
      "learning_rate": 2.294639387514402e-06,
      "loss": 2.4481,
      "step": 260300
    },
    {
      "epoch": 2.658390673173122,
      "grad_norm": 19.017332077026367,
      "learning_rate": 2.287821871953423e-06,
      "loss": 2.4227,
      "step": 260400
    },
    {
      "epoch": 2.659411560528411,
      "grad_norm": 13.801462173461914,
      "learning_rate": 2.2810043563924435e-06,
      "loss": 2.4568,
      "step": 260500
    },
    {
      "epoch": 2.6604324478837005,
      "grad_norm": 13.740234375,
      "learning_rate": 2.2741868408314644e-06,
      "loss": 2.4753,
      "step": 260600
    },
    {
      "epoch": 2.6614533352389897,
      "grad_norm": 18.0158748626709,
      "learning_rate": 2.2673693252704852e-06,
      "loss": 2.5005,
      "step": 260700
    },
    {
      "epoch": 2.662474222594279,
      "grad_norm": 18.437402725219727,
      "learning_rate": 2.2605518097095057e-06,
      "loss": 2.5189,
      "step": 260800
    },
    {
      "epoch": 2.6634951099495683,
      "grad_norm": 18.751602172851562,
      "learning_rate": 2.2537342941485266e-06,
      "loss": 2.4241,
      "step": 260900
    },
    {
      "epoch": 2.664515997304857,
      "grad_norm": 15.659122467041016,
      "learning_rate": 2.2469167785875475e-06,
      "loss": 2.3459,
      "step": 261000
    },
    {
      "epoch": 2.6655368846601464,
      "grad_norm": 16.259994506835938,
      "learning_rate": 2.2401674381821777e-06,
      "loss": 2.3971,
      "step": 261100
    },
    {
      "epoch": 2.6665577720154356,
      "grad_norm": 14.052949905395508,
      "learning_rate": 2.2333499226211986e-06,
      "loss": 2.4753,
      "step": 261200
    },
    {
      "epoch": 2.667578659370725,
      "grad_norm": 19.124975204467773,
      "learning_rate": 2.226532407060219e-06,
      "loss": 2.4694,
      "step": 261300
    },
    {
      "epoch": 2.668599546726014,
      "grad_norm": 15.426166534423828,
      "learning_rate": 2.21971489149924e-06,
      "loss": 2.5007,
      "step": 261400
    },
    {
      "epoch": 2.6696204340813035,
      "grad_norm": 18.717517852783203,
      "learning_rate": 2.212897375938261e-06,
      "loss": 2.5357,
      "step": 261500
    },
    {
      "epoch": 2.6706413214365927,
      "grad_norm": 20.163679122924805,
      "learning_rate": 2.2060798603772813e-06,
      "loss": 2.4339,
      "step": 261600
    },
    {
      "epoch": 2.671662208791882,
      "grad_norm": 18.200883865356445,
      "learning_rate": 2.1992623448163022e-06,
      "loss": 2.3577,
      "step": 261700
    },
    {
      "epoch": 2.6726830961471713,
      "grad_norm": 23.94089698791504,
      "learning_rate": 2.192444829255323e-06,
      "loss": 2.4711,
      "step": 261800
    },
    {
      "epoch": 2.6737039835024605,
      "grad_norm": 22.20125961303711,
      "learning_rate": 2.1856273136943436e-06,
      "loss": 2.4811,
      "step": 261900
    },
    {
      "epoch": 2.67472487085775,
      "grad_norm": 18.352169036865234,
      "learning_rate": 2.1788097981333645e-06,
      "loss": 2.5123,
      "step": 262000
    },
    {
      "epoch": 2.6757457582130386,
      "grad_norm": 21.141891479492188,
      "learning_rate": 2.1719922825723853e-06,
      "loss": 2.4598,
      "step": 262100
    },
    {
      "epoch": 2.676766645568328,
      "grad_norm": 23.114660263061523,
      "learning_rate": 2.165174767011406e-06,
      "loss": 2.4729,
      "step": 262200
    },
    {
      "epoch": 2.677787532923617,
      "grad_norm": 19.05756187438965,
      "learning_rate": 2.1583572514504267e-06,
      "loss": 2.4333,
      "step": 262300
    },
    {
      "epoch": 2.6788084202789064,
      "grad_norm": 17.12965965270996,
      "learning_rate": 2.1515397358894476e-06,
      "loss": 2.4652,
      "step": 262400
    },
    {
      "epoch": 2.6798293076341957,
      "grad_norm": 16.64082908630371,
      "learning_rate": 2.144722220328468e-06,
      "loss": 2.428,
      "step": 262500
    },
    {
      "epoch": 2.680850194989485,
      "grad_norm": 15.276872634887695,
      "learning_rate": 2.137904704767489e-06,
      "loss": 2.4399,
      "step": 262600
    },
    {
      "epoch": 2.6818710823447742,
      "grad_norm": 13.413046836853027,
      "learning_rate": 2.13108718920651e-06,
      "loss": 2.4244,
      "step": 262700
    },
    {
      "epoch": 2.682891969700063,
      "grad_norm": 19.286348342895508,
      "learning_rate": 2.1242696736455303e-06,
      "loss": 2.4791,
      "step": 262800
    },
    {
      "epoch": 2.6839128570553523,
      "grad_norm": 13.279884338378906,
      "learning_rate": 2.117452158084551e-06,
      "loss": 2.4539,
      "step": 262900
    },
    {
      "epoch": 2.6849337444106416,
      "grad_norm": 12.999993324279785,
      "learning_rate": 2.1106346425235717e-06,
      "loss": 2.4553,
      "step": 263000
    },
    {
      "epoch": 2.685954631765931,
      "grad_norm": 17.436166763305664,
      "learning_rate": 2.103817126962592e-06,
      "loss": 2.4105,
      "step": 263100
    },
    {
      "epoch": 2.68697551912122,
      "grad_norm": 15.925204277038574,
      "learning_rate": 2.096999611401613e-06,
      "loss": 2.4699,
      "step": 263200
    },
    {
      "epoch": 2.6879964064765094,
      "grad_norm": 15.630584716796875,
      "learning_rate": 2.090182095840634e-06,
      "loss": 2.4146,
      "step": 263300
    },
    {
      "epoch": 2.6890172938317987,
      "grad_norm": 14.426246643066406,
      "learning_rate": 2.0834327554352646e-06,
      "loss": 2.4322,
      "step": 263400
    },
    {
      "epoch": 2.690038181187088,
      "grad_norm": 22.669553756713867,
      "learning_rate": 2.0766152398742854e-06,
      "loss": 2.5016,
      "step": 263500
    },
    {
      "epoch": 2.691059068542377,
      "grad_norm": 19.671165466308594,
      "learning_rate": 2.069797724313306e-06,
      "loss": 2.4232,
      "step": 263600
    },
    {
      "epoch": 2.6920799558976665,
      "grad_norm": 16.139265060424805,
      "learning_rate": 2.0629802087523264e-06,
      "loss": 2.4536,
      "step": 263700
    },
    {
      "epoch": 2.6931008432529557,
      "grad_norm": 15.714768409729004,
      "learning_rate": 2.0561626931913473e-06,
      "loss": 2.5229,
      "step": 263800
    },
    {
      "epoch": 2.6941217306082446,
      "grad_norm": 14.714828491210938,
      "learning_rate": 2.049345177630368e-06,
      "loss": 2.4028,
      "step": 263900
    },
    {
      "epoch": 2.695142617963534,
      "grad_norm": 21.65608787536621,
      "learning_rate": 2.0425276620693886e-06,
      "loss": 2.4953,
      "step": 264000
    },
    {
      "epoch": 2.696163505318823,
      "grad_norm": 18.104284286499023,
      "learning_rate": 2.0357783216640193e-06,
      "loss": 2.4978,
      "step": 264100
    },
    {
      "epoch": 2.6971843926741124,
      "grad_norm": 18.682065963745117,
      "learning_rate": 2.02896080610304e-06,
      "loss": 2.5003,
      "step": 264200
    },
    {
      "epoch": 2.6982052800294016,
      "grad_norm": 12.621957778930664,
      "learning_rate": 2.0221432905420606e-06,
      "loss": 2.4791,
      "step": 264300
    },
    {
      "epoch": 2.699226167384691,
      "grad_norm": 21.501598358154297,
      "learning_rate": 2.0153257749810815e-06,
      "loss": 2.4501,
      "step": 264400
    },
    {
      "epoch": 2.7002470547399797,
      "grad_norm": 15.617094039916992,
      "learning_rate": 2.008508259420102e-06,
      "loss": 2.5754,
      "step": 264500
    },
    {
      "epoch": 2.701267942095269,
      "grad_norm": 13.761543273925781,
      "learning_rate": 2.001690743859123e-06,
      "loss": 2.4708,
      "step": 264600
    },
    {
      "epoch": 2.7022888294505583,
      "grad_norm": 21.410579681396484,
      "learning_rate": 1.9948732282981438e-06,
      "loss": 2.3995,
      "step": 264700
    },
    {
      "epoch": 2.7033097168058475,
      "grad_norm": 17.765335083007812,
      "learning_rate": 1.9880557127371642e-06,
      "loss": 2.4655,
      "step": 264800
    },
    {
      "epoch": 2.704330604161137,
      "grad_norm": 16.698301315307617,
      "learning_rate": 1.981238197176185e-06,
      "loss": 2.4138,
      "step": 264900
    },
    {
      "epoch": 2.705351491516426,
      "grad_norm": 20.32051658630371,
      "learning_rate": 1.974420681615206e-06,
      "loss": 2.4413,
      "step": 265000
    },
    {
      "epoch": 2.7063723788717153,
      "grad_norm": 17.082597732543945,
      "learning_rate": 1.9676031660542265e-06,
      "loss": 2.434,
      "step": 265100
    },
    {
      "epoch": 2.7073932662270046,
      "grad_norm": 16.341806411743164,
      "learning_rate": 1.9607856504932474e-06,
      "loss": 2.5197,
      "step": 265200
    },
    {
      "epoch": 2.708414153582294,
      "grad_norm": 11.978425979614258,
      "learning_rate": 1.9539681349322683e-06,
      "loss": 2.4205,
      "step": 265300
    },
    {
      "epoch": 2.709435040937583,
      "grad_norm": 17.58669662475586,
      "learning_rate": 1.9471506193712887e-06,
      "loss": 2.4651,
      "step": 265400
    },
    {
      "epoch": 2.7104559282928724,
      "grad_norm": 16.981361389160156,
      "learning_rate": 1.9403331038103096e-06,
      "loss": 2.4529,
      "step": 265500
    },
    {
      "epoch": 2.7114768156481612,
      "grad_norm": 19.30035400390625,
      "learning_rate": 1.9335155882493305e-06,
      "loss": 2.4406,
      "step": 265600
    },
    {
      "epoch": 2.7124977030034505,
      "grad_norm": 16.829742431640625,
      "learning_rate": 1.926698072688351e-06,
      "loss": 2.3861,
      "step": 265700
    },
    {
      "epoch": 2.7135185903587398,
      "grad_norm": 17.621780395507812,
      "learning_rate": 1.919880557127372e-06,
      "loss": 2.4129,
      "step": 265800
    },
    {
      "epoch": 2.714539477714029,
      "grad_norm": 17.22836685180664,
      "learning_rate": 1.9130630415663927e-06,
      "loss": 2.3408,
      "step": 265900
    },
    {
      "epoch": 2.7155603650693183,
      "grad_norm": 14.40985107421875,
      "learning_rate": 1.9062455260054132e-06,
      "loss": 2.4781,
      "step": 266000
    },
    {
      "epoch": 2.7165812524246076,
      "grad_norm": 16.340227127075195,
      "learning_rate": 1.899428010444434e-06,
      "loss": 2.5246,
      "step": 266100
    },
    {
      "epoch": 2.717602139779897,
      "grad_norm": 20.925565719604492,
      "learning_rate": 1.8926104948834548e-06,
      "loss": 2.4974,
      "step": 266200
    },
    {
      "epoch": 2.7186230271351857,
      "grad_norm": 20.41000747680664,
      "learning_rate": 1.8857929793224755e-06,
      "loss": 2.4928,
      "step": 266300
    },
    {
      "epoch": 2.719643914490475,
      "grad_norm": 22.64737892150879,
      "learning_rate": 1.8789754637614963e-06,
      "loss": 2.502,
      "step": 266400
    },
    {
      "epoch": 2.720664801845764,
      "grad_norm": 23.40280532836914,
      "learning_rate": 1.872157948200517e-06,
      "loss": 2.4934,
      "step": 266500
    },
    {
      "epoch": 2.7216856892010535,
      "grad_norm": 18.4329833984375,
      "learning_rate": 1.8653404326395377e-06,
      "loss": 2.3975,
      "step": 266600
    },
    {
      "epoch": 2.7227065765563427,
      "grad_norm": 12.90687084197998,
      "learning_rate": 1.8585229170785584e-06,
      "loss": 2.5314,
      "step": 266700
    },
    {
      "epoch": 2.723727463911632,
      "grad_norm": 14.264850616455078,
      "learning_rate": 1.8517054015175793e-06,
      "loss": 2.4106,
      "step": 266800
    },
    {
      "epoch": 2.7247483512669213,
      "grad_norm": 16.429540634155273,
      "learning_rate": 1.8448878859566e-06,
      "loss": 2.3781,
      "step": 266900
    },
    {
      "epoch": 2.7257692386222105,
      "grad_norm": 19.460906982421875,
      "learning_rate": 1.8380703703956206e-06,
      "loss": 2.4842,
      "step": 267000
    },
    {
      "epoch": 2.7267901259775,
      "grad_norm": 16.595741271972656,
      "learning_rate": 1.8312528548346415e-06,
      "loss": 2.4259,
      "step": 267100
    },
    {
      "epoch": 2.727811013332789,
      "grad_norm": 16.576583862304688,
      "learning_rate": 1.8244353392736622e-06,
      "loss": 2.5102,
      "step": 267200
    },
    {
      "epoch": 2.7288319006880783,
      "grad_norm": 19.05982780456543,
      "learning_rate": 1.8176178237126827e-06,
      "loss": 2.4693,
      "step": 267300
    },
    {
      "epoch": 2.729852788043367,
      "grad_norm": 15.96933364868164,
      "learning_rate": 1.8108003081517033e-06,
      "loss": 2.5395,
      "step": 267400
    },
    {
      "epoch": 2.7308736753986564,
      "grad_norm": 14.047759056091309,
      "learning_rate": 1.8039827925907242e-06,
      "loss": 2.5365,
      "step": 267500
    },
    {
      "epoch": 2.7318945627539457,
      "grad_norm": 18.357738494873047,
      "learning_rate": 1.797165277029745e-06,
      "loss": 2.4799,
      "step": 267600
    },
    {
      "epoch": 2.732915450109235,
      "grad_norm": 15.83919906616211,
      "learning_rate": 1.7903477614687656e-06,
      "loss": 2.3849,
      "step": 267700
    },
    {
      "epoch": 2.7339363374645242,
      "grad_norm": 11.300463676452637,
      "learning_rate": 1.7835302459077863e-06,
      "loss": 2.463,
      "step": 267800
    },
    {
      "epoch": 2.7349572248198135,
      "grad_norm": 16.037853240966797,
      "learning_rate": 1.7767127303468071e-06,
      "loss": 2.4904,
      "step": 267900
    },
    {
      "epoch": 2.7359781121751023,
      "grad_norm": 18.595064163208008,
      "learning_rate": 1.7698952147858278e-06,
      "loss": 2.4064,
      "step": 268000
    },
    {
      "epoch": 2.7369989995303916,
      "grad_norm": 15.311031341552734,
      "learning_rate": 1.7630776992248485e-06,
      "loss": 2.5369,
      "step": 268100
    },
    {
      "epoch": 2.738019886885681,
      "grad_norm": 16.794078826904297,
      "learning_rate": 1.7562601836638694e-06,
      "loss": 2.3933,
      "step": 268200
    },
    {
      "epoch": 2.73904077424097,
      "grad_norm": 17.927818298339844,
      "learning_rate": 1.74944266810289e-06,
      "loss": 2.3662,
      "step": 268300
    },
    {
      "epoch": 2.7400616615962594,
      "grad_norm": 16.516115188598633,
      "learning_rate": 1.7426251525419107e-06,
      "loss": 2.4185,
      "step": 268400
    },
    {
      "epoch": 2.7410825489515487,
      "grad_norm": 13.158591270446777,
      "learning_rate": 1.7358758121365412e-06,
      "loss": 2.5166,
      "step": 268500
    },
    {
      "epoch": 2.742103436306838,
      "grad_norm": 13.583525657653809,
      "learning_rate": 1.729058296575562e-06,
      "loss": 2.4765,
      "step": 268600
    },
    {
      "epoch": 2.743124323662127,
      "grad_norm": 14.76695442199707,
      "learning_rate": 1.7222407810145828e-06,
      "loss": 2.3697,
      "step": 268700
    },
    {
      "epoch": 2.7441452110174165,
      "grad_norm": 15.446078300476074,
      "learning_rate": 1.7154232654536034e-06,
      "loss": 2.4,
      "step": 268800
    },
    {
      "epoch": 2.7451660983727058,
      "grad_norm": 17.106008529663086,
      "learning_rate": 1.7086057498926243e-06,
      "loss": 2.4674,
      "step": 268900
    },
    {
      "epoch": 2.746186985727995,
      "grad_norm": 17.26306915283203,
      "learning_rate": 1.701788234331645e-06,
      "loss": 2.4189,
      "step": 269000
    },
    {
      "epoch": 2.747207873083284,
      "grad_norm": 22.986452102661133,
      "learning_rate": 1.6949707187706657e-06,
      "loss": 2.4385,
      "step": 269100
    },
    {
      "epoch": 2.748228760438573,
      "grad_norm": 12.847268104553223,
      "learning_rate": 1.6881532032096864e-06,
      "loss": 2.4555,
      "step": 269200
    },
    {
      "epoch": 2.7492496477938624,
      "grad_norm": 17.749305725097656,
      "learning_rate": 1.6813356876487072e-06,
      "loss": 2.4106,
      "step": 269300
    },
    {
      "epoch": 2.7502705351491517,
      "grad_norm": 15.05429744720459,
      "learning_rate": 1.674518172087728e-06,
      "loss": 2.4606,
      "step": 269400
    },
    {
      "epoch": 2.751291422504441,
      "grad_norm": 27.147974014282227,
      "learning_rate": 1.6677006565267486e-06,
      "loss": 2.5379,
      "step": 269500
    },
    {
      "epoch": 2.75231230985973,
      "grad_norm": 18.510461807250977,
      "learning_rate": 1.6608831409657695e-06,
      "loss": 2.4195,
      "step": 269600
    },
    {
      "epoch": 2.7533331972150195,
      "grad_norm": 22.25747299194336,
      "learning_rate": 1.6540656254047902e-06,
      "loss": 2.4599,
      "step": 269700
    },
    {
      "epoch": 2.7543540845703083,
      "grad_norm": 18.35601234436035,
      "learning_rate": 1.6472481098438108e-06,
      "loss": 2.5554,
      "step": 269800
    },
    {
      "epoch": 2.7553749719255975,
      "grad_norm": 15.118430137634277,
      "learning_rate": 1.6404305942828317e-06,
      "loss": 2.4724,
      "step": 269900
    },
    {
      "epoch": 2.756395859280887,
      "grad_norm": 18.888465881347656,
      "learning_rate": 1.6336130787218524e-06,
      "loss": 2.4971,
      "step": 270000
    },
    {
      "epoch": 2.757416746636176,
      "grad_norm": 14.544097900390625,
      "learning_rate": 1.626795563160873e-06,
      "loss": 2.4661,
      "step": 270100
    },
    {
      "epoch": 2.7584376339914654,
      "grad_norm": 15.104937553405762,
      "learning_rate": 1.6199780475998938e-06,
      "loss": 2.5065,
      "step": 270200
    },
    {
      "epoch": 2.7594585213467546,
      "grad_norm": 16.150848388671875,
      "learning_rate": 1.6131605320389147e-06,
      "loss": 2.3905,
      "step": 270300
    },
    {
      "epoch": 2.760479408702044,
      "grad_norm": 15.455398559570312,
      "learning_rate": 1.6063430164779353e-06,
      "loss": 2.6009,
      "step": 270400
    },
    {
      "epoch": 2.761500296057333,
      "grad_norm": 16.852243423461914,
      "learning_rate": 1.599525500916956e-06,
      "loss": 2.4663,
      "step": 270500
    },
    {
      "epoch": 2.7625211834126224,
      "grad_norm": 22.789445877075195,
      "learning_rate": 1.5927761605115867e-06,
      "loss": 2.3371,
      "step": 270600
    },
    {
      "epoch": 2.7635420707679117,
      "grad_norm": 20.144947052001953,
      "learning_rate": 1.5859586449506073e-06,
      "loss": 2.5303,
      "step": 270700
    },
    {
      "epoch": 2.7645629581232005,
      "grad_norm": 20.52823829650879,
      "learning_rate": 1.579141129389628e-06,
      "loss": 2.5123,
      "step": 270800
    },
    {
      "epoch": 2.76558384547849,
      "grad_norm": 14.595736503601074,
      "learning_rate": 1.5723236138286487e-06,
      "loss": 2.4886,
      "step": 270900
    },
    {
      "epoch": 2.766604732833779,
      "grad_norm": 17.680307388305664,
      "learning_rate": 1.5655060982676696e-06,
      "loss": 2.4897,
      "step": 271000
    },
    {
      "epoch": 2.7676256201890683,
      "grad_norm": 13.194381713867188,
      "learning_rate": 1.5586885827066903e-06,
      "loss": 2.4483,
      "step": 271100
    },
    {
      "epoch": 2.7686465075443576,
      "grad_norm": 18.28228187561035,
      "learning_rate": 1.551871067145711e-06,
      "loss": 2.4381,
      "step": 271200
    },
    {
      "epoch": 2.769667394899647,
      "grad_norm": 15.761902809143066,
      "learning_rate": 1.5450535515847318e-06,
      "loss": 2.452,
      "step": 271300
    },
    {
      "epoch": 2.770688282254936,
      "grad_norm": 20.246652603149414,
      "learning_rate": 1.5382360360237525e-06,
      "loss": 2.5481,
      "step": 271400
    },
    {
      "epoch": 2.771709169610225,
      "grad_norm": 18.216907501220703,
      "learning_rate": 1.531418520462773e-06,
      "loss": 2.6027,
      "step": 271500
    },
    {
      "epoch": 2.7727300569655142,
      "grad_norm": 17.368438720703125,
      "learning_rate": 1.5246010049017936e-06,
      "loss": 2.4514,
      "step": 271600
    },
    {
      "epoch": 2.7737509443208035,
      "grad_norm": 20.652145385742188,
      "learning_rate": 1.5177834893408143e-06,
      "loss": 2.4721,
      "step": 271700
    },
    {
      "epoch": 2.7747718316760928,
      "grad_norm": 17.147809982299805,
      "learning_rate": 1.5109659737798352e-06,
      "loss": 2.4647,
      "step": 271800
    },
    {
      "epoch": 2.775792719031382,
      "grad_norm": 12.192934036254883,
      "learning_rate": 1.5041484582188559e-06,
      "loss": 2.3749,
      "step": 271900
    },
    {
      "epoch": 2.7768136063866713,
      "grad_norm": 17.277265548706055,
      "learning_rate": 1.4973309426578766e-06,
      "loss": 2.4944,
      "step": 272000
    },
    {
      "epoch": 2.7778344937419606,
      "grad_norm": 16.171533584594727,
      "learning_rate": 1.4905134270968975e-06,
      "loss": 2.4621,
      "step": 272100
    },
    {
      "epoch": 2.77885538109725,
      "grad_norm": 18.397884368896484,
      "learning_rate": 1.4836959115359181e-06,
      "loss": 2.4397,
      "step": 272200
    },
    {
      "epoch": 2.779876268452539,
      "grad_norm": 17.559188842773438,
      "learning_rate": 1.4768783959749388e-06,
      "loss": 2.4249,
      "step": 272300
    },
    {
      "epoch": 2.7808971558078284,
      "grad_norm": 16.607297897338867,
      "learning_rate": 1.4700608804139597e-06,
      "loss": 2.3995,
      "step": 272400
    },
    {
      "epoch": 2.7819180431631176,
      "grad_norm": 13.9883394241333,
      "learning_rate": 1.4632433648529804e-06,
      "loss": 2.4832,
      "step": 272500
    },
    {
      "epoch": 2.7829389305184065,
      "grad_norm": 16.090267181396484,
      "learning_rate": 1.456425849292001e-06,
      "loss": 2.4318,
      "step": 272600
    },
    {
      "epoch": 2.7839598178736957,
      "grad_norm": 20.099485397338867,
      "learning_rate": 1.4496083337310217e-06,
      "loss": 2.4929,
      "step": 272700
    },
    {
      "epoch": 2.784980705228985,
      "grad_norm": 18.69122314453125,
      "learning_rate": 1.4427908181700426e-06,
      "loss": 2.4851,
      "step": 272800
    },
    {
      "epoch": 2.7860015925842743,
      "grad_norm": 22.48435401916504,
      "learning_rate": 1.4359733026090633e-06,
      "loss": 2.4546,
      "step": 272900
    },
    {
      "epoch": 2.7870224799395635,
      "grad_norm": 18.356895446777344,
      "learning_rate": 1.429155787048084e-06,
      "loss": 2.4462,
      "step": 273000
    },
    {
      "epoch": 2.788043367294853,
      "grad_norm": 19.00990867614746,
      "learning_rate": 1.4224064466427146e-06,
      "loss": 2.4536,
      "step": 273100
    },
    {
      "epoch": 2.7890642546501416,
      "grad_norm": 16.549711227416992,
      "learning_rate": 1.4155889310817353e-06,
      "loss": 2.4526,
      "step": 273200
    },
    {
      "epoch": 2.790085142005431,
      "grad_norm": 16.582500457763672,
      "learning_rate": 1.408771415520756e-06,
      "loss": 2.5866,
      "step": 273300
    },
    {
      "epoch": 2.79110602936072,
      "grad_norm": 15.91710376739502,
      "learning_rate": 1.4019538999597767e-06,
      "loss": 2.4257,
      "step": 273400
    },
    {
      "epoch": 2.7921269167160094,
      "grad_norm": 16.649965286254883,
      "learning_rate": 1.3951363843987976e-06,
      "loss": 2.458,
      "step": 273500
    },
    {
      "epoch": 2.7931478040712987,
      "grad_norm": 16.68878936767578,
      "learning_rate": 1.3883188688378182e-06,
      "loss": 2.5067,
      "step": 273600
    },
    {
      "epoch": 2.794168691426588,
      "grad_norm": 13.862386703491211,
      "learning_rate": 1.381501353276839e-06,
      "loss": 2.4112,
      "step": 273700
    },
    {
      "epoch": 2.7951895787818772,
      "grad_norm": 12.897029876708984,
      "learning_rate": 1.3746838377158598e-06,
      "loss": 2.4733,
      "step": 273800
    },
    {
      "epoch": 2.7962104661371665,
      "grad_norm": 13.725197792053223,
      "learning_rate": 1.3678663221548805e-06,
      "loss": 2.4092,
      "step": 273900
    },
    {
      "epoch": 2.7972313534924558,
      "grad_norm": 18.24995231628418,
      "learning_rate": 1.3610488065939012e-06,
      "loss": 2.4287,
      "step": 274000
    },
    {
      "epoch": 2.798252240847745,
      "grad_norm": 19.35311508178711,
      "learning_rate": 1.354231291032922e-06,
      "loss": 2.3698,
      "step": 274100
    },
    {
      "epoch": 2.7992731282030343,
      "grad_norm": 13.631929397583008,
      "learning_rate": 1.3474137754719427e-06,
      "loss": 2.4471,
      "step": 274200
    },
    {
      "epoch": 2.800294015558323,
      "grad_norm": 14.97031021118164,
      "learning_rate": 1.3405962599109634e-06,
      "loss": 2.578,
      "step": 274300
    },
    {
      "epoch": 2.8013149029136124,
      "grad_norm": 19.98737144470215,
      "learning_rate": 1.333778744349984e-06,
      "loss": 2.3796,
      "step": 274400
    },
    {
      "epoch": 2.8023357902689017,
      "grad_norm": 16.141206741333008,
      "learning_rate": 1.326961228789005e-06,
      "loss": 2.4524,
      "step": 274500
    },
    {
      "epoch": 2.803356677624191,
      "grad_norm": 17.84302520751953,
      "learning_rate": 1.3201437132280256e-06,
      "loss": 2.4614,
      "step": 274600
    },
    {
      "epoch": 2.80437756497948,
      "grad_norm": 16.3502254486084,
      "learning_rate": 1.3133261976670463e-06,
      "loss": 2.4965,
      "step": 274700
    },
    {
      "epoch": 2.8053984523347695,
      "grad_norm": 20.141185760498047,
      "learning_rate": 1.3065086821060672e-06,
      "loss": 2.5171,
      "step": 274800
    },
    {
      "epoch": 2.8064193396900587,
      "grad_norm": 18.004453659057617,
      "learning_rate": 1.2996911665450879e-06,
      "loss": 2.4772,
      "step": 274900
    },
    {
      "epoch": 2.8074402270453476,
      "grad_norm": 14.697667121887207,
      "learning_rate": 1.2928736509841086e-06,
      "loss": 2.4472,
      "step": 275000
    },
    {
      "epoch": 2.808461114400637,
      "grad_norm": 16.73862075805664,
      "learning_rate": 1.286056135423129e-06,
      "loss": 2.3847,
      "step": 275100
    },
    {
      "epoch": 2.809482001755926,
      "grad_norm": 15.063079833984375,
      "learning_rate": 1.27930679501776e-06,
      "loss": 2.4132,
      "step": 275200
    },
    {
      "epoch": 2.8105028891112154,
      "grad_norm": 18.763103485107422,
      "learning_rate": 1.2724892794567806e-06,
      "loss": 2.4775,
      "step": 275300
    },
    {
      "epoch": 2.8115237764665046,
      "grad_norm": 13.726439476013184,
      "learning_rate": 1.2656717638958013e-06,
      "loss": 2.4357,
      "step": 275400
    },
    {
      "epoch": 2.812544663821794,
      "grad_norm": 16.600074768066406,
      "learning_rate": 1.2588542483348221e-06,
      "loss": 2.4474,
      "step": 275500
    },
    {
      "epoch": 2.813565551177083,
      "grad_norm": 15.455991744995117,
      "learning_rate": 1.2520367327738426e-06,
      "loss": 2.4417,
      "step": 275600
    },
    {
      "epoch": 2.8145864385323724,
      "grad_norm": 15.013083457946777,
      "learning_rate": 1.2452192172128635e-06,
      "loss": 2.4243,
      "step": 275700
    },
    {
      "epoch": 2.8156073258876617,
      "grad_norm": 16.031827926635742,
      "learning_rate": 1.2384017016518842e-06,
      "loss": 2.5047,
      "step": 275800
    },
    {
      "epoch": 2.816628213242951,
      "grad_norm": 16.085779190063477,
      "learning_rate": 1.2315841860909049e-06,
      "loss": 2.4384,
      "step": 275900
    },
    {
      "epoch": 2.8176491005982403,
      "grad_norm": 15.306703567504883,
      "learning_rate": 1.2247666705299255e-06,
      "loss": 2.3995,
      "step": 276000
    },
    {
      "epoch": 2.818669987953529,
      "grad_norm": 14.125473022460938,
      "learning_rate": 1.2179491549689464e-06,
      "loss": 2.4549,
      "step": 276100
    },
    {
      "epoch": 2.8196908753088183,
      "grad_norm": 16.869083404541016,
      "learning_rate": 1.211131639407967e-06,
      "loss": 2.401,
      "step": 276200
    },
    {
      "epoch": 2.8207117626641076,
      "grad_norm": 19.594228744506836,
      "learning_rate": 1.2043141238469878e-06,
      "loss": 2.4592,
      "step": 276300
    },
    {
      "epoch": 2.821732650019397,
      "grad_norm": 19.034080505371094,
      "learning_rate": 1.1974966082860085e-06,
      "loss": 2.4122,
      "step": 276400
    },
    {
      "epoch": 2.822753537374686,
      "grad_norm": 20.078750610351562,
      "learning_rate": 1.1906790927250291e-06,
      "loss": 2.4276,
      "step": 276500
    },
    {
      "epoch": 2.8237744247299754,
      "grad_norm": 16.17262840270996,
      "learning_rate": 1.18386157716405e-06,
      "loss": 2.45,
      "step": 276600
    },
    {
      "epoch": 2.8247953120852642,
      "grad_norm": 21.171865463256836,
      "learning_rate": 1.1770440616030707e-06,
      "loss": 2.4897,
      "step": 276700
    },
    {
      "epoch": 2.8258161994405535,
      "grad_norm": 15.918770790100098,
      "learning_rate": 1.1702265460420914e-06,
      "loss": 2.4778,
      "step": 276800
    },
    {
      "epoch": 2.826837086795843,
      "grad_norm": 21.122573852539062,
      "learning_rate": 1.163409030481112e-06,
      "loss": 2.4261,
      "step": 276900
    },
    {
      "epoch": 2.827857974151132,
      "grad_norm": 18.973628997802734,
      "learning_rate": 1.156591514920133e-06,
      "loss": 2.4561,
      "step": 277000
    },
    {
      "epoch": 2.8288788615064213,
      "grad_norm": 14.964824676513672,
      "learning_rate": 1.1497739993591536e-06,
      "loss": 2.3789,
      "step": 277100
    },
    {
      "epoch": 2.8298997488617106,
      "grad_norm": 16.748388290405273,
      "learning_rate": 1.1429564837981743e-06,
      "loss": 2.4811,
      "step": 277200
    },
    {
      "epoch": 2.830920636217,
      "grad_norm": 22.229543685913086,
      "learning_rate": 1.1361389682371952e-06,
      "loss": 2.3755,
      "step": 277300
    },
    {
      "epoch": 2.831941523572289,
      "grad_norm": 15.468184471130371,
      "learning_rate": 1.1293214526762159e-06,
      "loss": 2.3997,
      "step": 277400
    },
    {
      "epoch": 2.8329624109275784,
      "grad_norm": 16.48225212097168,
      "learning_rate": 1.1225039371152365e-06,
      "loss": 2.521,
      "step": 277500
    },
    {
      "epoch": 2.8339832982828677,
      "grad_norm": 16.654111862182617,
      "learning_rate": 1.1156864215542572e-06,
      "loss": 2.5008,
      "step": 277600
    },
    {
      "epoch": 2.835004185638157,
      "grad_norm": 15.385568618774414,
      "learning_rate": 1.1088689059932781e-06,
      "loss": 2.4284,
      "step": 277700
    },
    {
      "epoch": 2.8360250729934458,
      "grad_norm": 19.579519271850586,
      "learning_rate": 1.1020513904322988e-06,
      "loss": 2.4267,
      "step": 277800
    },
    {
      "epoch": 2.837045960348735,
      "grad_norm": 15.161864280700684,
      "learning_rate": 1.0952338748713195e-06,
      "loss": 2.4265,
      "step": 277900
    },
    {
      "epoch": 2.8380668477040243,
      "grad_norm": 19.900074005126953,
      "learning_rate": 1.0884163593103401e-06,
      "loss": 2.4384,
      "step": 278000
    },
    {
      "epoch": 2.8390877350593136,
      "grad_norm": 12.156400680541992,
      "learning_rate": 1.0816670189049708e-06,
      "loss": 2.3385,
      "step": 278100
    },
    {
      "epoch": 2.840108622414603,
      "grad_norm": 17.4246826171875,
      "learning_rate": 1.0748495033439915e-06,
      "loss": 2.4377,
      "step": 278200
    },
    {
      "epoch": 2.841129509769892,
      "grad_norm": 16.19384765625,
      "learning_rate": 1.0680319877830122e-06,
      "loss": 2.463,
      "step": 278300
    },
    {
      "epoch": 2.8421503971251814,
      "grad_norm": 16.00928497314453,
      "learning_rate": 1.061214472222033e-06,
      "loss": 2.3783,
      "step": 278400
    },
    {
      "epoch": 2.84317128448047,
      "grad_norm": 16.309478759765625,
      "learning_rate": 1.0543969566610535e-06,
      "loss": 2.3834,
      "step": 278500
    },
    {
      "epoch": 2.8441921718357595,
      "grad_norm": 19.85737419128418,
      "learning_rate": 1.0475794411000744e-06,
      "loss": 2.492,
      "step": 278600
    },
    {
      "epoch": 2.8452130591910487,
      "grad_norm": 15.785083770751953,
      "learning_rate": 1.040761925539095e-06,
      "loss": 2.4317,
      "step": 278700
    },
    {
      "epoch": 2.846233946546338,
      "grad_norm": 17.258867263793945,
      "learning_rate": 1.0339444099781158e-06,
      "loss": 2.3562,
      "step": 278800
    },
    {
      "epoch": 2.8472548339016273,
      "grad_norm": 14.796795845031738,
      "learning_rate": 1.0271268944171366e-06,
      "loss": 2.406,
      "step": 278900
    },
    {
      "epoch": 2.8482757212569165,
      "grad_norm": 14.699329376220703,
      "learning_rate": 1.0203093788561573e-06,
      "loss": 2.3884,
      "step": 279000
    },
    {
      "epoch": 2.849296608612206,
      "grad_norm": 13.716256141662598,
      "learning_rate": 1.013491863295178e-06,
      "loss": 2.3391,
      "step": 279100
    },
    {
      "epoch": 2.850317495967495,
      "grad_norm": 17.169593811035156,
      "learning_rate": 1.0066743477341987e-06,
      "loss": 2.4614,
      "step": 279200
    },
    {
      "epoch": 2.8513383833227843,
      "grad_norm": 24.2255859375,
      "learning_rate": 9.999250073288293e-07,
      "loss": 2.4781,
      "step": 279300
    },
    {
      "epoch": 2.8523592706780736,
      "grad_norm": 21.614377975463867,
      "learning_rate": 9.9310749176785e-07,
      "loss": 2.4486,
      "step": 279400
    },
    {
      "epoch": 2.853380158033363,
      "grad_norm": 11.235755920410156,
      "learning_rate": 9.862899762068707e-07,
      "loss": 2.4292,
      "step": 279500
    },
    {
      "epoch": 2.8544010453886517,
      "grad_norm": 16.030799865722656,
      "learning_rate": 9.794724606458916e-07,
      "loss": 2.4564,
      "step": 279600
    },
    {
      "epoch": 2.855421932743941,
      "grad_norm": 14.908670425415039,
      "learning_rate": 9.726549450849123e-07,
      "loss": 2.3809,
      "step": 279700
    },
    {
      "epoch": 2.8564428200992302,
      "grad_norm": 15.749785423278809,
      "learning_rate": 9.65837429523933e-07,
      "loss": 2.4554,
      "step": 279800
    },
    {
      "epoch": 2.8574637074545195,
      "grad_norm": 23.82488250732422,
      "learning_rate": 9.590199139629536e-07,
      "loss": 2.5023,
      "step": 279900
    },
    {
      "epoch": 2.8584845948098088,
      "grad_norm": 18.22187042236328,
      "learning_rate": 9.522023984019745e-07,
      "loss": 2.383,
      "step": 280000
    },
    {
      "epoch": 2.859505482165098,
      "grad_norm": 15.396437644958496,
      "learning_rate": 9.453848828409952e-07,
      "loss": 2.3307,
      "step": 280100
    },
    {
      "epoch": 2.860526369520387,
      "grad_norm": 22.396774291992188,
      "learning_rate": 9.38567367280016e-07,
      "loss": 2.4548,
      "step": 280200
    },
    {
      "epoch": 2.861547256875676,
      "grad_norm": 14.077040672302246,
      "learning_rate": 9.317498517190366e-07,
      "loss": 2.391,
      "step": 280300
    },
    {
      "epoch": 2.8625681442309654,
      "grad_norm": 15.924925804138184,
      "learning_rate": 9.249323361580574e-07,
      "loss": 2.398,
      "step": 280400
    },
    {
      "epoch": 2.8635890315862547,
      "grad_norm": 21.07895278930664,
      "learning_rate": 9.181148205970782e-07,
      "loss": 2.4708,
      "step": 280500
    },
    {
      "epoch": 2.864609918941544,
      "grad_norm": 17.68722915649414,
      "learning_rate": 9.112973050360988e-07,
      "loss": 2.5075,
      "step": 280600
    },
    {
      "epoch": 2.865630806296833,
      "grad_norm": 15.65539836883545,
      "learning_rate": 9.044797894751195e-07,
      "loss": 2.3471,
      "step": 280700
    },
    {
      "epoch": 2.8666516936521225,
      "grad_norm": 22.62466049194336,
      "learning_rate": 8.976622739141402e-07,
      "loss": 2.4239,
      "step": 280800
    },
    {
      "epoch": 2.8676725810074117,
      "grad_norm": 22.280277252197266,
      "learning_rate": 8.90844758353161e-07,
      "loss": 2.5038,
      "step": 280900
    },
    {
      "epoch": 2.868693468362701,
      "grad_norm": 13.422011375427246,
      "learning_rate": 8.840272427921817e-07,
      "loss": 2.4842,
      "step": 281000
    },
    {
      "epoch": 2.8697143557179903,
      "grad_norm": 18.766883850097656,
      "learning_rate": 8.772097272312025e-07,
      "loss": 2.3082,
      "step": 281100
    },
    {
      "epoch": 2.8707352430732795,
      "grad_norm": 20.537097930908203,
      "learning_rate": 8.703922116702232e-07,
      "loss": 2.4137,
      "step": 281200
    },
    {
      "epoch": 2.8717561304285684,
      "grad_norm": 17.913726806640625,
      "learning_rate": 8.635746961092439e-07,
      "loss": 2.4375,
      "step": 281300
    },
    {
      "epoch": 2.8727770177838576,
      "grad_norm": 19.74944305419922,
      "learning_rate": 8.567571805482647e-07,
      "loss": 2.4876,
      "step": 281400
    },
    {
      "epoch": 2.873797905139147,
      "grad_norm": 16.45720863342285,
      "learning_rate": 8.499396649872854e-07,
      "loss": 2.4441,
      "step": 281500
    },
    {
      "epoch": 2.874818792494436,
      "grad_norm": 13.982361793518066,
      "learning_rate": 8.431221494263062e-07,
      "loss": 2.3164,
      "step": 281600
    },
    {
      "epoch": 2.8758396798497254,
      "grad_norm": 15.193480491638184,
      "learning_rate": 8.363046338653269e-07,
      "loss": 2.4038,
      "step": 281700
    },
    {
      "epoch": 2.8768605672050147,
      "grad_norm": 16.497343063354492,
      "learning_rate": 8.294871183043476e-07,
      "loss": 2.4169,
      "step": 281800
    },
    {
      "epoch": 2.877881454560304,
      "grad_norm": 15.98963737487793,
      "learning_rate": 8.226696027433684e-07,
      "loss": 2.511,
      "step": 281900
    },
    {
      "epoch": 2.878902341915593,
      "grad_norm": 16.276994705200195,
      "learning_rate": 8.158520871823891e-07,
      "loss": 2.4357,
      "step": 282000
    },
    {
      "epoch": 2.879923229270882,
      "grad_norm": 12.241246223449707,
      "learning_rate": 8.090345716214099e-07,
      "loss": 2.4482,
      "step": 282100
    },
    {
      "epoch": 2.8809441166261713,
      "grad_norm": 17.167085647583008,
      "learning_rate": 8.022170560604305e-07,
      "loss": 2.3134,
      "step": 282200
    },
    {
      "epoch": 2.8819650039814606,
      "grad_norm": 15.227715492248535,
      "learning_rate": 7.953995404994512e-07,
      "loss": 2.4496,
      "step": 282300
    },
    {
      "epoch": 2.88298589133675,
      "grad_norm": 16.25516700744629,
      "learning_rate": 7.885820249384719e-07,
      "loss": 2.4939,
      "step": 282400
    },
    {
      "epoch": 2.884006778692039,
      "grad_norm": 16.884510040283203,
      "learning_rate": 7.817645093774927e-07,
      "loss": 2.4222,
      "step": 282500
    },
    {
      "epoch": 2.8850276660473284,
      "grad_norm": 18.63909339904785,
      "learning_rate": 7.749469938165134e-07,
      "loss": 2.4152,
      "step": 282600
    },
    {
      "epoch": 2.8860485534026177,
      "grad_norm": 16.133848190307617,
      "learning_rate": 7.681294782555342e-07,
      "loss": 2.4081,
      "step": 282700
    },
    {
      "epoch": 2.887069440757907,
      "grad_norm": 22.96240997314453,
      "learning_rate": 7.61311962694555e-07,
      "loss": 2.4835,
      "step": 282800
    },
    {
      "epoch": 2.888090328113196,
      "grad_norm": 13.039169311523438,
      "learning_rate": 7.544944471335756e-07,
      "loss": 2.3636,
      "step": 282900
    },
    {
      "epoch": 2.8891112154684855,
      "grad_norm": 18.92811393737793,
      "learning_rate": 7.476769315725964e-07,
      "loss": 2.4032,
      "step": 283000
    },
    {
      "epoch": 2.8901321028237743,
      "grad_norm": 14.1493501663208,
      "learning_rate": 7.408594160116171e-07,
      "loss": 2.3748,
      "step": 283100
    },
    {
      "epoch": 2.8911529901790636,
      "grad_norm": 13.977937698364258,
      "learning_rate": 7.340419004506379e-07,
      "loss": 2.4474,
      "step": 283200
    },
    {
      "epoch": 2.892173877534353,
      "grad_norm": 16.397499084472656,
      "learning_rate": 7.272243848896587e-07,
      "loss": 2.3976,
      "step": 283300
    },
    {
      "epoch": 2.893194764889642,
      "grad_norm": 16.874778747558594,
      "learning_rate": 7.204068693286793e-07,
      "loss": 2.4138,
      "step": 283400
    },
    {
      "epoch": 2.8942156522449314,
      "grad_norm": 12.32835865020752,
      "learning_rate": 7.135893537677001e-07,
      "loss": 2.4989,
      "step": 283500
    },
    {
      "epoch": 2.8952365396002206,
      "grad_norm": 19.202226638793945,
      "learning_rate": 7.067718382067208e-07,
      "loss": 2.3908,
      "step": 283600
    },
    {
      "epoch": 2.8962574269555095,
      "grad_norm": 19.282093048095703,
      "learning_rate": 6.999543226457416e-07,
      "loss": 2.4027,
      "step": 283700
    },
    {
      "epoch": 2.8972783143107987,
      "grad_norm": 12.438176155090332,
      "learning_rate": 6.93204982240372e-07,
      "loss": 2.5312,
      "step": 283800
    },
    {
      "epoch": 2.898299201666088,
      "grad_norm": 16.314746856689453,
      "learning_rate": 6.863874666793928e-07,
      "loss": 2.4015,
      "step": 283900
    },
    {
      "epoch": 2.8993200890213773,
      "grad_norm": 18.407472610473633,
      "learning_rate": 6.795699511184135e-07,
      "loss": 2.4613,
      "step": 284000
    },
    {
      "epoch": 2.9003409763766665,
      "grad_norm": 18.354084014892578,
      "learning_rate": 6.727524355574343e-07,
      "loss": 2.3998,
      "step": 284100
    },
    {
      "epoch": 2.901361863731956,
      "grad_norm": 19.427684783935547,
      "learning_rate": 6.65934919996455e-07,
      "loss": 2.4216,
      "step": 284200
    },
    {
      "epoch": 2.902382751087245,
      "grad_norm": 16.718196868896484,
      "learning_rate": 6.591174044354756e-07,
      "loss": 2.4468,
      "step": 284300
    },
    {
      "epoch": 2.9034036384425344,
      "grad_norm": 16.33131217956543,
      "learning_rate": 6.522998888744964e-07,
      "loss": 2.3376,
      "step": 284400
    },
    {
      "epoch": 2.9044245257978236,
      "grad_norm": 16.10821533203125,
      "learning_rate": 6.454823733135171e-07,
      "loss": 2.4455,
      "step": 284500
    },
    {
      "epoch": 2.905445413153113,
      "grad_norm": 17.057621002197266,
      "learning_rate": 6.386648577525379e-07,
      "loss": 2.4473,
      "step": 284600
    },
    {
      "epoch": 2.906466300508402,
      "grad_norm": 14.148188591003418,
      "learning_rate": 6.318473421915585e-07,
      "loss": 2.4302,
      "step": 284700
    },
    {
      "epoch": 2.907487187863691,
      "grad_norm": 13.604464530944824,
      "learning_rate": 6.250298266305793e-07,
      "loss": 2.4251,
      "step": 284800
    },
    {
      "epoch": 2.9085080752189802,
      "grad_norm": 12.704695701599121,
      "learning_rate": 6.182123110696001e-07,
      "loss": 2.467,
      "step": 284900
    },
    {
      "epoch": 2.9095289625742695,
      "grad_norm": 15.77064037322998,
      "learning_rate": 6.113947955086208e-07,
      "loss": 2.4385,
      "step": 285000
    },
    {
      "epoch": 2.910549849929559,
      "grad_norm": 17.63701057434082,
      "learning_rate": 6.045772799476416e-07,
      "loss": 2.4329,
      "step": 285100
    },
    {
      "epoch": 2.911570737284848,
      "grad_norm": 18.83981704711914,
      "learning_rate": 5.977597643866622e-07,
      "loss": 2.3069,
      "step": 285200
    },
    {
      "epoch": 2.9125916246401373,
      "grad_norm": 21.325754165649414,
      "learning_rate": 5.90942248825683e-07,
      "loss": 2.3997,
      "step": 285300
    },
    {
      "epoch": 2.9136125119954266,
      "grad_norm": 20.130727767944336,
      "learning_rate": 5.841247332647037e-07,
      "loss": 2.5056,
      "step": 285400
    },
    {
      "epoch": 2.9146333993507154,
      "grad_norm": 15.812884330749512,
      "learning_rate": 5.773072177037244e-07,
      "loss": 2.4571,
      "step": 285500
    },
    {
      "epoch": 2.9156542867060047,
      "grad_norm": 17.165546417236328,
      "learning_rate": 5.704897021427452e-07,
      "loss": 2.4736,
      "step": 285600
    },
    {
      "epoch": 2.916675174061294,
      "grad_norm": 16.220905303955078,
      "learning_rate": 5.63672186581766e-07,
      "loss": 2.4416,
      "step": 285700
    },
    {
      "epoch": 2.917696061416583,
      "grad_norm": 18.169689178466797,
      "learning_rate": 5.568546710207866e-07,
      "loss": 2.4372,
      "step": 285800
    },
    {
      "epoch": 2.9187169487718725,
      "grad_norm": 13.8594970703125,
      "learning_rate": 5.501053306154172e-07,
      "loss": 2.5451,
      "step": 285900
    },
    {
      "epoch": 2.9197378361271618,
      "grad_norm": 14.593242645263672,
      "learning_rate": 5.432878150544379e-07,
      "loss": 2.4389,
      "step": 286000
    },
    {
      "epoch": 2.920758723482451,
      "grad_norm": 21.250730514526367,
      "learning_rate": 5.364702994934586e-07,
      "loss": 2.4635,
      "step": 286100
    },
    {
      "epoch": 2.9217796108377403,
      "grad_norm": 17.387977600097656,
      "learning_rate": 5.296527839324793e-07,
      "loss": 2.4552,
      "step": 286200
    },
    {
      "epoch": 2.9228004981930296,
      "grad_norm": 14.808769226074219,
      "learning_rate": 5.228352683715001e-07,
      "loss": 2.4386,
      "step": 286300
    },
    {
      "epoch": 2.923821385548319,
      "grad_norm": 17.869173049926758,
      "learning_rate": 5.160177528105209e-07,
      "loss": 2.4555,
      "step": 286400
    },
    {
      "epoch": 2.9248422729036077,
      "grad_norm": 14.12975788116455,
      "learning_rate": 5.092002372495416e-07,
      "loss": 2.4677,
      "step": 286500
    },
    {
      "epoch": 2.925863160258897,
      "grad_norm": 23.88980484008789,
      "learning_rate": 5.023827216885623e-07,
      "loss": 2.4523,
      "step": 286600
    },
    {
      "epoch": 2.926884047614186,
      "grad_norm": 19.475738525390625,
      "learning_rate": 4.95565206127583e-07,
      "loss": 2.4474,
      "step": 286700
    },
    {
      "epoch": 2.9279049349694755,
      "grad_norm": 16.694263458251953,
      "learning_rate": 4.887476905666037e-07,
      "loss": 2.3736,
      "step": 286800
    },
    {
      "epoch": 2.9289258223247647,
      "grad_norm": 15.459085464477539,
      "learning_rate": 4.819301750056245e-07,
      "loss": 2.3542,
      "step": 286900
    },
    {
      "epoch": 2.929946709680054,
      "grad_norm": 18.183576583862305,
      "learning_rate": 4.751126594446452e-07,
      "loss": 2.435,
      "step": 287000
    },
    {
      "epoch": 2.9309675970353433,
      "grad_norm": 20.262493133544922,
      "learning_rate": 4.6829514388366595e-07,
      "loss": 2.3702,
      "step": 287100
    },
    {
      "epoch": 2.931988484390632,
      "grad_norm": 16.00347137451172,
      "learning_rate": 4.614776283226867e-07,
      "loss": 2.4619,
      "step": 287200
    },
    {
      "epoch": 2.9330093717459214,
      "grad_norm": 16.745019912719727,
      "learning_rate": 4.5466011276170746e-07,
      "loss": 2.461,
      "step": 287300
    },
    {
      "epoch": 2.9340302591012106,
      "grad_norm": 15.630977630615234,
      "learning_rate": 4.478425972007282e-07,
      "loss": 2.447,
      "step": 287400
    },
    {
      "epoch": 2.9350511464565,
      "grad_norm": 13.446789741516113,
      "learning_rate": 4.4102508163974887e-07,
      "loss": 2.451,
      "step": 287500
    },
    {
      "epoch": 2.936072033811789,
      "grad_norm": 22.42988395690918,
      "learning_rate": 4.342075660787696e-07,
      "loss": 2.3913,
      "step": 287600
    },
    {
      "epoch": 2.9370929211670784,
      "grad_norm": 18.53255271911621,
      "learning_rate": 4.2739005051779033e-07,
      "loss": 2.4558,
      "step": 287700
    },
    {
      "epoch": 2.9381138085223677,
      "grad_norm": 17.02786636352539,
      "learning_rate": 4.2057253495681106e-07,
      "loss": 2.442,
      "step": 287800
    },
    {
      "epoch": 2.939134695877657,
      "grad_norm": 17.05421257019043,
      "learning_rate": 4.137550193958318e-07,
      "loss": 2.4779,
      "step": 287900
    },
    {
      "epoch": 2.9401555832329462,
      "grad_norm": 12.871294975280762,
      "learning_rate": 4.0693750383485257e-07,
      "loss": 2.4018,
      "step": 288000
    },
    {
      "epoch": 2.9411764705882355,
      "grad_norm": 16.07102394104004,
      "learning_rate": 4.001199882738733e-07,
      "loss": 2.4078,
      "step": 288100
    },
    {
      "epoch": 2.9421973579435248,
      "grad_norm": 19.881113052368164,
      "learning_rate": 3.9330247271289403e-07,
      "loss": 2.4442,
      "step": 288200
    },
    {
      "epoch": 2.9432182452988136,
      "grad_norm": 21.860029220581055,
      "learning_rate": 3.864849571519147e-07,
      "loss": 2.4192,
      "step": 288300
    },
    {
      "epoch": 2.944239132654103,
      "grad_norm": 18.783788681030273,
      "learning_rate": 3.7966744159093544e-07,
      "loss": 2.4479,
      "step": 288400
    },
    {
      "epoch": 2.945260020009392,
      "grad_norm": 13.293804168701172,
      "learning_rate": 3.72918101185566e-07,
      "loss": 2.4512,
      "step": 288500
    },
    {
      "epoch": 2.9462809073646814,
      "grad_norm": 16.08069610595703,
      "learning_rate": 3.661005856245867e-07,
      "loss": 2.3903,
      "step": 288600
    },
    {
      "epoch": 2.9473017947199707,
      "grad_norm": 15.996484756469727,
      "learning_rate": 3.5928307006360745e-07,
      "loss": 2.4165,
      "step": 288700
    },
    {
      "epoch": 2.94832268207526,
      "grad_norm": 20.312053680419922,
      "learning_rate": 3.5246555450262813e-07,
      "loss": 2.3057,
      "step": 288800
    },
    {
      "epoch": 2.9493435694305488,
      "grad_norm": 21.164398193359375,
      "learning_rate": 3.456480389416489e-07,
      "loss": 2.3269,
      "step": 288900
    },
    {
      "epoch": 2.950364456785838,
      "grad_norm": 16.427888870239258,
      "learning_rate": 3.3883052338066964e-07,
      "loss": 2.4907,
      "step": 289000
    },
    {
      "epoch": 2.9513853441411273,
      "grad_norm": 18.907190322875977,
      "learning_rate": 3.320130078196904e-07,
      "loss": 2.4542,
      "step": 289100
    },
    {
      "epoch": 2.9524062314964166,
      "grad_norm": 15.30894947052002,
      "learning_rate": 3.251954922587111e-07,
      "loss": 2.4692,
      "step": 289200
    },
    {
      "epoch": 2.953427118851706,
      "grad_norm": 16.288869857788086,
      "learning_rate": 3.1837797669773184e-07,
      "loss": 2.4651,
      "step": 289300
    },
    {
      "epoch": 2.954448006206995,
      "grad_norm": 14.759340286254883,
      "learning_rate": 3.116286362923624e-07,
      "loss": 2.4308,
      "step": 289400
    },
    {
      "epoch": 2.9554688935622844,
      "grad_norm": 17.524333953857422,
      "learning_rate": 3.0481112073138307e-07,
      "loss": 2.4765,
      "step": 289500
    },
    {
      "epoch": 2.9564897809175736,
      "grad_norm": 14.517355918884277,
      "learning_rate": 2.979936051704038e-07,
      "loss": 2.4325,
      "step": 289600
    },
    {
      "epoch": 2.957510668272863,
      "grad_norm": 16.47818946838379,
      "learning_rate": 2.911760896094246e-07,
      "loss": 2.4051,
      "step": 289700
    },
    {
      "epoch": 2.958531555628152,
      "grad_norm": 13.461228370666504,
      "learning_rate": 2.843585740484453e-07,
      "loss": 2.3719,
      "step": 289800
    },
    {
      "epoch": 2.9595524429834414,
      "grad_norm": 13.279057502746582,
      "learning_rate": 2.77541058487466e-07,
      "loss": 2.4408,
      "step": 289900
    },
    {
      "epoch": 2.9605733303387303,
      "grad_norm": 18.33638572692871,
      "learning_rate": 2.7072354292648677e-07,
      "loss": 2.3979,
      "step": 290000
    },
    {
      "epoch": 2.9615942176940195,
      "grad_norm": 13.941060066223145,
      "learning_rate": 2.639060273655075e-07,
      "loss": 2.4363,
      "step": 290100
    },
    {
      "epoch": 2.962615105049309,
      "grad_norm": 16.79057502746582,
      "learning_rate": 2.5708851180452823e-07,
      "loss": 2.4921,
      "step": 290200
    },
    {
      "epoch": 2.963635992404598,
      "grad_norm": 20.014352798461914,
      "learning_rate": 2.502709962435489e-07,
      "loss": 2.4471,
      "step": 290300
    },
    {
      "epoch": 2.9646568797598873,
      "grad_norm": 15.218033790588379,
      "learning_rate": 2.434534806825697e-07,
      "loss": 2.3517,
      "step": 290400
    },
    {
      "epoch": 2.9656777671151766,
      "grad_norm": 12.152961730957031,
      "learning_rate": 2.3663596512159042e-07,
      "loss": 2.3768,
      "step": 290500
    },
    {
      "epoch": 2.966698654470466,
      "grad_norm": 18.14368438720703,
      "learning_rate": 2.2981844956061115e-07,
      "loss": 2.4948,
      "step": 290600
    },
    {
      "epoch": 2.9677195418257547,
      "grad_norm": 16.893573760986328,
      "learning_rate": 2.2300093399963186e-07,
      "loss": 2.4218,
      "step": 290700
    },
    {
      "epoch": 2.968740429181044,
      "grad_norm": 16.87761116027832,
      "learning_rate": 2.161834184386526e-07,
      "loss": 2.4279,
      "step": 290800
    },
    {
      "epoch": 2.9697613165363332,
      "grad_norm": 16.04515838623047,
      "learning_rate": 2.0936590287767334e-07,
      "loss": 2.4095,
      "step": 290900
    },
    {
      "epoch": 2.9707822038916225,
      "grad_norm": 23.093374252319336,
      "learning_rate": 2.0254838731669407e-07,
      "loss": 2.5067,
      "step": 291000
    },
    {
      "epoch": 2.9718030912469118,
      "grad_norm": 18.285118103027344,
      "learning_rate": 1.9573087175571478e-07,
      "loss": 2.5057,
      "step": 291100
    },
    {
      "epoch": 2.972823978602201,
      "grad_norm": 17.7062931060791,
      "learning_rate": 1.8891335619473553e-07,
      "loss": 2.4244,
      "step": 291200
    },
    {
      "epoch": 2.9738448659574903,
      "grad_norm": 13.907143592834473,
      "learning_rate": 1.8209584063375627e-07,
      "loss": 2.3669,
      "step": 291300
    },
    {
      "epoch": 2.9748657533127796,
      "grad_norm": 13.376702308654785,
      "learning_rate": 1.75278325072777e-07,
      "loss": 2.398,
      "step": 291400
    },
    {
      "epoch": 2.975886640668069,
      "grad_norm": 15.228813171386719,
      "learning_rate": 1.684608095117977e-07,
      "loss": 2.4827,
      "step": 291500
    },
    {
      "epoch": 2.976907528023358,
      "grad_norm": 17.500244140625,
      "learning_rate": 1.6164329395081846e-07,
      "loss": 2.4864,
      "step": 291600
    },
    {
      "epoch": 2.9779284153786474,
      "grad_norm": 15.932963371276855,
      "learning_rate": 1.5482577838983919e-07,
      "loss": 2.4555,
      "step": 291700
    },
    {
      "epoch": 2.978949302733936,
      "grad_norm": 18.975603103637695,
      "learning_rate": 1.4800826282885992e-07,
      "loss": 2.5509,
      "step": 291800
    },
    {
      "epoch": 2.9799701900892255,
      "grad_norm": 17.83061981201172,
      "learning_rate": 1.4119074726788065e-07,
      "loss": 2.4119,
      "step": 291900
    },
    {
      "epoch": 2.9809910774445147,
      "grad_norm": 19.445798873901367,
      "learning_rate": 1.3437323170690138e-07,
      "loss": 2.5471,
      "step": 292000
    },
    {
      "epoch": 2.982011964799804,
      "grad_norm": 19.029094696044922,
      "learning_rate": 1.275557161459221e-07,
      "loss": 2.4318,
      "step": 292100
    },
    {
      "epoch": 2.9830328521550933,
      "grad_norm": 17.195941925048828,
      "learning_rate": 1.2073820058494284e-07,
      "loss": 2.4205,
      "step": 292200
    },
    {
      "epoch": 2.9840537395103826,
      "grad_norm": 17.410419464111328,
      "learning_rate": 1.1392068502396358e-07,
      "loss": 2.4485,
      "step": 292300
    },
    {
      "epoch": 2.9850746268656714,
      "grad_norm": 15.185273170471191,
      "learning_rate": 1.071031694629843e-07,
      "loss": 2.4584,
      "step": 292400
    },
    {
      "epoch": 2.9860955142209606,
      "grad_norm": 15.411540985107422,
      "learning_rate": 1.0028565390200504e-07,
      "loss": 2.4253,
      "step": 292500
    },
    {
      "epoch": 2.98711640157625,
      "grad_norm": 14.715523719787598,
      "learning_rate": 9.346813834102577e-08,
      "loss": 2.517,
      "step": 292600
    },
    {
      "epoch": 2.988137288931539,
      "grad_norm": 20.314496994018555,
      "learning_rate": 8.66506227800465e-08,
      "loss": 2.4551,
      "step": 292700
    },
    {
      "epoch": 2.9891581762868284,
      "grad_norm": 16.033620834350586,
      "learning_rate": 7.983310721906723e-08,
      "loss": 2.4245,
      "step": 292800
    },
    {
      "epoch": 2.9901790636421177,
      "grad_norm": 15.688019752502441,
      "learning_rate": 7.301559165808796e-08,
      "loss": 2.4823,
      "step": 292900
    },
    {
      "epoch": 2.991199950997407,
      "grad_norm": 21.80885124206543,
      "learning_rate": 6.619807609710871e-08,
      "loss": 2.5433,
      "step": 293000
    },
    {
      "epoch": 2.9922208383526963,
      "grad_norm": 18.492170333862305,
      "learning_rate": 5.938056053612943e-08,
      "loss": 2.3656,
      "step": 293100
    },
    {
      "epoch": 2.9932417257079855,
      "grad_norm": 13.303136825561523,
      "learning_rate": 5.256304497515016e-08,
      "loss": 2.4222,
      "step": 293200
    },
    {
      "epoch": 2.994262613063275,
      "grad_norm": 13.610508918762207,
      "learning_rate": 4.57455294141709e-08,
      "loss": 2.4307,
      "step": 293300
    },
    {
      "epoch": 2.995283500418564,
      "grad_norm": 16.16058349609375,
      "learning_rate": 3.892801385319162e-08,
      "loss": 2.4735,
      "step": 293400
    },
    {
      "epoch": 2.996304387773853,
      "grad_norm": 18.962223052978516,
      "learning_rate": 3.211049829221235e-08,
      "loss": 2.3717,
      "step": 293500
    },
    {
      "epoch": 2.997325275129142,
      "grad_norm": 21.804048538208008,
      "learning_rate": 2.5292982731233086e-08,
      "loss": 2.46,
      "step": 293600
    },
    {
      "epoch": 2.9983461624844314,
      "grad_norm": 12.417292594909668,
      "learning_rate": 1.8475467170253816e-08,
      "loss": 2.4323,
      "step": 293700
    },
    {
      "epoch": 2.9993670498397207,
      "grad_norm": 17.705087661743164,
      "learning_rate": 1.165795160927455e-08,
      "loss": 2.4656,
      "step": 293800
    }
  ],
  "logging_steps": 100,
  "max_steps": 293862,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.3112845856669429e+17,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
